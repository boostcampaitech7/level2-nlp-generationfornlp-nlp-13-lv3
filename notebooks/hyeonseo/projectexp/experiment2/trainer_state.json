{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1519,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006583278472679394,
      "grad_norm": 47.83777618408203,
      "learning_rate": 6.578947368421053e-08,
      "loss": 8.2211,
      "step": 1
    },
    {
      "epoch": 0.0013166556945358788,
      "grad_norm": 42.13090896606445,
      "learning_rate": 1.3157894736842107e-07,
      "loss": 8.425,
      "step": 2
    },
    {
      "epoch": 0.0019749835418038184,
      "grad_norm": 51.259239196777344,
      "learning_rate": 1.9736842105263157e-07,
      "loss": 8.9833,
      "step": 3
    },
    {
      "epoch": 0.0026333113890717576,
      "grad_norm": 40.82755661010742,
      "learning_rate": 2.6315789473684213e-07,
      "loss": 8.5887,
      "step": 4
    },
    {
      "epoch": 0.0032916392363396972,
      "grad_norm": 40.987430572509766,
      "learning_rate": 3.2894736842105264e-07,
      "loss": 8.5432,
      "step": 5
    },
    {
      "epoch": 0.003949967083607637,
      "grad_norm": 39.37958908081055,
      "learning_rate": 3.9473684210526315e-07,
      "loss": 7.9397,
      "step": 6
    },
    {
      "epoch": 0.004608294930875576,
      "grad_norm": 45.051612854003906,
      "learning_rate": 4.605263157894737e-07,
      "loss": 8.6318,
      "step": 7
    },
    {
      "epoch": 0.005266622778143515,
      "grad_norm": 36.30262756347656,
      "learning_rate": 5.263157894736843e-07,
      "loss": 8.1386,
      "step": 8
    },
    {
      "epoch": 0.005924950625411455,
      "grad_norm": 38.46149826049805,
      "learning_rate": 5.921052631578947e-07,
      "loss": 8.5182,
      "step": 9
    },
    {
      "epoch": 0.0065832784726793945,
      "grad_norm": 42.223575592041016,
      "learning_rate": 6.578947368421053e-07,
      "loss": 8.2784,
      "step": 10
    },
    {
      "epoch": 0.007241606319947334,
      "grad_norm": 41.62485885620117,
      "learning_rate": 7.236842105263158e-07,
      "loss": 8.4713,
      "step": 11
    },
    {
      "epoch": 0.007899934167215274,
      "grad_norm": 43.26200485229492,
      "learning_rate": 7.894736842105263e-07,
      "loss": 8.3436,
      "step": 12
    },
    {
      "epoch": 0.008558262014483212,
      "grad_norm": 37.4140510559082,
      "learning_rate": 8.55263157894737e-07,
      "loss": 8.4019,
      "step": 13
    },
    {
      "epoch": 0.009216589861751152,
      "grad_norm": 43.853904724121094,
      "learning_rate": 9.210526315789474e-07,
      "loss": 8.5987,
      "step": 14
    },
    {
      "epoch": 0.009874917709019092,
      "grad_norm": 39.34306335449219,
      "learning_rate": 9.86842105263158e-07,
      "loss": 8.0712,
      "step": 15
    },
    {
      "epoch": 0.01053324555628703,
      "grad_norm": 34.454620361328125,
      "learning_rate": 1.0526315789473685e-06,
      "loss": 7.894,
      "step": 16
    },
    {
      "epoch": 0.01119157340355497,
      "grad_norm": 41.116764068603516,
      "learning_rate": 1.118421052631579e-06,
      "loss": 8.8504,
      "step": 17
    },
    {
      "epoch": 0.01184990125082291,
      "grad_norm": 45.014827728271484,
      "learning_rate": 1.1842105263157894e-06,
      "loss": 8.8437,
      "step": 18
    },
    {
      "epoch": 0.012508229098090849,
      "grad_norm": 35.82747268676758,
      "learning_rate": 1.25e-06,
      "loss": 7.8877,
      "step": 19
    },
    {
      "epoch": 0.013166556945358789,
      "grad_norm": 43.6363410949707,
      "learning_rate": 1.3157894736842106e-06,
      "loss": 8.4081,
      "step": 20
    },
    {
      "epoch": 0.013824884792626729,
      "grad_norm": 35.098697662353516,
      "learning_rate": 1.3815789473684212e-06,
      "loss": 8.2499,
      "step": 21
    },
    {
      "epoch": 0.014483212639894667,
      "grad_norm": 38.470237731933594,
      "learning_rate": 1.4473684210526317e-06,
      "loss": 8.3264,
      "step": 22
    },
    {
      "epoch": 0.015141540487162607,
      "grad_norm": 37.62247085571289,
      "learning_rate": 1.5131578947368421e-06,
      "loss": 7.9302,
      "step": 23
    },
    {
      "epoch": 0.015799868334430547,
      "grad_norm": 33.24028015136719,
      "learning_rate": 1.5789473684210526e-06,
      "loss": 7.5285,
      "step": 24
    },
    {
      "epoch": 0.016458196181698487,
      "grad_norm": 41.0321044921875,
      "learning_rate": 1.6447368421052635e-06,
      "loss": 7.7329,
      "step": 25
    },
    {
      "epoch": 0.017116524028966424,
      "grad_norm": 40.35602951049805,
      "learning_rate": 1.710526315789474e-06,
      "loss": 7.7432,
      "step": 26
    },
    {
      "epoch": 0.017774851876234364,
      "grad_norm": 32.71498107910156,
      "learning_rate": 1.7763157894736844e-06,
      "loss": 7.7724,
      "step": 27
    },
    {
      "epoch": 0.018433179723502304,
      "grad_norm": 33.780418395996094,
      "learning_rate": 1.8421052631578948e-06,
      "loss": 7.4943,
      "step": 28
    },
    {
      "epoch": 0.019091507570770244,
      "grad_norm": 38.67194366455078,
      "learning_rate": 1.9078947368421057e-06,
      "loss": 7.4897,
      "step": 29
    },
    {
      "epoch": 0.019749835418038184,
      "grad_norm": 41.812164306640625,
      "learning_rate": 1.973684210526316e-06,
      "loss": 7.6964,
      "step": 30
    },
    {
      "epoch": 0.02040816326530612,
      "grad_norm": 32.95809555053711,
      "learning_rate": 2.0394736842105266e-06,
      "loss": 7.8088,
      "step": 31
    },
    {
      "epoch": 0.02106649111257406,
      "grad_norm": 44.060062408447266,
      "learning_rate": 2.105263157894737e-06,
      "loss": 7.2835,
      "step": 32
    },
    {
      "epoch": 0.021724818959842,
      "grad_norm": 50.208770751953125,
      "learning_rate": 2.1710526315789475e-06,
      "loss": 7.2839,
      "step": 33
    },
    {
      "epoch": 0.02238314680710994,
      "grad_norm": 48.06986618041992,
      "learning_rate": 2.236842105263158e-06,
      "loss": 7.048,
      "step": 34
    },
    {
      "epoch": 0.02304147465437788,
      "grad_norm": 32.714656829833984,
      "learning_rate": 2.3026315789473684e-06,
      "loss": 7.4504,
      "step": 35
    },
    {
      "epoch": 0.02369980250164582,
      "grad_norm": 41.78350830078125,
      "learning_rate": 2.368421052631579e-06,
      "loss": 6.8155,
      "step": 36
    },
    {
      "epoch": 0.024358130348913758,
      "grad_norm": 44.08203125,
      "learning_rate": 2.4342105263157898e-06,
      "loss": 6.9199,
      "step": 37
    },
    {
      "epoch": 0.025016458196181698,
      "grad_norm": 37.1431999206543,
      "learning_rate": 2.5e-06,
      "loss": 6.7976,
      "step": 38
    },
    {
      "epoch": 0.025674786043449638,
      "grad_norm": 52.23475646972656,
      "learning_rate": 2.565789473684211e-06,
      "loss": 6.6805,
      "step": 39
    },
    {
      "epoch": 0.026333113890717578,
      "grad_norm": 35.1065673828125,
      "learning_rate": 2.631578947368421e-06,
      "loss": 7.0864,
      "step": 40
    },
    {
      "epoch": 0.026991441737985518,
      "grad_norm": 50.15882110595703,
      "learning_rate": 2.697368421052632e-06,
      "loss": 6.6745,
      "step": 41
    },
    {
      "epoch": 0.027649769585253458,
      "grad_norm": 63.22974395751953,
      "learning_rate": 2.7631578947368424e-06,
      "loss": 6.3715,
      "step": 42
    },
    {
      "epoch": 0.028308097432521395,
      "grad_norm": 78.93370819091797,
      "learning_rate": 2.828947368421053e-06,
      "loss": 6.945,
      "step": 43
    },
    {
      "epoch": 0.028966425279789335,
      "grad_norm": 64.43952941894531,
      "learning_rate": 2.8947368421052634e-06,
      "loss": 6.1687,
      "step": 44
    },
    {
      "epoch": 0.029624753127057275,
      "grad_norm": 66.94886779785156,
      "learning_rate": 2.960526315789474e-06,
      "loss": 6.0844,
      "step": 45
    },
    {
      "epoch": 0.030283080974325215,
      "grad_norm": 72.31129455566406,
      "learning_rate": 3.0263157894736843e-06,
      "loss": 6.0838,
      "step": 46
    },
    {
      "epoch": 0.030941408821593155,
      "grad_norm": 72.17864227294922,
      "learning_rate": 3.092105263157895e-06,
      "loss": 6.7018,
      "step": 47
    },
    {
      "epoch": 0.031599736668861095,
      "grad_norm": 61.83097457885742,
      "learning_rate": 3.157894736842105e-06,
      "loss": 5.597,
      "step": 48
    },
    {
      "epoch": 0.03225806451612903,
      "grad_norm": 40.20801544189453,
      "learning_rate": 3.223684210526316e-06,
      "loss": 5.2704,
      "step": 49
    },
    {
      "epoch": 0.032916392363396975,
      "grad_norm": 56.92897415161133,
      "learning_rate": 3.289473684210527e-06,
      "loss": 5.9554,
      "step": 50
    },
    {
      "epoch": 0.03357472021066491,
      "grad_norm": 43.21034240722656,
      "learning_rate": 3.355263157894737e-06,
      "loss": 5.3561,
      "step": 51
    },
    {
      "epoch": 0.03423304805793285,
      "grad_norm": 45.105525970458984,
      "learning_rate": 3.421052631578948e-06,
      "loss": 5.7045,
      "step": 52
    },
    {
      "epoch": 0.03489137590520079,
      "grad_norm": 33.05732345581055,
      "learning_rate": 3.486842105263158e-06,
      "loss": 5.4605,
      "step": 53
    },
    {
      "epoch": 0.03554970375246873,
      "grad_norm": 26.514385223388672,
      "learning_rate": 3.5526315789473687e-06,
      "loss": 5.0297,
      "step": 54
    },
    {
      "epoch": 0.03620803159973667,
      "grad_norm": 28.825679779052734,
      "learning_rate": 3.618421052631579e-06,
      "loss": 4.8964,
      "step": 55
    },
    {
      "epoch": 0.03686635944700461,
      "grad_norm": 24.838916778564453,
      "learning_rate": 3.6842105263157896e-06,
      "loss": 4.8809,
      "step": 56
    },
    {
      "epoch": 0.037524687294272545,
      "grad_norm": 25.16185760498047,
      "learning_rate": 3.7500000000000005e-06,
      "loss": 4.6503,
      "step": 57
    },
    {
      "epoch": 0.03818301514154049,
      "grad_norm": 28.938310623168945,
      "learning_rate": 3.815789473684211e-06,
      "loss": 4.604,
      "step": 58
    },
    {
      "epoch": 0.038841342988808425,
      "grad_norm": 29.257671356201172,
      "learning_rate": 3.8815789473684214e-06,
      "loss": 4.5015,
      "step": 59
    },
    {
      "epoch": 0.03949967083607637,
      "grad_norm": 29.37628746032715,
      "learning_rate": 3.947368421052632e-06,
      "loss": 4.6035,
      "step": 60
    },
    {
      "epoch": 0.040157998683344305,
      "grad_norm": 26.981672286987305,
      "learning_rate": 4.013157894736842e-06,
      "loss": 4.3832,
      "step": 61
    },
    {
      "epoch": 0.04081632653061224,
      "grad_norm": 21.3386287689209,
      "learning_rate": 4.078947368421053e-06,
      "loss": 4.8691,
      "step": 62
    },
    {
      "epoch": 0.041474654377880185,
      "grad_norm": 21.89997100830078,
      "learning_rate": 4.144736842105263e-06,
      "loss": 4.3896,
      "step": 63
    },
    {
      "epoch": 0.04213298222514812,
      "grad_norm": 24.371509552001953,
      "learning_rate": 4.210526315789474e-06,
      "loss": 4.4079,
      "step": 64
    },
    {
      "epoch": 0.042791310072416065,
      "grad_norm": 22.21674346923828,
      "learning_rate": 4.276315789473684e-06,
      "loss": 4.0165,
      "step": 65
    },
    {
      "epoch": 0.043449637919684,
      "grad_norm": 23.516029357910156,
      "learning_rate": 4.342105263157895e-06,
      "loss": 3.9391,
      "step": 66
    },
    {
      "epoch": 0.044107965766951945,
      "grad_norm": 24.086210250854492,
      "learning_rate": 4.407894736842105e-06,
      "loss": 4.1893,
      "step": 67
    },
    {
      "epoch": 0.04476629361421988,
      "grad_norm": 20.57001304626465,
      "learning_rate": 4.473684210526316e-06,
      "loss": 3.7844,
      "step": 68
    },
    {
      "epoch": 0.04542462146148782,
      "grad_norm": 19.571491241455078,
      "learning_rate": 4.539473684210527e-06,
      "loss": 4.469,
      "step": 69
    },
    {
      "epoch": 0.04608294930875576,
      "grad_norm": 21.2838134765625,
      "learning_rate": 4.605263157894737e-06,
      "loss": 3.7643,
      "step": 70
    },
    {
      "epoch": 0.0467412771560237,
      "grad_norm": 22.45570182800293,
      "learning_rate": 4.671052631578948e-06,
      "loss": 4.4252,
      "step": 71
    },
    {
      "epoch": 0.04739960500329164,
      "grad_norm": 18.25789451599121,
      "learning_rate": 4.736842105263158e-06,
      "loss": 4.1616,
      "step": 72
    },
    {
      "epoch": 0.04805793285055958,
      "grad_norm": 27.125818252563477,
      "learning_rate": 4.802631578947369e-06,
      "loss": 4.4908,
      "step": 73
    },
    {
      "epoch": 0.048716260697827515,
      "grad_norm": 19.4455623626709,
      "learning_rate": 4.8684210526315795e-06,
      "loss": 3.7932,
      "step": 74
    },
    {
      "epoch": 0.04937458854509546,
      "grad_norm": 23.57302474975586,
      "learning_rate": 4.9342105263157895e-06,
      "loss": 4.0323,
      "step": 75
    },
    {
      "epoch": 0.050032916392363395,
      "grad_norm": 30.566192626953125,
      "learning_rate": 5e-06,
      "loss": 3.515,
      "step": 76
    },
    {
      "epoch": 0.05069124423963134,
      "grad_norm": 15.770687103271484,
      "learning_rate": 5.0657894736842104e-06,
      "loss": 3.6106,
      "step": 77
    },
    {
      "epoch": 0.051349572086899276,
      "grad_norm": 19.29024314880371,
      "learning_rate": 5.131578947368422e-06,
      "loss": 3.8882,
      "step": 78
    },
    {
      "epoch": 0.05200789993416721,
      "grad_norm": 16.736408233642578,
      "learning_rate": 5.197368421052632e-06,
      "loss": 3.923,
      "step": 79
    },
    {
      "epoch": 0.052666227781435156,
      "grad_norm": 13.25583553314209,
      "learning_rate": 5.263157894736842e-06,
      "loss": 3.2647,
      "step": 80
    },
    {
      "epoch": 0.05332455562870309,
      "grad_norm": 12.920097351074219,
      "learning_rate": 5.328947368421054e-06,
      "loss": 3.2503,
      "step": 81
    },
    {
      "epoch": 0.053982883475971036,
      "grad_norm": 22.8570499420166,
      "learning_rate": 5.394736842105264e-06,
      "loss": 3.6648,
      "step": 82
    },
    {
      "epoch": 0.05464121132323897,
      "grad_norm": 15.210628509521484,
      "learning_rate": 5.460526315789474e-06,
      "loss": 3.2176,
      "step": 83
    },
    {
      "epoch": 0.055299539170506916,
      "grad_norm": 25.37023162841797,
      "learning_rate": 5.526315789473685e-06,
      "loss": 3.5361,
      "step": 84
    },
    {
      "epoch": 0.05595786701777485,
      "grad_norm": 16.378665924072266,
      "learning_rate": 5.592105263157896e-06,
      "loss": 3.2093,
      "step": 85
    },
    {
      "epoch": 0.05661619486504279,
      "grad_norm": 15.32260513305664,
      "learning_rate": 5.657894736842106e-06,
      "loss": 3.1812,
      "step": 86
    },
    {
      "epoch": 0.05727452271231073,
      "grad_norm": 14.344649314880371,
      "learning_rate": 5.723684210526316e-06,
      "loss": 3.6315,
      "step": 87
    },
    {
      "epoch": 0.05793285055957867,
      "grad_norm": 12.072759628295898,
      "learning_rate": 5.789473684210527e-06,
      "loss": 3.091,
      "step": 88
    },
    {
      "epoch": 0.05859117840684661,
      "grad_norm": 9.674043655395508,
      "learning_rate": 5.855263157894738e-06,
      "loss": 3.0325,
      "step": 89
    },
    {
      "epoch": 0.05924950625411455,
      "grad_norm": 11.876795768737793,
      "learning_rate": 5.921052631578948e-06,
      "loss": 3.0378,
      "step": 90
    },
    {
      "epoch": 0.059907834101382486,
      "grad_norm": 15.794032096862793,
      "learning_rate": 5.9868421052631585e-06,
      "loss": 3.52,
      "step": 91
    },
    {
      "epoch": 0.06056616194865043,
      "grad_norm": 13.35291576385498,
      "learning_rate": 6.0526315789473685e-06,
      "loss": 2.9935,
      "step": 92
    },
    {
      "epoch": 0.061224489795918366,
      "grad_norm": 13.193611145019531,
      "learning_rate": 6.118421052631579e-06,
      "loss": 2.9141,
      "step": 93
    },
    {
      "epoch": 0.06188281764318631,
      "grad_norm": 9.888687133789062,
      "learning_rate": 6.18421052631579e-06,
      "loss": 2.9251,
      "step": 94
    },
    {
      "epoch": 0.06254114549045425,
      "grad_norm": 14.921979904174805,
      "learning_rate": 6.25e-06,
      "loss": 3.2705,
      "step": 95
    },
    {
      "epoch": 0.06319947333772219,
      "grad_norm": 14.531394958496094,
      "learning_rate": 6.31578947368421e-06,
      "loss": 3.277,
      "step": 96
    },
    {
      "epoch": 0.06385780118499013,
      "grad_norm": 20.452608108520508,
      "learning_rate": 6.381578947368422e-06,
      "loss": 3.2706,
      "step": 97
    },
    {
      "epoch": 0.06451612903225806,
      "grad_norm": 6.9991455078125,
      "learning_rate": 6.447368421052632e-06,
      "loss": 2.7528,
      "step": 98
    },
    {
      "epoch": 0.065174456879526,
      "grad_norm": 13.077787399291992,
      "learning_rate": 6.513157894736842e-06,
      "loss": 3.1528,
      "step": 99
    },
    {
      "epoch": 0.06583278472679395,
      "grad_norm": 36.763572692871094,
      "learning_rate": 6.578947368421054e-06,
      "loss": 3.3297,
      "step": 100
    },
    {
      "epoch": 0.06649111257406189,
      "grad_norm": 8.036036491394043,
      "learning_rate": 6.644736842105264e-06,
      "loss": 2.7243,
      "step": 101
    },
    {
      "epoch": 0.06714944042132982,
      "grad_norm": 9.902061462402344,
      "learning_rate": 6.710526315789474e-06,
      "loss": 2.7774,
      "step": 102
    },
    {
      "epoch": 0.06780776826859776,
      "grad_norm": 9.546537399291992,
      "learning_rate": 6.776315789473686e-06,
      "loss": 2.7181,
      "step": 103
    },
    {
      "epoch": 0.0684660961158657,
      "grad_norm": 11.61309814453125,
      "learning_rate": 6.842105263157896e-06,
      "loss": 3.1612,
      "step": 104
    },
    {
      "epoch": 0.06912442396313365,
      "grad_norm": 7.159648418426514,
      "learning_rate": 6.907894736842106e-06,
      "loss": 2.6887,
      "step": 105
    },
    {
      "epoch": 0.06978275181040158,
      "grad_norm": 17.845983505249023,
      "learning_rate": 6.973684210526316e-06,
      "loss": 3.3647,
      "step": 106
    },
    {
      "epoch": 0.07044107965766952,
      "grad_norm": 22.26984977722168,
      "learning_rate": 7.0394736842105274e-06,
      "loss": 3.2912,
      "step": 107
    },
    {
      "epoch": 0.07109940750493746,
      "grad_norm": 11.23533821105957,
      "learning_rate": 7.1052631578947375e-06,
      "loss": 2.6822,
      "step": 108
    },
    {
      "epoch": 0.07175773535220539,
      "grad_norm": 22.312557220458984,
      "learning_rate": 7.1710526315789475e-06,
      "loss": 3.3925,
      "step": 109
    },
    {
      "epoch": 0.07241606319947334,
      "grad_norm": 5.445653915405273,
      "learning_rate": 7.236842105263158e-06,
      "loss": 2.5838,
      "step": 110
    },
    {
      "epoch": 0.07307439104674128,
      "grad_norm": 5.859662055969238,
      "learning_rate": 7.302631578947369e-06,
      "loss": 2.5927,
      "step": 111
    },
    {
      "epoch": 0.07373271889400922,
      "grad_norm": 7.177794933319092,
      "learning_rate": 7.368421052631579e-06,
      "loss": 2.5799,
      "step": 112
    },
    {
      "epoch": 0.07439104674127715,
      "grad_norm": 7.460158348083496,
      "learning_rate": 7.43421052631579e-06,
      "loss": 2.5978,
      "step": 113
    },
    {
      "epoch": 0.07504937458854509,
      "grad_norm": 21.7626953125,
      "learning_rate": 7.500000000000001e-06,
      "loss": 3.124,
      "step": 114
    },
    {
      "epoch": 0.07570770243581304,
      "grad_norm": 5.8196516036987305,
      "learning_rate": 7.565789473684211e-06,
      "loss": 2.5003,
      "step": 115
    },
    {
      "epoch": 0.07636603028308098,
      "grad_norm": 23.747020721435547,
      "learning_rate": 7.631578947368423e-06,
      "loss": 3.1487,
      "step": 116
    },
    {
      "epoch": 0.07702435813034891,
      "grad_norm": 14.688176155090332,
      "learning_rate": 7.697368421052632e-06,
      "loss": 3.0535,
      "step": 117
    },
    {
      "epoch": 0.07768268597761685,
      "grad_norm": 28.64488410949707,
      "learning_rate": 7.763157894736843e-06,
      "loss": 3.1301,
      "step": 118
    },
    {
      "epoch": 0.07834101382488479,
      "grad_norm": 8.744466781616211,
      "learning_rate": 7.828947368421054e-06,
      "loss": 2.4762,
      "step": 119
    },
    {
      "epoch": 0.07899934167215274,
      "grad_norm": 6.6871466636657715,
      "learning_rate": 7.894736842105265e-06,
      "loss": 2.4797,
      "step": 120
    },
    {
      "epoch": 0.07965766951942067,
      "grad_norm": 9.442268371582031,
      "learning_rate": 7.960526315789474e-06,
      "loss": 2.4625,
      "step": 121
    },
    {
      "epoch": 0.08031599736668861,
      "grad_norm": 16.89934539794922,
      "learning_rate": 8.026315789473685e-06,
      "loss": 2.9927,
      "step": 122
    },
    {
      "epoch": 0.08097432521395655,
      "grad_norm": 11.824532508850098,
      "learning_rate": 8.092105263157896e-06,
      "loss": 2.5075,
      "step": 123
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 14.866434097290039,
      "learning_rate": 8.157894736842106e-06,
      "loss": 2.8995,
      "step": 124
    },
    {
      "epoch": 0.08229098090849243,
      "grad_norm": 27.785987854003906,
      "learning_rate": 8.223684210526316e-06,
      "loss": 3.1156,
      "step": 125
    },
    {
      "epoch": 0.08294930875576037,
      "grad_norm": 34.62830352783203,
      "learning_rate": 8.289473684210526e-06,
      "loss": 3.393,
      "step": 126
    },
    {
      "epoch": 0.0836076366030283,
      "grad_norm": 27.918432235717773,
      "learning_rate": 8.355263157894737e-06,
      "loss": 3.01,
      "step": 127
    },
    {
      "epoch": 0.08426596445029624,
      "grad_norm": 34.8498420715332,
      "learning_rate": 8.421052631578948e-06,
      "loss": 3.1563,
      "step": 128
    },
    {
      "epoch": 0.08492429229756418,
      "grad_norm": 25.119417190551758,
      "learning_rate": 8.486842105263159e-06,
      "loss": 3.0485,
      "step": 129
    },
    {
      "epoch": 0.08558262014483213,
      "grad_norm": 21.201295852661133,
      "learning_rate": 8.552631578947368e-06,
      "loss": 3.0722,
      "step": 130
    },
    {
      "epoch": 0.08624094799210007,
      "grad_norm": 24.57232093811035,
      "learning_rate": 8.61842105263158e-06,
      "loss": 3.2161,
      "step": 131
    },
    {
      "epoch": 0.086899275839368,
      "grad_norm": 27.59807014465332,
      "learning_rate": 8.68421052631579e-06,
      "loss": 2.8731,
      "step": 132
    },
    {
      "epoch": 0.08755760368663594,
      "grad_norm": 17.985687255859375,
      "learning_rate": 8.750000000000001e-06,
      "loss": 2.8766,
      "step": 133
    },
    {
      "epoch": 0.08821593153390389,
      "grad_norm": 16.015642166137695,
      "learning_rate": 8.81578947368421e-06,
      "loss": 2.7319,
      "step": 134
    },
    {
      "epoch": 0.08887425938117183,
      "grad_norm": 17.476486206054688,
      "learning_rate": 8.881578947368423e-06,
      "loss": 2.8028,
      "step": 135
    },
    {
      "epoch": 0.08953258722843976,
      "grad_norm": 15.771401405334473,
      "learning_rate": 8.947368421052632e-06,
      "loss": 2.7472,
      "step": 136
    },
    {
      "epoch": 0.0901909150757077,
      "grad_norm": 17.353275299072266,
      "learning_rate": 9.013157894736843e-06,
      "loss": 2.7613,
      "step": 137
    },
    {
      "epoch": 0.09084924292297564,
      "grad_norm": 15.197498321533203,
      "learning_rate": 9.078947368421054e-06,
      "loss": 2.644,
      "step": 138
    },
    {
      "epoch": 0.09150757077024359,
      "grad_norm": 18.117828369140625,
      "learning_rate": 9.144736842105264e-06,
      "loss": 2.6305,
      "step": 139
    },
    {
      "epoch": 0.09216589861751152,
      "grad_norm": 18.889812469482422,
      "learning_rate": 9.210526315789474e-06,
      "loss": 2.6104,
      "step": 140
    },
    {
      "epoch": 0.09282422646477946,
      "grad_norm": 11.54499340057373,
      "learning_rate": 9.276315789473686e-06,
      "loss": 2.4647,
      "step": 141
    },
    {
      "epoch": 0.0934825543120474,
      "grad_norm": 13.149286270141602,
      "learning_rate": 9.342105263157895e-06,
      "loss": 2.5398,
      "step": 142
    },
    {
      "epoch": 0.09414088215931533,
      "grad_norm": 19.268207550048828,
      "learning_rate": 9.407894736842106e-06,
      "loss": 2.5105,
      "step": 143
    },
    {
      "epoch": 0.09479921000658328,
      "grad_norm": 16.708703994750977,
      "learning_rate": 9.473684210526315e-06,
      "loss": 2.7574,
      "step": 144
    },
    {
      "epoch": 0.09545753785385122,
      "grad_norm": 25.16585922241211,
      "learning_rate": 9.539473684210528e-06,
      "loss": 3.0324,
      "step": 145
    },
    {
      "epoch": 0.09611586570111916,
      "grad_norm": 6.649579048156738,
      "learning_rate": 9.605263157894737e-06,
      "loss": 2.3956,
      "step": 146
    },
    {
      "epoch": 0.0967741935483871,
      "grad_norm": 22.21283721923828,
      "learning_rate": 9.671052631578948e-06,
      "loss": 2.7305,
      "step": 147
    },
    {
      "epoch": 0.09743252139565503,
      "grad_norm": 6.8444695472717285,
      "learning_rate": 9.736842105263159e-06,
      "loss": 2.3154,
      "step": 148
    },
    {
      "epoch": 0.09809084924292298,
      "grad_norm": 8.259839057922363,
      "learning_rate": 9.80263157894737e-06,
      "loss": 2.3362,
      "step": 149
    },
    {
      "epoch": 0.09874917709019092,
      "grad_norm": 32.82644271850586,
      "learning_rate": 9.868421052631579e-06,
      "loss": 3.0113,
      "step": 150
    },
    {
      "epoch": 0.09940750493745885,
      "grad_norm": 8.097620010375977,
      "learning_rate": 9.93421052631579e-06,
      "loss": 2.2874,
      "step": 151
    },
    {
      "epoch": 0.10006583278472679,
      "grad_norm": 25.0145320892334,
      "learning_rate": 1e-05,
      "loss": 3.0633,
      "step": 152
    },
    {
      "epoch": 0.10072416063199473,
      "grad_norm": 7.067431926727295,
      "learning_rate": 9.999986796090659e-06,
      "loss": 2.2582,
      "step": 153
    },
    {
      "epoch": 0.10138248847926268,
      "grad_norm": 19.566421508789062,
      "learning_rate": 9.999947184432372e-06,
      "loss": 2.7476,
      "step": 154
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 26.854373931884766,
      "learning_rate": 9.999881165234353e-06,
      "loss": 2.8391,
      "step": 155
    },
    {
      "epoch": 0.10269914417379855,
      "grad_norm": 14.508675575256348,
      "learning_rate": 9.999788738845284e-06,
      "loss": 2.4059,
      "step": 156
    },
    {
      "epoch": 0.10335747202106649,
      "grad_norm": 6.055184841156006,
      "learning_rate": 9.999669905753322e-06,
      "loss": 2.2401,
      "step": 157
    },
    {
      "epoch": 0.10401579986833442,
      "grad_norm": 30.736209869384766,
      "learning_rate": 9.999524666586092e-06,
      "loss": 2.8652,
      "step": 158
    },
    {
      "epoch": 0.10467412771560237,
      "grad_norm": 11.64423942565918,
      "learning_rate": 9.99935302211068e-06,
      "loss": 2.3299,
      "step": 159
    },
    {
      "epoch": 0.10533245556287031,
      "grad_norm": 4.677521228790283,
      "learning_rate": 9.999154973233645e-06,
      "loss": 2.1837,
      "step": 160
    },
    {
      "epoch": 0.10599078341013825,
      "grad_norm": 33.004703521728516,
      "learning_rate": 9.998930521000986e-06,
      "loss": 3.1011,
      "step": 161
    },
    {
      "epoch": 0.10664911125740618,
      "grad_norm": 27.711774826049805,
      "learning_rate": 9.99867966659817e-06,
      "loss": 2.9125,
      "step": 162
    },
    {
      "epoch": 0.10730743910467412,
      "grad_norm": 6.733295440673828,
      "learning_rate": 9.998402411350095e-06,
      "loss": 2.1763,
      "step": 163
    },
    {
      "epoch": 0.10796576695194207,
      "grad_norm": 7.6746392250061035,
      "learning_rate": 9.998098756721103e-06,
      "loss": 2.1818,
      "step": 164
    },
    {
      "epoch": 0.10862409479921001,
      "grad_norm": 42.06002426147461,
      "learning_rate": 9.997768704314967e-06,
      "loss": 3.0359,
      "step": 165
    },
    {
      "epoch": 0.10928242264647794,
      "grad_norm": 9.048055648803711,
      "learning_rate": 9.997412255874879e-06,
      "loss": 2.1545,
      "step": 166
    },
    {
      "epoch": 0.10994075049374588,
      "grad_norm": 51.025814056396484,
      "learning_rate": 9.997029413283444e-06,
      "loss": 2.9959,
      "step": 167
    },
    {
      "epoch": 0.11059907834101383,
      "grad_norm": 9.941596984863281,
      "learning_rate": 9.996620178562668e-06,
      "loss": 2.1681,
      "step": 168
    },
    {
      "epoch": 0.11125740618828177,
      "grad_norm": 28.002424240112305,
      "learning_rate": 9.996184553873954e-06,
      "loss": 2.5966,
      "step": 169
    },
    {
      "epoch": 0.1119157340355497,
      "grad_norm": 7.34149694442749,
      "learning_rate": 9.995722541518079e-06,
      "loss": 2.1236,
      "step": 170
    },
    {
      "epoch": 0.11257406188281764,
      "grad_norm": 42.63494110107422,
      "learning_rate": 9.99523414393519e-06,
      "loss": 2.8373,
      "step": 171
    },
    {
      "epoch": 0.11323238973008558,
      "grad_norm": 10.352226257324219,
      "learning_rate": 9.994719363704792e-06,
      "loss": 2.1438,
      "step": 172
    },
    {
      "epoch": 0.11389071757735353,
      "grad_norm": 23.296600341796875,
      "learning_rate": 9.994178203545729e-06,
      "loss": 2.5137,
      "step": 173
    },
    {
      "epoch": 0.11454904542462147,
      "grad_norm": 32.93185806274414,
      "learning_rate": 9.993610666316172e-06,
      "loss": 2.5866,
      "step": 174
    },
    {
      "epoch": 0.1152073732718894,
      "grad_norm": 30.129467010498047,
      "learning_rate": 9.993016755013607e-06,
      "loss": 2.9144,
      "step": 175
    },
    {
      "epoch": 0.11586570111915734,
      "grad_norm": 40.04563522338867,
      "learning_rate": 9.99239647277481e-06,
      "loss": 3.147,
      "step": 176
    },
    {
      "epoch": 0.11652402896642527,
      "grad_norm": 18.56069564819336,
      "learning_rate": 9.991749822875847e-06,
      "loss": 2.4181,
      "step": 177
    },
    {
      "epoch": 0.11718235681369323,
      "grad_norm": 12.945226669311523,
      "learning_rate": 9.991076808732036e-06,
      "loss": 2.1864,
      "step": 178
    },
    {
      "epoch": 0.11784068466096116,
      "grad_norm": 31.269725799560547,
      "learning_rate": 9.990377433897949e-06,
      "loss": 2.8308,
      "step": 179
    },
    {
      "epoch": 0.1184990125082291,
      "grad_norm": 18.00020408630371,
      "learning_rate": 9.989651702067373e-06,
      "loss": 2.3537,
      "step": 180
    },
    {
      "epoch": 0.11915734035549704,
      "grad_norm": 13.636062622070312,
      "learning_rate": 9.98889961707331e-06,
      "loss": 2.1692,
      "step": 181
    },
    {
      "epoch": 0.11981566820276497,
      "grad_norm": 15.937899589538574,
      "learning_rate": 9.988121182887945e-06,
      "loss": 2.2035,
      "step": 182
    },
    {
      "epoch": 0.12047399605003292,
      "grad_norm": 15.84671401977539,
      "learning_rate": 9.987316403622626e-06,
      "loss": 2.2491,
      "step": 183
    },
    {
      "epoch": 0.12113232389730086,
      "grad_norm": 11.934985160827637,
      "learning_rate": 9.986485283527849e-06,
      "loss": 2.0866,
      "step": 184
    },
    {
      "epoch": 0.1217906517445688,
      "grad_norm": 20.0519962310791,
      "learning_rate": 9.985627826993225e-06,
      "loss": 2.5107,
      "step": 185
    },
    {
      "epoch": 0.12244897959183673,
      "grad_norm": 24.542686462402344,
      "learning_rate": 9.984744038547466e-06,
      "loss": 2.5409,
      "step": 186
    },
    {
      "epoch": 0.12310730743910467,
      "grad_norm": 28.8828067779541,
      "learning_rate": 9.983833922858359e-06,
      "loss": 2.3431,
      "step": 187
    },
    {
      "epoch": 0.12376563528637262,
      "grad_norm": 32.11430358886719,
      "learning_rate": 9.982897484732736e-06,
      "loss": 2.5424,
      "step": 188
    },
    {
      "epoch": 0.12442396313364056,
      "grad_norm": 9.075669288635254,
      "learning_rate": 9.981934729116454e-06,
      "loss": 2.0716,
      "step": 189
    },
    {
      "epoch": 0.1250822909809085,
      "grad_norm": 40.18180465698242,
      "learning_rate": 9.98094566109437e-06,
      "loss": 2.9449,
      "step": 190
    },
    {
      "epoch": 0.12574061882817644,
      "grad_norm": 22.604022979736328,
      "learning_rate": 9.979930285890307e-06,
      "loss": 2.5354,
      "step": 191
    },
    {
      "epoch": 0.12639894667544438,
      "grad_norm": 20.25874900817871,
      "learning_rate": 9.97888860886704e-06,
      "loss": 2.1748,
      "step": 192
    },
    {
      "epoch": 0.12705727452271232,
      "grad_norm": 8.903761863708496,
      "learning_rate": 9.977820635526243e-06,
      "loss": 2.0394,
      "step": 193
    },
    {
      "epoch": 0.12771560236998025,
      "grad_norm": 49.253963470458984,
      "learning_rate": 9.976726371508493e-06,
      "loss": 3.1412,
      "step": 194
    },
    {
      "epoch": 0.1283739302172482,
      "grad_norm": 8.434272766113281,
      "learning_rate": 9.975605822593214e-06,
      "loss": 2.0498,
      "step": 195
    },
    {
      "epoch": 0.12903225806451613,
      "grad_norm": 50.999916076660156,
      "learning_rate": 9.974458994698655e-06,
      "loss": 4.0747,
      "step": 196
    },
    {
      "epoch": 0.12969058591178406,
      "grad_norm": 9.085465431213379,
      "learning_rate": 9.97328589388186e-06,
      "loss": 2.0694,
      "step": 197
    },
    {
      "epoch": 0.130348913759052,
      "grad_norm": 9.905807495117188,
      "learning_rate": 9.972086526338637e-06,
      "loss": 2.0276,
      "step": 198
    },
    {
      "epoch": 0.13100724160631994,
      "grad_norm": 31.87232208251953,
      "learning_rate": 9.970860898403522e-06,
      "loss": 2.5992,
      "step": 199
    },
    {
      "epoch": 0.1316655694535879,
      "grad_norm": 11.536282539367676,
      "learning_rate": 9.969609016549746e-06,
      "loss": 1.9863,
      "step": 200
    },
    {
      "epoch": 0.13232389730085584,
      "grad_norm": 34.1309700012207,
      "learning_rate": 9.968330887389205e-06,
      "loss": 2.4569,
      "step": 201
    },
    {
      "epoch": 0.13298222514812377,
      "grad_norm": 31.805017471313477,
      "learning_rate": 9.967026517672418e-06,
      "loss": 2.6445,
      "step": 202
    },
    {
      "epoch": 0.1336405529953917,
      "grad_norm": 23.43123435974121,
      "learning_rate": 9.965695914288496e-06,
      "loss": 2.4285,
      "step": 203
    },
    {
      "epoch": 0.13429888084265965,
      "grad_norm": 10.112924575805664,
      "learning_rate": 9.964339084265106e-06,
      "loss": 1.9951,
      "step": 204
    },
    {
      "epoch": 0.13495720868992758,
      "grad_norm": 14.611676216125488,
      "learning_rate": 9.962956034768436e-06,
      "loss": 1.9992,
      "step": 205
    },
    {
      "epoch": 0.13561553653719552,
      "grad_norm": 9.05212688446045,
      "learning_rate": 9.961546773103146e-06,
      "loss": 1.9709,
      "step": 206
    },
    {
      "epoch": 0.13627386438446346,
      "grad_norm": 24.57251739501953,
      "learning_rate": 9.960111306712341e-06,
      "loss": 2.2978,
      "step": 207
    },
    {
      "epoch": 0.1369321922317314,
      "grad_norm": 27.70526123046875,
      "learning_rate": 9.95864964317753e-06,
      "loss": 2.4963,
      "step": 208
    },
    {
      "epoch": 0.13759052007899933,
      "grad_norm": 26.620258331298828,
      "learning_rate": 9.957161790218582e-06,
      "loss": 2.2983,
      "step": 209
    },
    {
      "epoch": 0.1382488479262673,
      "grad_norm": 14.720450401306152,
      "learning_rate": 9.955647755693687e-06,
      "loss": 2.0056,
      "step": 210
    },
    {
      "epoch": 0.13890717577353523,
      "grad_norm": 10.417869567871094,
      "learning_rate": 9.954107547599314e-06,
      "loss": 1.9488,
      "step": 211
    },
    {
      "epoch": 0.13956550362080317,
      "grad_norm": 7.895816326141357,
      "learning_rate": 9.95254117407017e-06,
      "loss": 1.9269,
      "step": 212
    },
    {
      "epoch": 0.1402238314680711,
      "grad_norm": 62.265724182128906,
      "learning_rate": 9.950948643379158e-06,
      "loss": 3.7681,
      "step": 213
    },
    {
      "epoch": 0.14088215931533904,
      "grad_norm": 35.7304573059082,
      "learning_rate": 9.94932996393733e-06,
      "loss": 2.327,
      "step": 214
    },
    {
      "epoch": 0.14154048716260698,
      "grad_norm": 25.612716674804688,
      "learning_rate": 9.947685144293847e-06,
      "loss": 2.1254,
      "step": 215
    },
    {
      "epoch": 0.1421988150098749,
      "grad_norm": 42.057090759277344,
      "learning_rate": 9.946014193135924e-06,
      "loss": 2.4516,
      "step": 216
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 58.566036224365234,
      "learning_rate": 9.9443171192888e-06,
      "loss": 2.8592,
      "step": 217
    },
    {
      "epoch": 0.14351547070441079,
      "grad_norm": 25.131547927856445,
      "learning_rate": 9.942593931715676e-06,
      "loss": 2.1828,
      "step": 218
    },
    {
      "epoch": 0.14417379855167872,
      "grad_norm": 49.21752166748047,
      "learning_rate": 9.940844639517676e-06,
      "loss": 2.6992,
      "step": 219
    },
    {
      "epoch": 0.1448321263989467,
      "grad_norm": 52.9821662902832,
      "learning_rate": 9.939069251933804e-06,
      "loss": 2.8382,
      "step": 220
    },
    {
      "epoch": 0.14549045424621462,
      "grad_norm": 32.97377395629883,
      "learning_rate": 9.937267778340878e-06,
      "loss": 2.485,
      "step": 221
    },
    {
      "epoch": 0.14614878209348256,
      "grad_norm": 27.021629333496094,
      "learning_rate": 9.935440228253495e-06,
      "loss": 2.5583,
      "step": 222
    },
    {
      "epoch": 0.1468071099407505,
      "grad_norm": 19.070266723632812,
      "learning_rate": 9.93358661132398e-06,
      "loss": 2.283,
      "step": 223
    },
    {
      "epoch": 0.14746543778801843,
      "grad_norm": 35.0984001159668,
      "learning_rate": 9.931706937342328e-06,
      "loss": 2.5836,
      "step": 224
    },
    {
      "epoch": 0.14812376563528637,
      "grad_norm": 12.054112434387207,
      "learning_rate": 9.929801216236155e-06,
      "loss": 2.1151,
      "step": 225
    },
    {
      "epoch": 0.1487820934825543,
      "grad_norm": 18.207761764526367,
      "learning_rate": 9.927869458070653e-06,
      "loss": 2.0628,
      "step": 226
    },
    {
      "epoch": 0.14944042132982224,
      "grad_norm": 17.57317352294922,
      "learning_rate": 9.92591167304852e-06,
      "loss": 2.2648,
      "step": 227
    },
    {
      "epoch": 0.15009874917709018,
      "grad_norm": 18.944894790649414,
      "learning_rate": 9.923927871509928e-06,
      "loss": 2.2785,
      "step": 228
    },
    {
      "epoch": 0.15075707702435814,
      "grad_norm": 19.83168601989746,
      "learning_rate": 9.921918063932448e-06,
      "loss": 2.1008,
      "step": 229
    },
    {
      "epoch": 0.15141540487162608,
      "grad_norm": 19.669607162475586,
      "learning_rate": 9.919882260931007e-06,
      "loss": 2.2881,
      "step": 230
    },
    {
      "epoch": 0.15207373271889402,
      "grad_norm": 21.108827590942383,
      "learning_rate": 9.91782047325783e-06,
      "loss": 2.0486,
      "step": 231
    },
    {
      "epoch": 0.15273206056616195,
      "grad_norm": 16.62952423095703,
      "learning_rate": 9.915732711802377e-06,
      "loss": 1.9978,
      "step": 232
    },
    {
      "epoch": 0.1533903884134299,
      "grad_norm": 17.46816635131836,
      "learning_rate": 9.913618987591297e-06,
      "loss": 2.1612,
      "step": 233
    },
    {
      "epoch": 0.15404871626069783,
      "grad_norm": 25.179174423217773,
      "learning_rate": 9.911479311788356e-06,
      "loss": 2.3486,
      "step": 234
    },
    {
      "epoch": 0.15470704410796576,
      "grad_norm": 20.70836639404297,
      "learning_rate": 9.90931369569439e-06,
      "loss": 2.1356,
      "step": 235
    },
    {
      "epoch": 0.1553653719552337,
      "grad_norm": 13.692814826965332,
      "learning_rate": 9.907122150747237e-06,
      "loss": 1.9543,
      "step": 236
    },
    {
      "epoch": 0.15602369980250164,
      "grad_norm": 32.6320686340332,
      "learning_rate": 9.904904688521683e-06,
      "loss": 2.4256,
      "step": 237
    },
    {
      "epoch": 0.15668202764976957,
      "grad_norm": 6.658361911773682,
      "learning_rate": 9.902661320729396e-06,
      "loss": 1.8526,
      "step": 238
    },
    {
      "epoch": 0.15734035549703754,
      "grad_norm": 26.28474235534668,
      "learning_rate": 9.900392059218864e-06,
      "loss": 2.2789,
      "step": 239
    },
    {
      "epoch": 0.15799868334430547,
      "grad_norm": 5.194279193878174,
      "learning_rate": 9.898096915975337e-06,
      "loss": 1.8193,
      "step": 240
    },
    {
      "epoch": 0.1586570111915734,
      "grad_norm": 5.189924240112305,
      "learning_rate": 9.895775903120761e-06,
      "loss": 1.8169,
      "step": 241
    },
    {
      "epoch": 0.15931533903884135,
      "grad_norm": 31.77667999267578,
      "learning_rate": 9.893429032913713e-06,
      "loss": 2.1153,
      "step": 242
    },
    {
      "epoch": 0.15997366688610928,
      "grad_norm": 24.15814208984375,
      "learning_rate": 9.891056317749339e-06,
      "loss": 2.0935,
      "step": 243
    },
    {
      "epoch": 0.16063199473337722,
      "grad_norm": 39.949161529541016,
      "learning_rate": 9.888657770159283e-06,
      "loss": 2.4115,
      "step": 244
    },
    {
      "epoch": 0.16129032258064516,
      "grad_norm": 43.40283203125,
      "learning_rate": 9.886233402811626e-06,
      "loss": 2.9443,
      "step": 245
    },
    {
      "epoch": 0.1619486504279131,
      "grad_norm": 25.271543502807617,
      "learning_rate": 9.883783228510824e-06,
      "loss": 2.0829,
      "step": 246
    },
    {
      "epoch": 0.16260697827518103,
      "grad_norm": 54.56127166748047,
      "learning_rate": 9.881307260197624e-06,
      "loss": 2.7126,
      "step": 247
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 20.430238723754883,
      "learning_rate": 9.878805510949012e-06,
      "loss": 2.0614,
      "step": 248
    },
    {
      "epoch": 0.16392363396971693,
      "grad_norm": 36.39876174926758,
      "learning_rate": 9.876277993978137e-06,
      "loss": 2.9793,
      "step": 249
    },
    {
      "epoch": 0.16458196181698487,
      "grad_norm": 35.25171661376953,
      "learning_rate": 9.873724722634238e-06,
      "loss": 2.6844,
      "step": 250
    },
    {
      "epoch": 0.1652402896642528,
      "grad_norm": 59.97026443481445,
      "learning_rate": 9.871145710402583e-06,
      "loss": 2.6789,
      "step": 251
    },
    {
      "epoch": 0.16589861751152074,
      "grad_norm": 38.355899810791016,
      "learning_rate": 9.86854097090439e-06,
      "loss": 2.7109,
      "step": 252
    },
    {
      "epoch": 0.16655694535878868,
      "grad_norm": 31.718753814697266,
      "learning_rate": 9.865910517896754e-06,
      "loss": 2.5089,
      "step": 253
    },
    {
      "epoch": 0.1672152732060566,
      "grad_norm": 43.155799865722656,
      "learning_rate": 9.863254365272582e-06,
      "loss": 2.7138,
      "step": 254
    },
    {
      "epoch": 0.16787360105332455,
      "grad_norm": 31.939199447631836,
      "learning_rate": 9.860572527060514e-06,
      "loss": 2.3752,
      "step": 255
    },
    {
      "epoch": 0.1685319289005925,
      "grad_norm": 23.21610450744629,
      "learning_rate": 9.857865017424849e-06,
      "loss": 2.5724,
      "step": 256
    },
    {
      "epoch": 0.16919025674786042,
      "grad_norm": 23.874629974365234,
      "learning_rate": 9.855131850665468e-06,
      "loss": 2.2154,
      "step": 257
    },
    {
      "epoch": 0.16984858459512836,
      "grad_norm": 17.745513916015625,
      "learning_rate": 9.85237304121777e-06,
      "loss": 2.001,
      "step": 258
    },
    {
      "epoch": 0.17050691244239632,
      "grad_norm": 29.872953414916992,
      "learning_rate": 9.849588603652584e-06,
      "loss": 2.5023,
      "step": 259
    },
    {
      "epoch": 0.17116524028966426,
      "grad_norm": 22.992523193359375,
      "learning_rate": 9.84677855267609e-06,
      "loss": 2.3327,
      "step": 260
    },
    {
      "epoch": 0.1718235681369322,
      "grad_norm": 22.689180374145508,
      "learning_rate": 9.843942903129753e-06,
      "loss": 2.2864,
      "step": 261
    },
    {
      "epoch": 0.17248189598420013,
      "grad_norm": 24.88196563720703,
      "learning_rate": 9.841081669990238e-06,
      "loss": 2.4522,
      "step": 262
    },
    {
      "epoch": 0.17314022383146807,
      "grad_norm": 20.760709762573242,
      "learning_rate": 9.838194868369328e-06,
      "loss": 2.2386,
      "step": 263
    },
    {
      "epoch": 0.173798551678736,
      "grad_norm": 17.086139678955078,
      "learning_rate": 9.835282513513853e-06,
      "loss": 2.2527,
      "step": 264
    },
    {
      "epoch": 0.17445687952600394,
      "grad_norm": 10.578505516052246,
      "learning_rate": 9.832344620805598e-06,
      "loss": 1.9104,
      "step": 265
    },
    {
      "epoch": 0.17511520737327188,
      "grad_norm": 18.667150497436523,
      "learning_rate": 9.829381205761233e-06,
      "loss": 2.3084,
      "step": 266
    },
    {
      "epoch": 0.17577353522053982,
      "grad_norm": 12.968279838562012,
      "learning_rate": 9.82639228403222e-06,
      "loss": 1.8454,
      "step": 267
    },
    {
      "epoch": 0.17643186306780778,
      "grad_norm": 17.34552574157715,
      "learning_rate": 9.823377871404743e-06,
      "loss": 2.054,
      "step": 268
    },
    {
      "epoch": 0.17709019091507572,
      "grad_norm": 26.121898651123047,
      "learning_rate": 9.820337983799613e-06,
      "loss": 2.3709,
      "step": 269
    },
    {
      "epoch": 0.17774851876234365,
      "grad_norm": 18.047449111938477,
      "learning_rate": 9.817272637272189e-06,
      "loss": 2.0535,
      "step": 270
    },
    {
      "epoch": 0.1784068466096116,
      "grad_norm": 30.610660552978516,
      "learning_rate": 9.814181848012297e-06,
      "loss": 2.4982,
      "step": 271
    },
    {
      "epoch": 0.17906517445687953,
      "grad_norm": 44.485565185546875,
      "learning_rate": 9.811065632344135e-06,
      "loss": 2.5093,
      "step": 272
    },
    {
      "epoch": 0.17972350230414746,
      "grad_norm": 33.52333450317383,
      "learning_rate": 9.807924006726194e-06,
      "loss": 2.4931,
      "step": 273
    },
    {
      "epoch": 0.1803818301514154,
      "grad_norm": 26.848894119262695,
      "learning_rate": 9.804756987751172e-06,
      "loss": 2.3343,
      "step": 274
    },
    {
      "epoch": 0.18104015799868334,
      "grad_norm": 20.274293899536133,
      "learning_rate": 9.801564592145881e-06,
      "loss": 2.1574,
      "step": 275
    },
    {
      "epoch": 0.18169848584595127,
      "grad_norm": 37.54410934448242,
      "learning_rate": 9.798346836771162e-06,
      "loss": 2.5151,
      "step": 276
    },
    {
      "epoch": 0.1823568136932192,
      "grad_norm": 19.513702392578125,
      "learning_rate": 9.795103738621794e-06,
      "loss": 1.9905,
      "step": 277
    },
    {
      "epoch": 0.18301514154048718,
      "grad_norm": 23.663230895996094,
      "learning_rate": 9.791835314826408e-06,
      "loss": 2.1899,
      "step": 278
    },
    {
      "epoch": 0.1836734693877551,
      "grad_norm": 14.333423614501953,
      "learning_rate": 9.788541582647391e-06,
      "loss": 2.0402,
      "step": 279
    },
    {
      "epoch": 0.18433179723502305,
      "grad_norm": 15.585783958435059,
      "learning_rate": 9.785222559480802e-06,
      "loss": 1.9095,
      "step": 280
    },
    {
      "epoch": 0.18499012508229098,
      "grad_norm": 25.11286163330078,
      "learning_rate": 9.78187826285627e-06,
      "loss": 2.2925,
      "step": 281
    },
    {
      "epoch": 0.18564845292955892,
      "grad_norm": 20.195846557617188,
      "learning_rate": 9.778508710436914e-06,
      "loss": 2.0463,
      "step": 282
    },
    {
      "epoch": 0.18630678077682686,
      "grad_norm": 16.620216369628906,
      "learning_rate": 9.77511392001924e-06,
      "loss": 1.9228,
      "step": 283
    },
    {
      "epoch": 0.1869651086240948,
      "grad_norm": 13.07235050201416,
      "learning_rate": 9.771693909533046e-06,
      "loss": 2.0241,
      "step": 284
    },
    {
      "epoch": 0.18762343647136273,
      "grad_norm": 21.852563858032227,
      "learning_rate": 9.768248697041338e-06,
      "loss": 2.14,
      "step": 285
    },
    {
      "epoch": 0.18828176431863067,
      "grad_norm": 14.981077194213867,
      "learning_rate": 9.764778300740225e-06,
      "loss": 1.934,
      "step": 286
    },
    {
      "epoch": 0.1889400921658986,
      "grad_norm": 12.89024829864502,
      "learning_rate": 9.761282738958828e-06,
      "loss": 1.8495,
      "step": 287
    },
    {
      "epoch": 0.18959842001316657,
      "grad_norm": 34.63429641723633,
      "learning_rate": 9.757762030159175e-06,
      "loss": 2.1232,
      "step": 288
    },
    {
      "epoch": 0.1902567478604345,
      "grad_norm": 46.757904052734375,
      "learning_rate": 9.754216192936119e-06,
      "loss": 2.7054,
      "step": 289
    },
    {
      "epoch": 0.19091507570770244,
      "grad_norm": 13.567793846130371,
      "learning_rate": 9.750645246017222e-06,
      "loss": 1.8148,
      "step": 290
    },
    {
      "epoch": 0.19157340355497038,
      "grad_norm": 35.85970687866211,
      "learning_rate": 9.747049208262668e-06,
      "loss": 2.6575,
      "step": 291
    },
    {
      "epoch": 0.19223173140223832,
      "grad_norm": 30.025279998779297,
      "learning_rate": 9.743428098665161e-06,
      "loss": 2.0411,
      "step": 292
    },
    {
      "epoch": 0.19289005924950625,
      "grad_norm": 40.355316162109375,
      "learning_rate": 9.739781936349823e-06,
      "loss": 2.6479,
      "step": 293
    },
    {
      "epoch": 0.1935483870967742,
      "grad_norm": 31.76351547241211,
      "learning_rate": 9.73611074057409e-06,
      "loss": 2.2894,
      "step": 294
    },
    {
      "epoch": 0.19420671494404212,
      "grad_norm": 31.87701416015625,
      "learning_rate": 9.732414530727618e-06,
      "loss": 2.0687,
      "step": 295
    },
    {
      "epoch": 0.19486504279131006,
      "grad_norm": 23.955936431884766,
      "learning_rate": 9.728693326332172e-06,
      "loss": 2.0631,
      "step": 296
    },
    {
      "epoch": 0.19552337063857803,
      "grad_norm": 40.41564178466797,
      "learning_rate": 9.724947147041536e-06,
      "loss": 2.3426,
      "step": 297
    },
    {
      "epoch": 0.19618169848584596,
      "grad_norm": 11.366352081298828,
      "learning_rate": 9.72117601264139e-06,
      "loss": 1.7598,
      "step": 298
    },
    {
      "epoch": 0.1968400263331139,
      "grad_norm": 11.184182167053223,
      "learning_rate": 9.717379943049221e-06,
      "loss": 1.769,
      "step": 299
    },
    {
      "epoch": 0.19749835418038184,
      "grad_norm": 21.926050186157227,
      "learning_rate": 9.713558958314215e-06,
      "loss": 2.2134,
      "step": 300
    },
    {
      "epoch": 0.19815668202764977,
      "grad_norm": 18.26495361328125,
      "learning_rate": 9.709713078617145e-06,
      "loss": 1.9739,
      "step": 301
    },
    {
      "epoch": 0.1988150098749177,
      "grad_norm": 29.488245010375977,
      "learning_rate": 9.70584232427027e-06,
      "loss": 2.2407,
      "step": 302
    },
    {
      "epoch": 0.19947333772218565,
      "grad_norm": 30.24107551574707,
      "learning_rate": 9.701946715717223e-06,
      "loss": 2.2244,
      "step": 303
    },
    {
      "epoch": 0.20013166556945358,
      "grad_norm": 28.356605529785156,
      "learning_rate": 9.698026273532914e-06,
      "loss": 2.0816,
      "step": 304
    },
    {
      "epoch": 0.20078999341672152,
      "grad_norm": 17.312664031982422,
      "learning_rate": 9.694081018423404e-06,
      "loss": 1.8111,
      "step": 305
    },
    {
      "epoch": 0.20144832126398945,
      "grad_norm": 23.712583541870117,
      "learning_rate": 9.690110971225812e-06,
      "loss": 1.9419,
      "step": 306
    },
    {
      "epoch": 0.20210664911125742,
      "grad_norm": 37.248931884765625,
      "learning_rate": 9.686116152908194e-06,
      "loss": 2.4567,
      "step": 307
    },
    {
      "epoch": 0.20276497695852536,
      "grad_norm": 21.840530395507812,
      "learning_rate": 9.682096584569439e-06,
      "loss": 1.914,
      "step": 308
    },
    {
      "epoch": 0.2034233048057933,
      "grad_norm": 25.627559661865234,
      "learning_rate": 9.67805228743915e-06,
      "loss": 2.1009,
      "step": 309
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 10.638587951660156,
      "learning_rate": 9.673983282877543e-06,
      "loss": 1.7495,
      "step": 310
    },
    {
      "epoch": 0.20473996050032917,
      "grad_norm": 26.72202491760254,
      "learning_rate": 9.669889592375324e-06,
      "loss": 2.181,
      "step": 311
    },
    {
      "epoch": 0.2053982883475971,
      "grad_norm": 7.466237545013428,
      "learning_rate": 9.66577123755358e-06,
      "loss": 1.7053,
      "step": 312
    },
    {
      "epoch": 0.20605661619486504,
      "grad_norm": 31.610509872436523,
      "learning_rate": 9.661628240163667e-06,
      "loss": 2.2207,
      "step": 313
    },
    {
      "epoch": 0.20671494404213298,
      "grad_norm": 5.757580280303955,
      "learning_rate": 9.657460622087088e-06,
      "loss": 1.6784,
      "step": 314
    },
    {
      "epoch": 0.2073732718894009,
      "grad_norm": 50.62678909301758,
      "learning_rate": 9.653268405335381e-06,
      "loss": 2.6117,
      "step": 315
    },
    {
      "epoch": 0.20803159973666885,
      "grad_norm": 29.72972297668457,
      "learning_rate": 9.649051612050009e-06,
      "loss": 2.1859,
      "step": 316
    },
    {
      "epoch": 0.2086899275839368,
      "grad_norm": 37.989654541015625,
      "learning_rate": 9.644810264502233e-06,
      "loss": 2.1795,
      "step": 317
    },
    {
      "epoch": 0.20934825543120475,
      "grad_norm": 18.05922508239746,
      "learning_rate": 9.640544385093003e-06,
      "loss": 2.108,
      "step": 318
    },
    {
      "epoch": 0.21000658327847269,
      "grad_norm": 25.82298469543457,
      "learning_rate": 9.636253996352829e-06,
      "loss": 2.0827,
      "step": 319
    },
    {
      "epoch": 0.21066491112574062,
      "grad_norm": 26.841535568237305,
      "learning_rate": 9.631939120941675e-06,
      "loss": 2.3739,
      "step": 320
    },
    {
      "epoch": 0.21132323897300856,
      "grad_norm": 32.19401168823242,
      "learning_rate": 9.627599781648834e-06,
      "loss": 2.3072,
      "step": 321
    },
    {
      "epoch": 0.2119815668202765,
      "grad_norm": 10.416670799255371,
      "learning_rate": 9.623236001392795e-06,
      "loss": 1.7984,
      "step": 322
    },
    {
      "epoch": 0.21263989466754443,
      "grad_norm": 12.398447036743164,
      "learning_rate": 9.618847803221148e-06,
      "loss": 1.7701,
      "step": 323
    },
    {
      "epoch": 0.21329822251481237,
      "grad_norm": 16.59376335144043,
      "learning_rate": 9.614435210310437e-06,
      "loss": 1.9801,
      "step": 324
    },
    {
      "epoch": 0.2139565503620803,
      "grad_norm": 32.132713317871094,
      "learning_rate": 9.609998245966056e-06,
      "loss": 2.4445,
      "step": 325
    },
    {
      "epoch": 0.21461487820934824,
      "grad_norm": 13.852622985839844,
      "learning_rate": 9.605536933622113e-06,
      "loss": 1.9284,
      "step": 326
    },
    {
      "epoch": 0.2152732060566162,
      "grad_norm": 9.929463386535645,
      "learning_rate": 9.601051296841315e-06,
      "loss": 1.8073,
      "step": 327
    },
    {
      "epoch": 0.21593153390388414,
      "grad_norm": 14.43867301940918,
      "learning_rate": 9.59654135931484e-06,
      "loss": 1.9032,
      "step": 328
    },
    {
      "epoch": 0.21658986175115208,
      "grad_norm": 25.325061798095703,
      "learning_rate": 9.592007144862205e-06,
      "loss": 1.9662,
      "step": 329
    },
    {
      "epoch": 0.21724818959842002,
      "grad_norm": 39.82174301147461,
      "learning_rate": 9.587448677431158e-06,
      "loss": 2.2453,
      "step": 330
    },
    {
      "epoch": 0.21790651744568795,
      "grad_norm": 22.57803726196289,
      "learning_rate": 9.582865981097533e-06,
      "loss": 2.2062,
      "step": 331
    },
    {
      "epoch": 0.2185648452929559,
      "grad_norm": 28.47216796875,
      "learning_rate": 9.578259080065133e-06,
      "loss": 2.2143,
      "step": 332
    },
    {
      "epoch": 0.21922317314022383,
      "grad_norm": 20.064619064331055,
      "learning_rate": 9.573627998665599e-06,
      "loss": 1.9148,
      "step": 333
    },
    {
      "epoch": 0.21988150098749176,
      "grad_norm": 24.379150390625,
      "learning_rate": 9.568972761358285e-06,
      "loss": 1.9923,
      "step": 334
    },
    {
      "epoch": 0.2205398288347597,
      "grad_norm": 14.762144088745117,
      "learning_rate": 9.564293392730118e-06,
      "loss": 1.7308,
      "step": 335
    },
    {
      "epoch": 0.22119815668202766,
      "grad_norm": 37.550418853759766,
      "learning_rate": 9.559589917495488e-06,
      "loss": 2.5011,
      "step": 336
    },
    {
      "epoch": 0.2218564845292956,
      "grad_norm": 41.323020935058594,
      "learning_rate": 9.554862360496096e-06,
      "loss": 2.5786,
      "step": 337
    },
    {
      "epoch": 0.22251481237656354,
      "grad_norm": 30.938894271850586,
      "learning_rate": 9.550110746700835e-06,
      "loss": 1.9632,
      "step": 338
    },
    {
      "epoch": 0.22317314022383147,
      "grad_norm": 22.821758270263672,
      "learning_rate": 9.545335101205659e-06,
      "loss": 1.839,
      "step": 339
    },
    {
      "epoch": 0.2238314680710994,
      "grad_norm": 21.524696350097656,
      "learning_rate": 9.540535449233441e-06,
      "loss": 1.848,
      "step": 340
    },
    {
      "epoch": 0.22448979591836735,
      "grad_norm": 30.30607032775879,
      "learning_rate": 9.53571181613385e-06,
      "loss": 2.1192,
      "step": 341
    },
    {
      "epoch": 0.22514812376563528,
      "grad_norm": 33.04135513305664,
      "learning_rate": 9.530864227383212e-06,
      "loss": 2.1776,
      "step": 342
    },
    {
      "epoch": 0.22580645161290322,
      "grad_norm": 36.66232681274414,
      "learning_rate": 9.525992708584375e-06,
      "loss": 2.0862,
      "step": 343
    },
    {
      "epoch": 0.22646477946017116,
      "grad_norm": 25.221908569335938,
      "learning_rate": 9.52109728546658e-06,
      "loss": 1.9736,
      "step": 344
    },
    {
      "epoch": 0.2271231073074391,
      "grad_norm": 48.238162994384766,
      "learning_rate": 9.516177983885309e-06,
      "loss": 2.3557,
      "step": 345
    },
    {
      "epoch": 0.22778143515470706,
      "grad_norm": 9.883174896240234,
      "learning_rate": 9.511234829822172e-06,
      "loss": 1.6828,
      "step": 346
    },
    {
      "epoch": 0.228439763001975,
      "grad_norm": 15.910589218139648,
      "learning_rate": 9.506267849384752e-06,
      "loss": 1.9119,
      "step": 347
    },
    {
      "epoch": 0.22909809084924293,
      "grad_norm": 12.283720016479492,
      "learning_rate": 9.50127706880647e-06,
      "loss": 1.7267,
      "step": 348
    },
    {
      "epoch": 0.22975641869651087,
      "grad_norm": 29.258485794067383,
      "learning_rate": 9.496262514446455e-06,
      "loss": 2.0328,
      "step": 349
    },
    {
      "epoch": 0.2304147465437788,
      "grad_norm": 26.916688919067383,
      "learning_rate": 9.491224212789393e-06,
      "loss": 2.2605,
      "step": 350
    },
    {
      "epoch": 0.23107307439104674,
      "grad_norm": 9.735316276550293,
      "learning_rate": 9.486162190445397e-06,
      "loss": 1.6855,
      "step": 351
    },
    {
      "epoch": 0.23173140223831468,
      "grad_norm": 15.08314323425293,
      "learning_rate": 9.48107647414986e-06,
      "loss": 1.7675,
      "step": 352
    },
    {
      "epoch": 0.2323897300855826,
      "grad_norm": 27.234437942504883,
      "learning_rate": 9.475967090763316e-06,
      "loss": 2.2879,
      "step": 353
    },
    {
      "epoch": 0.23304805793285055,
      "grad_norm": 17.46415138244629,
      "learning_rate": 9.470834067271301e-06,
      "loss": 1.8132,
      "step": 354
    },
    {
      "epoch": 0.2337063857801185,
      "grad_norm": 24.01343536376953,
      "learning_rate": 9.465677430784203e-06,
      "loss": 2.108,
      "step": 355
    },
    {
      "epoch": 0.23436471362738645,
      "grad_norm": 27.353593826293945,
      "learning_rate": 9.460497208537129e-06,
      "loss": 2.1364,
      "step": 356
    },
    {
      "epoch": 0.2350230414746544,
      "grad_norm": 28.16156005859375,
      "learning_rate": 9.45529342788975e-06,
      "loss": 2.0793,
      "step": 357
    },
    {
      "epoch": 0.23568136932192232,
      "grad_norm": 6.193238258361816,
      "learning_rate": 9.450066116326169e-06,
      "loss": 1.6369,
      "step": 358
    },
    {
      "epoch": 0.23633969716919026,
      "grad_norm": 4.689695358276367,
      "learning_rate": 9.444815301454761e-06,
      "loss": 1.6199,
      "step": 359
    },
    {
      "epoch": 0.2369980250164582,
      "grad_norm": 37.34064865112305,
      "learning_rate": 9.439541011008042e-06,
      "loss": 2.314,
      "step": 360
    },
    {
      "epoch": 0.23765635286372613,
      "grad_norm": 51.948734283447266,
      "learning_rate": 9.434243272842514e-06,
      "loss": 2.7576,
      "step": 361
    },
    {
      "epoch": 0.23831468071099407,
      "grad_norm": 59.44189453125,
      "learning_rate": 9.428922114938516e-06,
      "loss": 2.7629,
      "step": 362
    },
    {
      "epoch": 0.238973008558262,
      "grad_norm": 37.430076599121094,
      "learning_rate": 9.423577565400084e-06,
      "loss": 2.2103,
      "step": 363
    },
    {
      "epoch": 0.23963133640552994,
      "grad_norm": 3.6985714435577393,
      "learning_rate": 9.418209652454797e-06,
      "loss": 1.5986,
      "step": 364
    },
    {
      "epoch": 0.24028966425279788,
      "grad_norm": 31.8156795501709,
      "learning_rate": 9.41281840445363e-06,
      "loss": 2.17,
      "step": 365
    },
    {
      "epoch": 0.24094799210006584,
      "grad_norm": 23.21010971069336,
      "learning_rate": 9.407403849870801e-06,
      "loss": 1.8452,
      "step": 366
    },
    {
      "epoch": 0.24160631994733378,
      "grad_norm": 24.01224708557129,
      "learning_rate": 9.401966017303629e-06,
      "loss": 2.0152,
      "step": 367
    },
    {
      "epoch": 0.24226464779460172,
      "grad_norm": 6.4297871589660645,
      "learning_rate": 9.396504935472369e-06,
      "loss": 1.6179,
      "step": 368
    },
    {
      "epoch": 0.24292297564186965,
      "grad_norm": 34.57278060913086,
      "learning_rate": 9.391020633220075e-06,
      "loss": 2.2659,
      "step": 369
    },
    {
      "epoch": 0.2435813034891376,
      "grad_norm": 5.434507369995117,
      "learning_rate": 9.38551313951244e-06,
      "loss": 1.6091,
      "step": 370
    },
    {
      "epoch": 0.24423963133640553,
      "grad_norm": 18.509227752685547,
      "learning_rate": 9.379982483437637e-06,
      "loss": 1.7731,
      "step": 371
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 26.2936954498291,
      "learning_rate": 9.374428694206185e-06,
      "loss": 1.9343,
      "step": 372
    },
    {
      "epoch": 0.2455562870309414,
      "grad_norm": 33.26068878173828,
      "learning_rate": 9.368851801150775e-06,
      "loss": 1.982,
      "step": 373
    },
    {
      "epoch": 0.24621461487820934,
      "grad_norm": 26.693899154663086,
      "learning_rate": 9.36325183372612e-06,
      "loss": 2.0484,
      "step": 374
    },
    {
      "epoch": 0.2468729427254773,
      "grad_norm": 25.72292137145996,
      "learning_rate": 9.35762882150881e-06,
      "loss": 2.0187,
      "step": 375
    },
    {
      "epoch": 0.24753127057274524,
      "grad_norm": 6.818916320800781,
      "learning_rate": 9.351982794197138e-06,
      "loss": 1.6019,
      "step": 376
    },
    {
      "epoch": 0.24818959842001317,
      "grad_norm": 11.446715354919434,
      "learning_rate": 9.346313781610958e-06,
      "loss": 1.6124,
      "step": 377
    },
    {
      "epoch": 0.2488479262672811,
      "grad_norm": 73.9709701538086,
      "learning_rate": 9.340621813691522e-06,
      "loss": 4.0148,
      "step": 378
    },
    {
      "epoch": 0.24950625411454905,
      "grad_norm": 64.94288635253906,
      "learning_rate": 9.33490692050132e-06,
      "loss": 2.6612,
      "step": 379
    },
    {
      "epoch": 0.250164581961817,
      "grad_norm": 14.088906288146973,
      "learning_rate": 9.329169132223926e-06,
      "loss": 1.7793,
      "step": 380
    },
    {
      "epoch": 0.25082290980908495,
      "grad_norm": 31.798812866210938,
      "learning_rate": 9.323408479163834e-06,
      "loss": 2.1793,
      "step": 381
    },
    {
      "epoch": 0.2514812376563529,
      "grad_norm": 23.088951110839844,
      "learning_rate": 9.317624991746301e-06,
      "loss": 2.1337,
      "step": 382
    },
    {
      "epoch": 0.2521395655036208,
      "grad_norm": 30.17809295654297,
      "learning_rate": 9.311818700517183e-06,
      "loss": 2.1171,
      "step": 383
    },
    {
      "epoch": 0.25279789335088876,
      "grad_norm": 49.55208969116211,
      "learning_rate": 9.305989636142779e-06,
      "loss": 2.507,
      "step": 384
    },
    {
      "epoch": 0.2534562211981567,
      "grad_norm": 16.73456573486328,
      "learning_rate": 9.300137829409662e-06,
      "loss": 1.6757,
      "step": 385
    },
    {
      "epoch": 0.25411454904542463,
      "grad_norm": 7.1599626541137695,
      "learning_rate": 9.294263311224524e-06,
      "loss": 1.6174,
      "step": 386
    },
    {
      "epoch": 0.25477287689269257,
      "grad_norm": 16.62588119506836,
      "learning_rate": 9.288366112614007e-06,
      "loss": 1.8882,
      "step": 387
    },
    {
      "epoch": 0.2554312047399605,
      "grad_norm": 17.297183990478516,
      "learning_rate": 9.282446264724539e-06,
      "loss": 1.8516,
      "step": 388
    },
    {
      "epoch": 0.25608953258722844,
      "grad_norm": 10.219428062438965,
      "learning_rate": 9.276503798822178e-06,
      "loss": 1.6951,
      "step": 389
    },
    {
      "epoch": 0.2567478604344964,
      "grad_norm": 4.937880039215088,
      "learning_rate": 9.270538746292432e-06,
      "loss": 1.6002,
      "step": 390
    },
    {
      "epoch": 0.2574061882817643,
      "grad_norm": 23.282848358154297,
      "learning_rate": 9.26455113864011e-06,
      "loss": 1.8611,
      "step": 391
    },
    {
      "epoch": 0.25806451612903225,
      "grad_norm": 4.916152000427246,
      "learning_rate": 9.258541007489142e-06,
      "loss": 1.5813,
      "step": 392
    },
    {
      "epoch": 0.2587228439763002,
      "grad_norm": 28.58133888244629,
      "learning_rate": 9.252508384582416e-06,
      "loss": 1.9782,
      "step": 393
    },
    {
      "epoch": 0.2593811718235681,
      "grad_norm": 13.399981498718262,
      "learning_rate": 9.24645330178162e-06,
      "loss": 1.7784,
      "step": 394
    },
    {
      "epoch": 0.26003949967083606,
      "grad_norm": 65.2419662475586,
      "learning_rate": 9.240375791067055e-06,
      "loss": 2.6752,
      "step": 395
    },
    {
      "epoch": 0.260697827518104,
      "grad_norm": 24.111780166625977,
      "learning_rate": 9.234275884537483e-06,
      "loss": 1.6731,
      "step": 396
    },
    {
      "epoch": 0.26135615536537193,
      "grad_norm": 41.32785415649414,
      "learning_rate": 9.22815361440995e-06,
      "loss": 2.4861,
      "step": 397
    },
    {
      "epoch": 0.26201448321263987,
      "grad_norm": 4.942941665649414,
      "learning_rate": 9.222009013019613e-06,
      "loss": 1.5598,
      "step": 398
    },
    {
      "epoch": 0.2626728110599078,
      "grad_norm": 33.522926330566406,
      "learning_rate": 9.215842112819577e-06,
      "loss": 1.8302,
      "step": 399
    },
    {
      "epoch": 0.2633311389071758,
      "grad_norm": 3.8790700435638428,
      "learning_rate": 9.209652946380721e-06,
      "loss": 1.5621,
      "step": 400
    },
    {
      "epoch": 0.26398946675444374,
      "grad_norm": 11.443785667419434,
      "learning_rate": 9.203441546391521e-06,
      "loss": 1.7012,
      "step": 401
    },
    {
      "epoch": 0.2646477946017117,
      "grad_norm": 57.72576141357422,
      "learning_rate": 9.197207945657879e-06,
      "loss": 2.3352,
      "step": 402
    },
    {
      "epoch": 0.2653061224489796,
      "grad_norm": 39.72055435180664,
      "learning_rate": 9.190952177102957e-06,
      "loss": 2.4448,
      "step": 403
    },
    {
      "epoch": 0.26596445029624755,
      "grad_norm": 6.136214256286621,
      "learning_rate": 9.184674273766996e-06,
      "loss": 1.6584,
      "step": 404
    },
    {
      "epoch": 0.2666227781435155,
      "grad_norm": 17.449329376220703,
      "learning_rate": 9.178374268807141e-06,
      "loss": 1.7125,
      "step": 405
    },
    {
      "epoch": 0.2672811059907834,
      "grad_norm": 5.0378899574279785,
      "learning_rate": 9.172052195497271e-06,
      "loss": 1.5726,
      "step": 406
    },
    {
      "epoch": 0.26793943383805136,
      "grad_norm": 40.18376922607422,
      "learning_rate": 9.165708087227818e-06,
      "loss": 2.4774,
      "step": 407
    },
    {
      "epoch": 0.2685977616853193,
      "grad_norm": 6.997981071472168,
      "learning_rate": 9.159341977505595e-06,
      "loss": 1.6449,
      "step": 408
    },
    {
      "epoch": 0.26925608953258723,
      "grad_norm": 3.8274879455566406,
      "learning_rate": 9.152953899953616e-06,
      "loss": 1.5515,
      "step": 409
    },
    {
      "epoch": 0.26991441737985516,
      "grad_norm": 27.493566513061523,
      "learning_rate": 9.14654388831092e-06,
      "loss": 2.108,
      "step": 410
    },
    {
      "epoch": 0.2705727452271231,
      "grad_norm": 4.177044868469238,
      "learning_rate": 9.140111976432391e-06,
      "loss": 1.5503,
      "step": 411
    },
    {
      "epoch": 0.27123107307439104,
      "grad_norm": 46.099910736083984,
      "learning_rate": 9.133658198288584e-06,
      "loss": 2.6626,
      "step": 412
    },
    {
      "epoch": 0.271889400921659,
      "grad_norm": 9.691720008850098,
      "learning_rate": 9.127182587965535e-06,
      "loss": 1.6767,
      "step": 413
    },
    {
      "epoch": 0.2725477287689269,
      "grad_norm": 53.39277267456055,
      "learning_rate": 9.120685179664597e-06,
      "loss": 2.5478,
      "step": 414
    },
    {
      "epoch": 0.27320605661619485,
      "grad_norm": 41.986854553222656,
      "learning_rate": 9.114166007702245e-06,
      "loss": 2.1472,
      "step": 415
    },
    {
      "epoch": 0.2738643844634628,
      "grad_norm": 46.09077453613281,
      "learning_rate": 9.107625106509901e-06,
      "loss": 2.3759,
      "step": 416
    },
    {
      "epoch": 0.2745227123107307,
      "grad_norm": 37.214229583740234,
      "learning_rate": 9.10106251063375e-06,
      "loss": 2.0008,
      "step": 417
    },
    {
      "epoch": 0.27518104015799866,
      "grad_norm": 34.69703674316406,
      "learning_rate": 9.094478254734563e-06,
      "loss": 2.0793,
      "step": 418
    },
    {
      "epoch": 0.27583936800526665,
      "grad_norm": 30.187274932861328,
      "learning_rate": 9.087872373587505e-06,
      "loss": 1.7879,
      "step": 419
    },
    {
      "epoch": 0.2764976958525346,
      "grad_norm": 48.464603424072266,
      "learning_rate": 9.08124490208196e-06,
      "loss": 2.2357,
      "step": 420
    },
    {
      "epoch": 0.2771560236998025,
      "grad_norm": 64.15319061279297,
      "learning_rate": 9.07459587522134e-06,
      "loss": 2.6494,
      "step": 421
    },
    {
      "epoch": 0.27781435154707046,
      "grad_norm": 31.455373764038086,
      "learning_rate": 9.067925328122905e-06,
      "loss": 1.6435,
      "step": 422
    },
    {
      "epoch": 0.2784726793943384,
      "grad_norm": 70.06006622314453,
      "learning_rate": 9.061233296017574e-06,
      "loss": 2.587,
      "step": 423
    },
    {
      "epoch": 0.27913100724160633,
      "grad_norm": 13.944816589355469,
      "learning_rate": 9.054519814249741e-06,
      "loss": 1.6077,
      "step": 424
    },
    {
      "epoch": 0.27978933508887427,
      "grad_norm": 12.784144401550293,
      "learning_rate": 9.04778491827709e-06,
      "loss": 1.6138,
      "step": 425
    },
    {
      "epoch": 0.2804476629361422,
      "grad_norm": 31.4361515045166,
      "learning_rate": 9.0410286436704e-06,
      "loss": 2.1261,
      "step": 426
    },
    {
      "epoch": 0.28110599078341014,
      "grad_norm": 56.499813079833984,
      "learning_rate": 9.034251026113367e-06,
      "loss": 2.8777,
      "step": 427
    },
    {
      "epoch": 0.2817643186306781,
      "grad_norm": 31.89084815979004,
      "learning_rate": 9.02745210140241e-06,
      "loss": 2.1595,
      "step": 428
    },
    {
      "epoch": 0.282422646477946,
      "grad_norm": 18.029211044311523,
      "learning_rate": 9.020631905446487e-06,
      "loss": 1.6326,
      "step": 429
    },
    {
      "epoch": 0.28308097432521395,
      "grad_norm": 33.03289031982422,
      "learning_rate": 9.01379047426689e-06,
      "loss": 2.275,
      "step": 430
    },
    {
      "epoch": 0.2837393021724819,
      "grad_norm": 24.715173721313477,
      "learning_rate": 9.006927843997082e-06,
      "loss": 1.9834,
      "step": 431
    },
    {
      "epoch": 0.2843976300197498,
      "grad_norm": 9.935020446777344,
      "learning_rate": 9.000044050882478e-06,
      "loss": 1.6242,
      "step": 432
    },
    {
      "epoch": 0.28505595786701776,
      "grad_norm": 13.577828407287598,
      "learning_rate": 8.993139131280268e-06,
      "loss": 1.6574,
      "step": 433
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 24.389211654663086,
      "learning_rate": 8.986213121659228e-06,
      "loss": 1.8746,
      "step": 434
    },
    {
      "epoch": 0.28637261356155364,
      "grad_norm": 21.30864143371582,
      "learning_rate": 8.97926605859952e-06,
      "loss": 2.1887,
      "step": 435
    },
    {
      "epoch": 0.28703094140882157,
      "grad_norm": 69.37622833251953,
      "learning_rate": 8.972297978792499e-06,
      "loss": 3.5009,
      "step": 436
    },
    {
      "epoch": 0.2876892692560895,
      "grad_norm": 11.13915729522705,
      "learning_rate": 8.965308919040523e-06,
      "loss": 1.6878,
      "step": 437
    },
    {
      "epoch": 0.28834759710335744,
      "grad_norm": 25.038846969604492,
      "learning_rate": 8.958298916256755e-06,
      "loss": 2.0186,
      "step": 438
    },
    {
      "epoch": 0.28900592495062544,
      "grad_norm": 30.810245513916016,
      "learning_rate": 8.951268007464973e-06,
      "loss": 2.2548,
      "step": 439
    },
    {
      "epoch": 0.2896642527978934,
      "grad_norm": 21.756805419921875,
      "learning_rate": 8.944216229799371e-06,
      "loss": 2.0176,
      "step": 440
    },
    {
      "epoch": 0.2903225806451613,
      "grad_norm": 21.325809478759766,
      "learning_rate": 8.93714362050436e-06,
      "loss": 1.7876,
      "step": 441
    },
    {
      "epoch": 0.29098090849242925,
      "grad_norm": 28.9744815826416,
      "learning_rate": 8.930050216934377e-06,
      "loss": 1.8679,
      "step": 442
    },
    {
      "epoch": 0.2916392363396972,
      "grad_norm": 12.737908363342285,
      "learning_rate": 8.922936056553685e-06,
      "loss": 1.7131,
      "step": 443
    },
    {
      "epoch": 0.2922975641869651,
      "grad_norm": 32.933162689208984,
      "learning_rate": 8.915801176936178e-06,
      "loss": 2.2002,
      "step": 444
    },
    {
      "epoch": 0.29295589203423306,
      "grad_norm": 17.756744384765625,
      "learning_rate": 8.908645615765175e-06,
      "loss": 1.6724,
      "step": 445
    },
    {
      "epoch": 0.293614219881501,
      "grad_norm": 32.953224182128906,
      "learning_rate": 8.901469410833229e-06,
      "loss": 2.2254,
      "step": 446
    },
    {
      "epoch": 0.29427254772876893,
      "grad_norm": 22.427541732788086,
      "learning_rate": 8.894272600041923e-06,
      "loss": 1.9802,
      "step": 447
    },
    {
      "epoch": 0.29493087557603687,
      "grad_norm": 13.964644432067871,
      "learning_rate": 8.887055221401673e-06,
      "loss": 1.9738,
      "step": 448
    },
    {
      "epoch": 0.2955892034233048,
      "grad_norm": 14.437100410461426,
      "learning_rate": 8.879817313031525e-06,
      "loss": 1.9204,
      "step": 449
    },
    {
      "epoch": 0.29624753127057274,
      "grad_norm": 43.94041061401367,
      "learning_rate": 8.87255891315895e-06,
      "loss": 2.5503,
      "step": 450
    },
    {
      "epoch": 0.2969058591178407,
      "grad_norm": 14.58128547668457,
      "learning_rate": 8.865280060119654e-06,
      "loss": 1.6525,
      "step": 451
    },
    {
      "epoch": 0.2975641869651086,
      "grad_norm": 32.09221267700195,
      "learning_rate": 8.857980792357361e-06,
      "loss": 2.1828,
      "step": 452
    },
    {
      "epoch": 0.29822251481237655,
      "grad_norm": 25.76227569580078,
      "learning_rate": 8.850661148423618e-06,
      "loss": 1.8523,
      "step": 453
    },
    {
      "epoch": 0.2988808426596445,
      "grad_norm": 39.982357025146484,
      "learning_rate": 8.843321166977592e-06,
      "loss": 2.0329,
      "step": 454
    },
    {
      "epoch": 0.2995391705069124,
      "grad_norm": 21.54912567138672,
      "learning_rate": 8.835960886785864e-06,
      "loss": 2.0234,
      "step": 455
    },
    {
      "epoch": 0.30019749835418036,
      "grad_norm": 14.138433456420898,
      "learning_rate": 8.82858034672222e-06,
      "loss": 1.7872,
      "step": 456
    },
    {
      "epoch": 0.3008558262014483,
      "grad_norm": 21.439416885375977,
      "learning_rate": 8.821179585767456e-06,
      "loss": 1.918,
      "step": 457
    },
    {
      "epoch": 0.3015141540487163,
      "grad_norm": 23.24237823486328,
      "learning_rate": 8.81375864300916e-06,
      "loss": 2.055,
      "step": 458
    },
    {
      "epoch": 0.3021724818959842,
      "grad_norm": 22.8996639251709,
      "learning_rate": 8.806317557641516e-06,
      "loss": 1.6477,
      "step": 459
    },
    {
      "epoch": 0.30283080974325216,
      "grad_norm": 4.24159574508667,
      "learning_rate": 8.79885636896509e-06,
      "loss": 1.5474,
      "step": 460
    },
    {
      "epoch": 0.3034891375905201,
      "grad_norm": 27.972301483154297,
      "learning_rate": 8.791375116386625e-06,
      "loss": 2.0215,
      "step": 461
    },
    {
      "epoch": 0.30414746543778803,
      "grad_norm": 25.478492736816406,
      "learning_rate": 8.783873839418834e-06,
      "loss": 2.0308,
      "step": 462
    },
    {
      "epoch": 0.30480579328505597,
      "grad_norm": 4.22457218170166,
      "learning_rate": 8.77635257768019e-06,
      "loss": 1.5553,
      "step": 463
    },
    {
      "epoch": 0.3054641211323239,
      "grad_norm": 6.2402544021606445,
      "learning_rate": 8.768811370894714e-06,
      "loss": 1.5566,
      "step": 464
    },
    {
      "epoch": 0.30612244897959184,
      "grad_norm": 18.637296676635742,
      "learning_rate": 8.761250258891773e-06,
      "loss": 1.8102,
      "step": 465
    },
    {
      "epoch": 0.3067807768268598,
      "grad_norm": 22.982999801635742,
      "learning_rate": 8.75366928160586e-06,
      "loss": 1.8602,
      "step": 466
    },
    {
      "epoch": 0.3074391046741277,
      "grad_norm": 26.629545211791992,
      "learning_rate": 8.746068479076394e-06,
      "loss": 1.8334,
      "step": 467
    },
    {
      "epoch": 0.30809743252139565,
      "grad_norm": 21.772302627563477,
      "learning_rate": 8.73844789144749e-06,
      "loss": 2.0382,
      "step": 468
    },
    {
      "epoch": 0.3087557603686636,
      "grad_norm": 21.641523361206055,
      "learning_rate": 8.730807558967772e-06,
      "loss": 1.7505,
      "step": 469
    },
    {
      "epoch": 0.3094140882159315,
      "grad_norm": 55.75273132324219,
      "learning_rate": 8.723147521990147e-06,
      "loss": 1.9618,
      "step": 470
    },
    {
      "epoch": 0.31007241606319946,
      "grad_norm": 3.120802164077759,
      "learning_rate": 8.715467820971581e-06,
      "loss": 1.5313,
      "step": 471
    },
    {
      "epoch": 0.3107307439104674,
      "grad_norm": 18.0201473236084,
      "learning_rate": 8.707768496472908e-06,
      "loss": 1.67,
      "step": 472
    },
    {
      "epoch": 0.31138907175773534,
      "grad_norm": 3.040605068206787,
      "learning_rate": 8.700049589158602e-06,
      "loss": 1.5329,
      "step": 473
    },
    {
      "epoch": 0.3120473996050033,
      "grad_norm": 7.472654342651367,
      "learning_rate": 8.69231113979656e-06,
      "loss": 1.5805,
      "step": 474
    },
    {
      "epoch": 0.3127057274522712,
      "grad_norm": 3.5720345973968506,
      "learning_rate": 8.6845531892579e-06,
      "loss": 1.527,
      "step": 475
    },
    {
      "epoch": 0.31336405529953915,
      "grad_norm": 2.9582555294036865,
      "learning_rate": 8.676775778516731e-06,
      "loss": 1.5148,
      "step": 476
    },
    {
      "epoch": 0.3140223831468071,
      "grad_norm": 77.9144058227539,
      "learning_rate": 8.668978948649943e-06,
      "loss": 2.2749,
      "step": 477
    },
    {
      "epoch": 0.3146807109940751,
      "grad_norm": 16.582256317138672,
      "learning_rate": 8.661162740836988e-06,
      "loss": 1.6592,
      "step": 478
    },
    {
      "epoch": 0.315339038841343,
      "grad_norm": 42.17671203613281,
      "learning_rate": 8.653327196359669e-06,
      "loss": 1.93,
      "step": 479
    },
    {
      "epoch": 0.31599736668861095,
      "grad_norm": 79.72752380371094,
      "learning_rate": 8.645472356601912e-06,
      "loss": 2.0848,
      "step": 480
    },
    {
      "epoch": 0.3166556945358789,
      "grad_norm": 3.637293577194214,
      "learning_rate": 8.637598263049554e-06,
      "loss": 1.5083,
      "step": 481
    },
    {
      "epoch": 0.3173140223831468,
      "grad_norm": 4.197712421417236,
      "learning_rate": 8.629704957290122e-06,
      "loss": 1.5169,
      "step": 482
    },
    {
      "epoch": 0.31797235023041476,
      "grad_norm": 28.7612247467041,
      "learning_rate": 8.621792481012612e-06,
      "loss": 1.7568,
      "step": 483
    },
    {
      "epoch": 0.3186306780776827,
      "grad_norm": 8.423476219177246,
      "learning_rate": 8.613860876007274e-06,
      "loss": 1.5501,
      "step": 484
    },
    {
      "epoch": 0.31928900592495063,
      "grad_norm": 4.955844879150391,
      "learning_rate": 8.605910184165385e-06,
      "loss": 1.5282,
      "step": 485
    },
    {
      "epoch": 0.31994733377221857,
      "grad_norm": 39.23635482788086,
      "learning_rate": 8.597940447479031e-06,
      "loss": 2.1763,
      "step": 486
    },
    {
      "epoch": 0.3206056616194865,
      "grad_norm": 26.507543563842773,
      "learning_rate": 8.589951708040882e-06,
      "loss": 1.7439,
      "step": 487
    },
    {
      "epoch": 0.32126398946675444,
      "grad_norm": 34.39255905151367,
      "learning_rate": 8.581944008043979e-06,
      "loss": 2.0823,
      "step": 488
    },
    {
      "epoch": 0.3219223173140224,
      "grad_norm": 5.042296886444092,
      "learning_rate": 8.573917389781493e-06,
      "loss": 1.5048,
      "step": 489
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 31.019390106201172,
      "learning_rate": 8.565871895646527e-06,
      "loss": 2.0379,
      "step": 490
    },
    {
      "epoch": 0.32323897300855825,
      "grad_norm": 5.880908966064453,
      "learning_rate": 8.557807568131866e-06,
      "loss": 1.5119,
      "step": 491
    },
    {
      "epoch": 0.3238973008558262,
      "grad_norm": 36.18767166137695,
      "learning_rate": 8.549724449829774e-06,
      "loss": 1.9476,
      "step": 492
    },
    {
      "epoch": 0.3245556287030941,
      "grad_norm": 30.184425354003906,
      "learning_rate": 8.54162258343175e-06,
      "loss": 1.8992,
      "step": 493
    },
    {
      "epoch": 0.32521395655036206,
      "grad_norm": 7.302596092224121,
      "learning_rate": 8.533502011728322e-06,
      "loss": 1.5152,
      "step": 494
    },
    {
      "epoch": 0.32587228439763,
      "grad_norm": 48.88936996459961,
      "learning_rate": 8.525362777608808e-06,
      "loss": 2.0439,
      "step": 495
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 42.553951263427734,
      "learning_rate": 8.517204924061088e-06,
      "loss": 1.6018,
      "step": 496
    },
    {
      "epoch": 0.3271889400921659,
      "grad_norm": 40.43186950683594,
      "learning_rate": 8.509028494171389e-06,
      "loss": 1.8239,
      "step": 497
    },
    {
      "epoch": 0.32784726793943386,
      "grad_norm": 5.069027900695801,
      "learning_rate": 8.500833531124044e-06,
      "loss": 1.5313,
      "step": 498
    },
    {
      "epoch": 0.3285055957867018,
      "grad_norm": 24.102901458740234,
      "learning_rate": 8.492620078201272e-06,
      "loss": 1.9463,
      "step": 499
    },
    {
      "epoch": 0.32916392363396973,
      "grad_norm": 88.39151000976562,
      "learning_rate": 8.484388178782951e-06,
      "loss": 2.9432,
      "step": 500
    },
    {
      "epoch": 0.32982225148123767,
      "grad_norm": 44.37199020385742,
      "learning_rate": 8.476137876346382e-06,
      "loss": 2.1618,
      "step": 501
    },
    {
      "epoch": 0.3304805793285056,
      "grad_norm": 4.582010746002197,
      "learning_rate": 8.467869214466061e-06,
      "loss": 1.5025,
      "step": 502
    },
    {
      "epoch": 0.33113890717577354,
      "grad_norm": 4.794940948486328,
      "learning_rate": 8.459582236813455e-06,
      "loss": 1.505,
      "step": 503
    },
    {
      "epoch": 0.3317972350230415,
      "grad_norm": 4.2419304847717285,
      "learning_rate": 8.451276987156762e-06,
      "loss": 1.5016,
      "step": 504
    },
    {
      "epoch": 0.3324555628703094,
      "grad_norm": 48.700401306152344,
      "learning_rate": 8.442953509360691e-06,
      "loss": 1.9906,
      "step": 505
    },
    {
      "epoch": 0.33311389071757735,
      "grad_norm": 15.471949577331543,
      "learning_rate": 8.434611847386217e-06,
      "loss": 1.5852,
      "step": 506
    },
    {
      "epoch": 0.3337722185648453,
      "grad_norm": 27.364673614501953,
      "learning_rate": 8.426252045290362e-06,
      "loss": 1.638,
      "step": 507
    },
    {
      "epoch": 0.3344305464121132,
      "grad_norm": 2.41083025932312,
      "learning_rate": 8.417874147225951e-06,
      "loss": 1.4864,
      "step": 508
    },
    {
      "epoch": 0.33508887425938116,
      "grad_norm": 24.654232025146484,
      "learning_rate": 8.409478197441389e-06,
      "loss": 1.7108,
      "step": 509
    },
    {
      "epoch": 0.3357472021066491,
      "grad_norm": 21.30550765991211,
      "learning_rate": 8.401064240280421e-06,
      "loss": 1.7864,
      "step": 510
    },
    {
      "epoch": 0.33640552995391704,
      "grad_norm": 2.3276872634887695,
      "learning_rate": 8.392632320181892e-06,
      "loss": 1.4884,
      "step": 511
    },
    {
      "epoch": 0.337063857801185,
      "grad_norm": 47.231117248535156,
      "learning_rate": 8.384182481679532e-06,
      "loss": 2.0543,
      "step": 512
    },
    {
      "epoch": 0.3377221856484529,
      "grad_norm": 38.33640670776367,
      "learning_rate": 8.3757147694017e-06,
      "loss": 1.5422,
      "step": 513
    },
    {
      "epoch": 0.33838051349572085,
      "grad_norm": 2.4537124633789062,
      "learning_rate": 8.367229228071156e-06,
      "loss": 1.4892,
      "step": 514
    },
    {
      "epoch": 0.3390388413429888,
      "grad_norm": 42.83787155151367,
      "learning_rate": 8.358725902504827e-06,
      "loss": 2.1695,
      "step": 515
    },
    {
      "epoch": 0.3396971691902567,
      "grad_norm": 1.9057328701019287,
      "learning_rate": 8.350204837613574e-06,
      "loss": 1.4801,
      "step": 516
    },
    {
      "epoch": 0.3403554970375247,
      "grad_norm": 1.525270938873291,
      "learning_rate": 8.34166607840194e-06,
      "loss": 1.4741,
      "step": 517
    },
    {
      "epoch": 0.34101382488479265,
      "grad_norm": 3.311675548553467,
      "learning_rate": 8.333109669967924e-06,
      "loss": 1.4897,
      "step": 518
    },
    {
      "epoch": 0.3416721527320606,
      "grad_norm": 31.53557586669922,
      "learning_rate": 8.324535657502748e-06,
      "loss": 1.9975,
      "step": 519
    },
    {
      "epoch": 0.3423304805793285,
      "grad_norm": 34.183387756347656,
      "learning_rate": 8.315944086290602e-06,
      "loss": 1.8963,
      "step": 520
    },
    {
      "epoch": 0.34298880842659646,
      "grad_norm": 13.724237442016602,
      "learning_rate": 8.307335001708416e-06,
      "loss": 1.5908,
      "step": 521
    },
    {
      "epoch": 0.3436471362738644,
      "grad_norm": 24.626161575317383,
      "learning_rate": 8.298708449225624e-06,
      "loss": 1.8579,
      "step": 522
    },
    {
      "epoch": 0.34430546412113233,
      "grad_norm": 92.42198944091797,
      "learning_rate": 8.290064474403904e-06,
      "loss": 2.7703,
      "step": 523
    },
    {
      "epoch": 0.34496379196840027,
      "grad_norm": 40.8856315612793,
      "learning_rate": 8.28140312289697e-06,
      "loss": 2.3264,
      "step": 524
    },
    {
      "epoch": 0.3456221198156682,
      "grad_norm": 14.285638809204102,
      "learning_rate": 8.272724440450297e-06,
      "loss": 1.6543,
      "step": 525
    },
    {
      "epoch": 0.34628044766293614,
      "grad_norm": 6.365965366363525,
      "learning_rate": 8.264028472900897e-06,
      "loss": 1.5131,
      "step": 526
    },
    {
      "epoch": 0.3469387755102041,
      "grad_norm": 24.694061279296875,
      "learning_rate": 8.255315266177081e-06,
      "loss": 1.682,
      "step": 527
    },
    {
      "epoch": 0.347597103357472,
      "grad_norm": 4.092640399932861,
      "learning_rate": 8.246584866298206e-06,
      "loss": 1.4836,
      "step": 528
    },
    {
      "epoch": 0.34825543120473995,
      "grad_norm": 30.624238967895508,
      "learning_rate": 8.237837319374434e-06,
      "loss": 2.0455,
      "step": 529
    },
    {
      "epoch": 0.3489137590520079,
      "grad_norm": 39.60075378417969,
      "learning_rate": 8.229072671606488e-06,
      "loss": 1.8339,
      "step": 530
    },
    {
      "epoch": 0.3495720868992758,
      "grad_norm": 29.018163681030273,
      "learning_rate": 8.22029096928542e-06,
      "loss": 1.7313,
      "step": 531
    },
    {
      "epoch": 0.35023041474654376,
      "grad_norm": 61.254093170166016,
      "learning_rate": 8.211492258792346e-06,
      "loss": 2.3106,
      "step": 532
    },
    {
      "epoch": 0.3508887425938117,
      "grad_norm": 21.249568939208984,
      "learning_rate": 8.202676586598217e-06,
      "loss": 1.7161,
      "step": 533
    },
    {
      "epoch": 0.35154707044107963,
      "grad_norm": 6.716342926025391,
      "learning_rate": 8.19384399926357e-06,
      "loss": 1.5051,
      "step": 534
    },
    {
      "epoch": 0.35220539828834757,
      "grad_norm": 10.2349853515625,
      "learning_rate": 8.184994543438277e-06,
      "loss": 1.5958,
      "step": 535
    },
    {
      "epoch": 0.35286372613561556,
      "grad_norm": 47.404945373535156,
      "learning_rate": 8.176128265861304e-06,
      "loss": 2.0952,
      "step": 536
    },
    {
      "epoch": 0.3535220539828835,
      "grad_norm": 2.993730068206787,
      "learning_rate": 8.167245213360456e-06,
      "loss": 1.4793,
      "step": 537
    },
    {
      "epoch": 0.35418038183015144,
      "grad_norm": 34.15560531616211,
      "learning_rate": 8.158345432852146e-06,
      "loss": 2.0484,
      "step": 538
    },
    {
      "epoch": 0.3548387096774194,
      "grad_norm": 45.38874435424805,
      "learning_rate": 8.149428971341131e-06,
      "loss": 2.0992,
      "step": 539
    },
    {
      "epoch": 0.3554970375246873,
      "grad_norm": 34.81380844116211,
      "learning_rate": 8.140495875920271e-06,
      "loss": 2.0296,
      "step": 540
    },
    {
      "epoch": 0.35615536537195525,
      "grad_norm": 33.13304901123047,
      "learning_rate": 8.131546193770277e-06,
      "loss": 1.7778,
      "step": 541
    },
    {
      "epoch": 0.3568136932192232,
      "grad_norm": 44.086944580078125,
      "learning_rate": 8.122579972159466e-06,
      "loss": 2.129,
      "step": 542
    },
    {
      "epoch": 0.3574720210664911,
      "grad_norm": 42.37623596191406,
      "learning_rate": 8.113597258443511e-06,
      "loss": 1.9388,
      "step": 543
    },
    {
      "epoch": 0.35813034891375906,
      "grad_norm": 3.645392894744873,
      "learning_rate": 8.104598100065186e-06,
      "loss": 1.4853,
      "step": 544
    },
    {
      "epoch": 0.358788676761027,
      "grad_norm": 2.7742819786071777,
      "learning_rate": 8.095582544554119e-06,
      "loss": 1.4803,
      "step": 545
    },
    {
      "epoch": 0.35944700460829493,
      "grad_norm": 9.678589820861816,
      "learning_rate": 8.086550639526541e-06,
      "loss": 1.5199,
      "step": 546
    },
    {
      "epoch": 0.36010533245556287,
      "grad_norm": 23.53388214111328,
      "learning_rate": 8.077502432685035e-06,
      "loss": 1.7722,
      "step": 547
    },
    {
      "epoch": 0.3607636603028308,
      "grad_norm": 26.34982681274414,
      "learning_rate": 8.068437971818282e-06,
      "loss": 1.7965,
      "step": 548
    },
    {
      "epoch": 0.36142198815009874,
      "grad_norm": 33.683807373046875,
      "learning_rate": 8.059357304800808e-06,
      "loss": 1.809,
      "step": 549
    },
    {
      "epoch": 0.3620803159973667,
      "grad_norm": 43.10690689086914,
      "learning_rate": 8.050260479592735e-06,
      "loss": 1.9077,
      "step": 550
    },
    {
      "epoch": 0.3627386438446346,
      "grad_norm": 48.28982925415039,
      "learning_rate": 8.04114754423953e-06,
      "loss": 2.0486,
      "step": 551
    },
    {
      "epoch": 0.36339697169190255,
      "grad_norm": 34.1959342956543,
      "learning_rate": 8.032018546871735e-06,
      "loss": 2.1097,
      "step": 552
    },
    {
      "epoch": 0.3640552995391705,
      "grad_norm": 81.45889282226562,
      "learning_rate": 8.022873535704736e-06,
      "loss": 1.827,
      "step": 553
    },
    {
      "epoch": 0.3647136273864384,
      "grad_norm": 77.62801361083984,
      "learning_rate": 8.01371255903849e-06,
      "loss": 2.2808,
      "step": 554
    },
    {
      "epoch": 0.36537195523370636,
      "grad_norm": 3.6479945182800293,
      "learning_rate": 8.00453566525728e-06,
      "loss": 1.4769,
      "step": 555
    },
    {
      "epoch": 0.36603028308097435,
      "grad_norm": 4.061846733093262,
      "learning_rate": 7.995342902829455e-06,
      "loss": 1.4922,
      "step": 556
    },
    {
      "epoch": 0.3666886109282423,
      "grad_norm": 5.98704195022583,
      "learning_rate": 7.986134320307175e-06,
      "loss": 1.4805,
      "step": 557
    },
    {
      "epoch": 0.3673469387755102,
      "grad_norm": 21.546266555786133,
      "learning_rate": 7.976909966326158e-06,
      "loss": 1.8471,
      "step": 558
    },
    {
      "epoch": 0.36800526662277816,
      "grad_norm": 20.241737365722656,
      "learning_rate": 7.967669889605418e-06,
      "loss": 1.7386,
      "step": 559
    },
    {
      "epoch": 0.3686635944700461,
      "grad_norm": 24.669170379638672,
      "learning_rate": 7.958414138947003e-06,
      "loss": 1.8325,
      "step": 560
    },
    {
      "epoch": 0.36932192231731403,
      "grad_norm": 22.845657348632812,
      "learning_rate": 7.949142763235757e-06,
      "loss": 1.9135,
      "step": 561
    },
    {
      "epoch": 0.36998025016458197,
      "grad_norm": 2.129936695098877,
      "learning_rate": 7.93985581143904e-06,
      "loss": 1.4705,
      "step": 562
    },
    {
      "epoch": 0.3706385780118499,
      "grad_norm": 2.5726301670074463,
      "learning_rate": 7.930553332606477e-06,
      "loss": 1.4769,
      "step": 563
    },
    {
      "epoch": 0.37129690585911784,
      "grad_norm": 18.495227813720703,
      "learning_rate": 7.921235375869705e-06,
      "loss": 1.6963,
      "step": 564
    },
    {
      "epoch": 0.3719552337063858,
      "grad_norm": 68.47293090820312,
      "learning_rate": 7.911901990442106e-06,
      "loss": 2.2275,
      "step": 565
    },
    {
      "epoch": 0.3726135615536537,
      "grad_norm": 2.780380964279175,
      "learning_rate": 7.90255322561855e-06,
      "loss": 1.4745,
      "step": 566
    },
    {
      "epoch": 0.37327188940092165,
      "grad_norm": 98.41505432128906,
      "learning_rate": 7.893189130775135e-06,
      "loss": 2.2458,
      "step": 567
    },
    {
      "epoch": 0.3739302172481896,
      "grad_norm": 47.087364196777344,
      "learning_rate": 7.883809755368922e-06,
      "loss": 1.9998,
      "step": 568
    },
    {
      "epoch": 0.3745885450954575,
      "grad_norm": 4.2612528800964355,
      "learning_rate": 7.874415148937686e-06,
      "loss": 1.479,
      "step": 569
    },
    {
      "epoch": 0.37524687294272546,
      "grad_norm": 60.863136291503906,
      "learning_rate": 7.865005361099633e-06,
      "loss": 3.0017,
      "step": 570
    },
    {
      "epoch": 0.3759052007899934,
      "grad_norm": 78.31595611572266,
      "learning_rate": 7.855580441553161e-06,
      "loss": 2.1306,
      "step": 571
    },
    {
      "epoch": 0.37656352863726134,
      "grad_norm": 40.75505065917969,
      "learning_rate": 7.84614044007658e-06,
      "loss": 1.9436,
      "step": 572
    },
    {
      "epoch": 0.37722185648452927,
      "grad_norm": 62.44800567626953,
      "learning_rate": 7.836685406527864e-06,
      "loss": 2.6425,
      "step": 573
    },
    {
      "epoch": 0.3778801843317972,
      "grad_norm": 40.197113037109375,
      "learning_rate": 7.827215390844373e-06,
      "loss": 2.1666,
      "step": 574
    },
    {
      "epoch": 0.3785385121790652,
      "grad_norm": 28.743331909179688,
      "learning_rate": 7.817730443042597e-06,
      "loss": 2.0243,
      "step": 575
    },
    {
      "epoch": 0.37919684002633314,
      "grad_norm": 20.155061721801758,
      "learning_rate": 7.808230613217896e-06,
      "loss": 1.7595,
      "step": 576
    },
    {
      "epoch": 0.3798551678736011,
      "grad_norm": 23.226551055908203,
      "learning_rate": 7.798715951544222e-06,
      "loss": 1.8086,
      "step": 577
    },
    {
      "epoch": 0.380513495720869,
      "grad_norm": 6.123249053955078,
      "learning_rate": 7.789186508273872e-06,
      "loss": 1.4845,
      "step": 578
    },
    {
      "epoch": 0.38117182356813695,
      "grad_norm": 2.439014434814453,
      "learning_rate": 7.779642333737203e-06,
      "loss": 1.474,
      "step": 579
    },
    {
      "epoch": 0.3818301514154049,
      "grad_norm": 81.17042541503906,
      "learning_rate": 7.770083478342386e-06,
      "loss": 2.9267,
      "step": 580
    },
    {
      "epoch": 0.3824884792626728,
      "grad_norm": 82.48339080810547,
      "learning_rate": 7.760509992575123e-06,
      "loss": 2.9833,
      "step": 581
    },
    {
      "epoch": 0.38314680710994076,
      "grad_norm": 4.104960918426514,
      "learning_rate": 7.750921926998387e-06,
      "loss": 1.4906,
      "step": 582
    },
    {
      "epoch": 0.3838051349572087,
      "grad_norm": 28.8724308013916,
      "learning_rate": 7.74131933225216e-06,
      "loss": 2.0459,
      "step": 583
    },
    {
      "epoch": 0.38446346280447663,
      "grad_norm": 13.576680183410645,
      "learning_rate": 7.731702259053158e-06,
      "loss": 1.7893,
      "step": 584
    },
    {
      "epoch": 0.38512179065174457,
      "grad_norm": 9.34554386138916,
      "learning_rate": 7.722070758194566e-06,
      "loss": 1.5181,
      "step": 585
    },
    {
      "epoch": 0.3857801184990125,
      "grad_norm": 14.9401216506958,
      "learning_rate": 7.712424880545768e-06,
      "loss": 1.8287,
      "step": 586
    },
    {
      "epoch": 0.38643844634628044,
      "grad_norm": 91.0771484375,
      "learning_rate": 7.702764677052083e-06,
      "loss": 3.2653,
      "step": 587
    },
    {
      "epoch": 0.3870967741935484,
      "grad_norm": 23.263669967651367,
      "learning_rate": 7.693090198734494e-06,
      "loss": 1.9487,
      "step": 588
    },
    {
      "epoch": 0.3877551020408163,
      "grad_norm": 27.618648529052734,
      "learning_rate": 7.68340149668937e-06,
      "loss": 1.7678,
      "step": 589
    },
    {
      "epoch": 0.38841342988808425,
      "grad_norm": 32.51015090942383,
      "learning_rate": 7.673698622088212e-06,
      "loss": 1.8133,
      "step": 590
    },
    {
      "epoch": 0.3890717577353522,
      "grad_norm": 18.921382904052734,
      "learning_rate": 7.663981626177368e-06,
      "loss": 1.8445,
      "step": 591
    },
    {
      "epoch": 0.3897300855826201,
      "grad_norm": 27.031639099121094,
      "learning_rate": 7.654250560277773e-06,
      "loss": 1.8264,
      "step": 592
    },
    {
      "epoch": 0.39038841342988806,
      "grad_norm": 21.42900848388672,
      "learning_rate": 7.644505475784673e-06,
      "loss": 1.8938,
      "step": 593
    },
    {
      "epoch": 0.39104674127715605,
      "grad_norm": 26.911483764648438,
      "learning_rate": 7.63474642416735e-06,
      "loss": 2.0966,
      "step": 594
    },
    {
      "epoch": 0.391705069124424,
      "grad_norm": 16.776657104492188,
      "learning_rate": 7.6249734569688576e-06,
      "loss": 1.6791,
      "step": 595
    },
    {
      "epoch": 0.3923633969716919,
      "grad_norm": 23.56498908996582,
      "learning_rate": 7.615186625805745e-06,
      "loss": 2.0997,
      "step": 596
    },
    {
      "epoch": 0.39302172481895986,
      "grad_norm": 21.740070343017578,
      "learning_rate": 7.605385982367787e-06,
      "loss": 2.1019,
      "step": 597
    },
    {
      "epoch": 0.3936800526662278,
      "grad_norm": 13.880366325378418,
      "learning_rate": 7.595571578417704e-06,
      "loss": 1.7158,
      "step": 598
    },
    {
      "epoch": 0.39433838051349573,
      "grad_norm": 31.255565643310547,
      "learning_rate": 7.585743465790898e-06,
      "loss": 1.8124,
      "step": 599
    },
    {
      "epoch": 0.39499670836076367,
      "grad_norm": 54.803611755371094,
      "learning_rate": 7.57590169639517e-06,
      "loss": 2.4644,
      "step": 600
    },
    {
      "epoch": 0.3956550362080316,
      "grad_norm": 6.310842037200928,
      "learning_rate": 7.566046322210454e-06,
      "loss": 1.4935,
      "step": 601
    },
    {
      "epoch": 0.39631336405529954,
      "grad_norm": 11.033890724182129,
      "learning_rate": 7.556177395288535e-06,
      "loss": 1.6691,
      "step": 602
    },
    {
      "epoch": 0.3969716919025675,
      "grad_norm": 13.592185974121094,
      "learning_rate": 7.546294967752783e-06,
      "loss": 1.7036,
      "step": 603
    },
    {
      "epoch": 0.3976300197498354,
      "grad_norm": 28.39473533630371,
      "learning_rate": 7.536399091797867e-06,
      "loss": 1.7678,
      "step": 604
    },
    {
      "epoch": 0.39828834759710335,
      "grad_norm": 11.354865074157715,
      "learning_rate": 7.526489819689487e-06,
      "loss": 1.7082,
      "step": 605
    },
    {
      "epoch": 0.3989466754443713,
      "grad_norm": 18.541349411010742,
      "learning_rate": 7.516567203764092e-06,
      "loss": 1.8533,
      "step": 606
    },
    {
      "epoch": 0.3996050032916392,
      "grad_norm": 25.150711059570312,
      "learning_rate": 7.506631296428615e-06,
      "loss": 1.9935,
      "step": 607
    },
    {
      "epoch": 0.40026333113890716,
      "grad_norm": 4.948416709899902,
      "learning_rate": 7.496682150160182e-06,
      "loss": 1.4881,
      "step": 608
    },
    {
      "epoch": 0.4009216589861751,
      "grad_norm": 92.15434265136719,
      "learning_rate": 7.486719817505843e-06,
      "loss": 2.9015,
      "step": 609
    },
    {
      "epoch": 0.40157998683344304,
      "grad_norm": 34.36882019042969,
      "learning_rate": 7.476744351082292e-06,
      "loss": 1.8312,
      "step": 610
    },
    {
      "epoch": 0.402238314680711,
      "grad_norm": 30.29952621459961,
      "learning_rate": 7.466755803575594e-06,
      "loss": 1.773,
      "step": 611
    },
    {
      "epoch": 0.4028966425279789,
      "grad_norm": 23.332082748413086,
      "learning_rate": 7.456754227740896e-06,
      "loss": 1.8585,
      "step": 612
    },
    {
      "epoch": 0.40355497037524685,
      "grad_norm": 6.820039749145508,
      "learning_rate": 7.446739676402158e-06,
      "loss": 1.4745,
      "step": 613
    },
    {
      "epoch": 0.40421329822251484,
      "grad_norm": 60.913299560546875,
      "learning_rate": 7.436712202451874e-06,
      "loss": 2.0643,
      "step": 614
    },
    {
      "epoch": 0.4048716260697828,
      "grad_norm": 21.23456382751465,
      "learning_rate": 7.426671858850785e-06,
      "loss": 1.8021,
      "step": 615
    },
    {
      "epoch": 0.4055299539170507,
      "grad_norm": 13.215648651123047,
      "learning_rate": 7.416618698627606e-06,
      "loss": 1.6295,
      "step": 616
    },
    {
      "epoch": 0.40618828176431865,
      "grad_norm": 14.823143005371094,
      "learning_rate": 7.406552774878744e-06,
      "loss": 1.5792,
      "step": 617
    },
    {
      "epoch": 0.4068466096115866,
      "grad_norm": 14.610265731811523,
      "learning_rate": 7.396474140768014e-06,
      "loss": 1.7359,
      "step": 618
    },
    {
      "epoch": 0.4075049374588545,
      "grad_norm": 1.8074305057525635,
      "learning_rate": 7.386382849526369e-06,
      "loss": 1.4609,
      "step": 619
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 25.107263565063477,
      "learning_rate": 7.376278954451604e-06,
      "loss": 1.9608,
      "step": 620
    },
    {
      "epoch": 0.4088215931533904,
      "grad_norm": 30.16103744506836,
      "learning_rate": 7.366162508908084e-06,
      "loss": 1.8668,
      "step": 621
    },
    {
      "epoch": 0.40947992100065833,
      "grad_norm": 12.85173511505127,
      "learning_rate": 7.356033566326464e-06,
      "loss": 1.5005,
      "step": 622
    },
    {
      "epoch": 0.41013824884792627,
      "grad_norm": 133.1451416015625,
      "learning_rate": 7.3458921802033974e-06,
      "loss": 2.5699,
      "step": 623
    },
    {
      "epoch": 0.4107965766951942,
      "grad_norm": 6.397195339202881,
      "learning_rate": 7.335738404101263e-06,
      "loss": 1.4802,
      "step": 624
    },
    {
      "epoch": 0.41145490454246214,
      "grad_norm": 3.3090264797210693,
      "learning_rate": 7.3255722916478745e-06,
      "loss": 1.4722,
      "step": 625
    },
    {
      "epoch": 0.4121132323897301,
      "grad_norm": 2.7112607955932617,
      "learning_rate": 7.315393896536205e-06,
      "loss": 1.4708,
      "step": 626
    },
    {
      "epoch": 0.412771560236998,
      "grad_norm": 39.930965423583984,
      "learning_rate": 7.305203272524094e-06,
      "loss": 1.635,
      "step": 627
    },
    {
      "epoch": 0.41342988808426595,
      "grad_norm": 10.185600280761719,
      "learning_rate": 7.295000473433976e-06,
      "loss": 1.5537,
      "step": 628
    },
    {
      "epoch": 0.4140882159315339,
      "grad_norm": 23.018627166748047,
      "learning_rate": 7.28478555315258e-06,
      "loss": 1.6556,
      "step": 629
    },
    {
      "epoch": 0.4147465437788018,
      "grad_norm": 17.84083366394043,
      "learning_rate": 7.27455856563066e-06,
      "loss": 1.5479,
      "step": 630
    },
    {
      "epoch": 0.41540487162606976,
      "grad_norm": 32.961585998535156,
      "learning_rate": 7.264319564882707e-06,
      "loss": 1.7602,
      "step": 631
    },
    {
      "epoch": 0.4160631994733377,
      "grad_norm": 27.610700607299805,
      "learning_rate": 7.254068604986651e-06,
      "loss": 1.7328,
      "step": 632
    },
    {
      "epoch": 0.4167215273206057,
      "grad_norm": 35.31785583496094,
      "learning_rate": 7.2438057400835895e-06,
      "loss": 1.8891,
      "step": 633
    },
    {
      "epoch": 0.4173798551678736,
      "grad_norm": 16.41847801208496,
      "learning_rate": 7.233531024377501e-06,
      "loss": 1.6006,
      "step": 634
    },
    {
      "epoch": 0.41803818301514156,
      "grad_norm": 16.21233558654785,
      "learning_rate": 7.2232445121349495e-06,
      "loss": 1.5993,
      "step": 635
    },
    {
      "epoch": 0.4186965108624095,
      "grad_norm": 47.06550598144531,
      "learning_rate": 7.212946257684805e-06,
      "loss": 2.3644,
      "step": 636
    },
    {
      "epoch": 0.41935483870967744,
      "grad_norm": 46.101566314697266,
      "learning_rate": 7.202636315417955e-06,
      "loss": 2.2042,
      "step": 637
    },
    {
      "epoch": 0.42001316655694537,
      "grad_norm": 4.914795398712158,
      "learning_rate": 7.192314739787017e-06,
      "loss": 1.4802,
      "step": 638
    },
    {
      "epoch": 0.4206714944042133,
      "grad_norm": 3.70756196975708,
      "learning_rate": 7.181981585306051e-06,
      "loss": 1.4682,
      "step": 639
    },
    {
      "epoch": 0.42132982225148125,
      "grad_norm": 55.49492645263672,
      "learning_rate": 7.17163690655027e-06,
      "loss": 2.0222,
      "step": 640
    },
    {
      "epoch": 0.4219881500987492,
      "grad_norm": 3.9624016284942627,
      "learning_rate": 7.161280758155754e-06,
      "loss": 1.463,
      "step": 641
    },
    {
      "epoch": 0.4226464779460171,
      "grad_norm": 1.4805744886398315,
      "learning_rate": 7.150913194819162e-06,
      "loss": 1.4545,
      "step": 642
    },
    {
      "epoch": 0.42330480579328505,
      "grad_norm": 1.5753933191299438,
      "learning_rate": 7.14053427129744e-06,
      "loss": 1.4485,
      "step": 643
    },
    {
      "epoch": 0.423963133640553,
      "grad_norm": 89.75425720214844,
      "learning_rate": 7.130144042407534e-06,
      "loss": 2.5121,
      "step": 644
    },
    {
      "epoch": 0.42462146148782093,
      "grad_norm": 35.821807861328125,
      "learning_rate": 7.1197425630261e-06,
      "loss": 1.6915,
      "step": 645
    },
    {
      "epoch": 0.42527978933508886,
      "grad_norm": 2.1950156688690186,
      "learning_rate": 7.1093298880892144e-06,
      "loss": 1.4528,
      "step": 646
    },
    {
      "epoch": 0.4259381171823568,
      "grad_norm": 1.6891834735870361,
      "learning_rate": 7.0989060725920836e-06,
      "loss": 1.445,
      "step": 647
    },
    {
      "epoch": 0.42659644502962474,
      "grad_norm": 2.7384774684906006,
      "learning_rate": 7.088471171588752e-06,
      "loss": 1.4484,
      "step": 648
    },
    {
      "epoch": 0.4272547728768927,
      "grad_norm": 2.427001953125,
      "learning_rate": 7.078025240191818e-06,
      "loss": 1.4483,
      "step": 649
    },
    {
      "epoch": 0.4279131007241606,
      "grad_norm": 289.88800048828125,
      "learning_rate": 7.067568333572131e-06,
      "loss": 2.7802,
      "step": 650
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 23.441661834716797,
      "learning_rate": 7.057100506958512e-06,
      "loss": 1.7205,
      "step": 651
    },
    {
      "epoch": 0.4292297564186965,
      "grad_norm": 6.86558198928833,
      "learning_rate": 7.046621815637451e-06,
      "loss": 1.4981,
      "step": 652
    },
    {
      "epoch": 0.4298880842659645,
      "grad_norm": 2.7244553565979004,
      "learning_rate": 7.036132314952829e-06,
      "loss": 1.454,
      "step": 653
    },
    {
      "epoch": 0.4305464121132324,
      "grad_norm": 69.8942642211914,
      "learning_rate": 7.025632060305606e-06,
      "loss": 2.0398,
      "step": 654
    },
    {
      "epoch": 0.43120473996050035,
      "grad_norm": 5.652652740478516,
      "learning_rate": 7.015121107153552e-06,
      "loss": 1.4838,
      "step": 655
    },
    {
      "epoch": 0.4318630678077683,
      "grad_norm": 18.50936508178711,
      "learning_rate": 7.004599511010933e-06,
      "loss": 1.5905,
      "step": 656
    },
    {
      "epoch": 0.4325213956550362,
      "grad_norm": 11.741366386413574,
      "learning_rate": 6.994067327448231e-06,
      "loss": 1.4991,
      "step": 657
    },
    {
      "epoch": 0.43317972350230416,
      "grad_norm": 33.44754409790039,
      "learning_rate": 6.983524612091845e-06,
      "loss": 1.6991,
      "step": 658
    },
    {
      "epoch": 0.4338380513495721,
      "grad_norm": 2.8548648357391357,
      "learning_rate": 6.972971420623797e-06,
      "loss": 1.4583,
      "step": 659
    },
    {
      "epoch": 0.43449637919684003,
      "grad_norm": 1.925551414489746,
      "learning_rate": 6.9624078087814395e-06,
      "loss": 1.4446,
      "step": 660
    },
    {
      "epoch": 0.43515470704410797,
      "grad_norm": 70.6187744140625,
      "learning_rate": 6.951833832357163e-06,
      "loss": 3.4114,
      "step": 661
    },
    {
      "epoch": 0.4358130348913759,
      "grad_norm": 54.0711784362793,
      "learning_rate": 6.941249547198099e-06,
      "loss": 2.196,
      "step": 662
    },
    {
      "epoch": 0.43647136273864384,
      "grad_norm": 45.60224151611328,
      "learning_rate": 6.930655009205823e-06,
      "loss": 1.8372,
      "step": 663
    },
    {
      "epoch": 0.4371296905859118,
      "grad_norm": 38.10825729370117,
      "learning_rate": 6.920050274336061e-06,
      "loss": 1.9518,
      "step": 664
    },
    {
      "epoch": 0.4377880184331797,
      "grad_norm": 73.89047241210938,
      "learning_rate": 6.9094353985984e-06,
      "loss": 2.2446,
      "step": 665
    },
    {
      "epoch": 0.43844634628044765,
      "grad_norm": 71.91390991210938,
      "learning_rate": 6.8988104380559794e-06,
      "loss": 1.7744,
      "step": 666
    },
    {
      "epoch": 0.4391046741277156,
      "grad_norm": 39.27928161621094,
      "learning_rate": 6.888175448825207e-06,
      "loss": 1.8908,
      "step": 667
    },
    {
      "epoch": 0.4397630019749835,
      "grad_norm": 1.7072926759719849,
      "learning_rate": 6.877530487075455e-06,
      "loss": 1.4418,
      "step": 668
    },
    {
      "epoch": 0.44042132982225146,
      "grad_norm": 29.360292434692383,
      "learning_rate": 6.8668756090287705e-06,
      "loss": 1.942,
      "step": 669
    },
    {
      "epoch": 0.4410796576695194,
      "grad_norm": 33.01707458496094,
      "learning_rate": 6.856210870959567e-06,
      "loss": 1.8895,
      "step": 670
    },
    {
      "epoch": 0.44173798551678733,
      "grad_norm": 2.945484161376953,
      "learning_rate": 6.84553632919434e-06,
      "loss": 1.4479,
      "step": 671
    },
    {
      "epoch": 0.4423963133640553,
      "grad_norm": 4.060972213745117,
      "learning_rate": 6.834852040111364e-06,
      "loss": 1.4759,
      "step": 672
    },
    {
      "epoch": 0.44305464121132326,
      "grad_norm": 59.68422317504883,
      "learning_rate": 6.82415806014039e-06,
      "loss": 1.6178,
      "step": 673
    },
    {
      "epoch": 0.4437129690585912,
      "grad_norm": 35.6826286315918,
      "learning_rate": 6.813454445762357e-06,
      "loss": 2.2594,
      "step": 674
    },
    {
      "epoch": 0.44437129690585914,
      "grad_norm": 68.73306274414062,
      "learning_rate": 6.8027412535090835e-06,
      "loss": 2.1576,
      "step": 675
    },
    {
      "epoch": 0.4450296247531271,
      "grad_norm": 36.39014434814453,
      "learning_rate": 6.7920185399629815e-06,
      "loss": 2.0042,
      "step": 676
    },
    {
      "epoch": 0.445687952600395,
      "grad_norm": 1.9577221870422363,
      "learning_rate": 6.7812863617567415e-06,
      "loss": 1.4381,
      "step": 677
    },
    {
      "epoch": 0.44634628044766295,
      "grad_norm": 25.04003143310547,
      "learning_rate": 6.77054477557305e-06,
      "loss": 1.5425,
      "step": 678
    },
    {
      "epoch": 0.4470046082949309,
      "grad_norm": 27.952089309692383,
      "learning_rate": 6.759793838144278e-06,
      "loss": 2.0719,
      "step": 679
    },
    {
      "epoch": 0.4476629361421988,
      "grad_norm": 2.5846340656280518,
      "learning_rate": 6.7490336062521865e-06,
      "loss": 1.4475,
      "step": 680
    },
    {
      "epoch": 0.44832126398946676,
      "grad_norm": 2.474093198776245,
      "learning_rate": 6.7382641367276266e-06,
      "loss": 1.4465,
      "step": 681
    },
    {
      "epoch": 0.4489795918367347,
      "grad_norm": 6.03606653213501,
      "learning_rate": 6.727485486450238e-06,
      "loss": 1.4586,
      "step": 682
    },
    {
      "epoch": 0.44963791968400263,
      "grad_norm": 38.501304626464844,
      "learning_rate": 6.716697712348148e-06,
      "loss": 2.0741,
      "step": 683
    },
    {
      "epoch": 0.45029624753127057,
      "grad_norm": 4.135509967803955,
      "learning_rate": 6.705900871397675e-06,
      "loss": 1.4643,
      "step": 684
    },
    {
      "epoch": 0.4509545753785385,
      "grad_norm": 122.00706481933594,
      "learning_rate": 6.695095020623021e-06,
      "loss": 2.6554,
      "step": 685
    },
    {
      "epoch": 0.45161290322580644,
      "grad_norm": 9.033156394958496,
      "learning_rate": 6.6842802170959775e-06,
      "loss": 1.481,
      "step": 686
    },
    {
      "epoch": 0.4522712310730744,
      "grad_norm": 5.584234714508057,
      "learning_rate": 6.673456517935615e-06,
      "loss": 1.4728,
      "step": 687
    },
    {
      "epoch": 0.4529295589203423,
      "grad_norm": 44.213623046875,
      "learning_rate": 6.662623980307994e-06,
      "loss": 2.0284,
      "step": 688
    },
    {
      "epoch": 0.45358788676761025,
      "grad_norm": 117.30972290039062,
      "learning_rate": 6.651782661425851e-06,
      "loss": 2.1782,
      "step": 689
    },
    {
      "epoch": 0.4542462146148782,
      "grad_norm": 4.323456287384033,
      "learning_rate": 6.640932618548303e-06,
      "loss": 1.4393,
      "step": 690
    },
    {
      "epoch": 0.4549045424621461,
      "grad_norm": 2.182795524597168,
      "learning_rate": 6.630073908980541e-06,
      "loss": 1.44,
      "step": 691
    },
    {
      "epoch": 0.4555628703094141,
      "grad_norm": 120.89344787597656,
      "learning_rate": 6.619206590073537e-06,
      "loss": 2.21,
      "step": 692
    },
    {
      "epoch": 0.45622119815668205,
      "grad_norm": 29.164505004882812,
      "learning_rate": 6.6083307192237235e-06,
      "loss": 1.7184,
      "step": 693
    },
    {
      "epoch": 0.45687952600395,
      "grad_norm": 3.427557945251465,
      "learning_rate": 6.597446353872706e-06,
      "loss": 1.4406,
      "step": 694
    },
    {
      "epoch": 0.4575378538512179,
      "grad_norm": 49.32469177246094,
      "learning_rate": 6.5865535515069555e-06,
      "loss": 2.0745,
      "step": 695
    },
    {
      "epoch": 0.45819618169848586,
      "grad_norm": 9.500650405883789,
      "learning_rate": 6.575652369657502e-06,
      "loss": 1.4768,
      "step": 696
    },
    {
      "epoch": 0.4588545095457538,
      "grad_norm": 26.691707611083984,
      "learning_rate": 6.56474286589963e-06,
      "loss": 1.7569,
      "step": 697
    },
    {
      "epoch": 0.45951283739302173,
      "grad_norm": 127.25115966796875,
      "learning_rate": 6.5538250978525816e-06,
      "loss": 2.0806,
      "step": 698
    },
    {
      "epoch": 0.46017116524028967,
      "grad_norm": 85.90562438964844,
      "learning_rate": 6.542899123179245e-06,
      "loss": 1.6587,
      "step": 699
    },
    {
      "epoch": 0.4608294930875576,
      "grad_norm": 1.464855432510376,
      "learning_rate": 6.5319649995858496e-06,
      "loss": 1.4314,
      "step": 700
    },
    {
      "epoch": 0.46148782093482554,
      "grad_norm": 2.245194911956787,
      "learning_rate": 6.521022784821665e-06,
      "loss": 1.4378,
      "step": 701
    },
    {
      "epoch": 0.4621461487820935,
      "grad_norm": 4.51123571395874,
      "learning_rate": 6.510072536678699e-06,
      "loss": 1.4736,
      "step": 702
    },
    {
      "epoch": 0.4628044766293614,
      "grad_norm": 139.79959106445312,
      "learning_rate": 6.499114312991384e-06,
      "loss": 2.1033,
      "step": 703
    },
    {
      "epoch": 0.46346280447662935,
      "grad_norm": 16.007402420043945,
      "learning_rate": 6.488148171636276e-06,
      "loss": 1.5726,
      "step": 704
    },
    {
      "epoch": 0.4641211323238973,
      "grad_norm": 6.069209098815918,
      "learning_rate": 6.47717417053175e-06,
      "loss": 1.4719,
      "step": 705
    },
    {
      "epoch": 0.4647794601711652,
      "grad_norm": 3.5335938930511475,
      "learning_rate": 6.466192367637694e-06,
      "loss": 1.4342,
      "step": 706
    },
    {
      "epoch": 0.46543778801843316,
      "grad_norm": 86.18780517578125,
      "learning_rate": 6.4552028209551975e-06,
      "loss": 2.3532,
      "step": 707
    },
    {
      "epoch": 0.4660961158657011,
      "grad_norm": 53.64669418334961,
      "learning_rate": 6.444205588526252e-06,
      "loss": 2.0714,
      "step": 708
    },
    {
      "epoch": 0.46675444371296904,
      "grad_norm": 80.91067504882812,
      "learning_rate": 6.433200728433443e-06,
      "loss": 3.3442,
      "step": 709
    },
    {
      "epoch": 0.467412771560237,
      "grad_norm": 4.57351016998291,
      "learning_rate": 6.422188298799639e-06,
      "loss": 1.456,
      "step": 710
    },
    {
      "epoch": 0.46807109940750496,
      "grad_norm": 2.06532883644104,
      "learning_rate": 6.411168357787689e-06,
      "loss": 1.4359,
      "step": 711
    },
    {
      "epoch": 0.4687294272547729,
      "grad_norm": 2.361356019973755,
      "learning_rate": 6.400140963600116e-06,
      "loss": 1.4339,
      "step": 712
    },
    {
      "epoch": 0.46938775510204084,
      "grad_norm": 46.64920425415039,
      "learning_rate": 6.389106174478802e-06,
      "loss": 1.9791,
      "step": 713
    },
    {
      "epoch": 0.4700460829493088,
      "grad_norm": 4.305062770843506,
      "learning_rate": 6.378064048704692e-06,
      "loss": 1.4379,
      "step": 714
    },
    {
      "epoch": 0.4707044107965767,
      "grad_norm": 10.791182518005371,
      "learning_rate": 6.367014644597475e-06,
      "loss": 1.4741,
      "step": 715
    },
    {
      "epoch": 0.47136273864384465,
      "grad_norm": 1.8131804466247559,
      "learning_rate": 6.355958020515285e-06,
      "loss": 1.4355,
      "step": 716
    },
    {
      "epoch": 0.4720210664911126,
      "grad_norm": 36.65810775756836,
      "learning_rate": 6.344894234854384e-06,
      "loss": 1.7294,
      "step": 717
    },
    {
      "epoch": 0.4726793943383805,
      "grad_norm": 1.6260591745376587,
      "learning_rate": 6.333823346048864e-06,
      "loss": 1.4275,
      "step": 718
    },
    {
      "epoch": 0.47333772218564846,
      "grad_norm": 2.0756168365478516,
      "learning_rate": 6.322745412570328e-06,
      "loss": 1.4295,
      "step": 719
    },
    {
      "epoch": 0.4739960500329164,
      "grad_norm": 1.243680477142334,
      "learning_rate": 6.3116604929275895e-06,
      "loss": 1.4277,
      "step": 720
    },
    {
      "epoch": 0.47465437788018433,
      "grad_norm": 35.052066802978516,
      "learning_rate": 6.3005686456663565e-06,
      "loss": 1.782,
      "step": 721
    },
    {
      "epoch": 0.47531270572745227,
      "grad_norm": 1.7646652460098267,
      "learning_rate": 6.289469929368926e-06,
      "loss": 1.4266,
      "step": 722
    },
    {
      "epoch": 0.4759710335747202,
      "grad_norm": 56.3264045715332,
      "learning_rate": 6.278364402653879e-06,
      "loss": 2.1646,
      "step": 723
    },
    {
      "epoch": 0.47662936142198814,
      "grad_norm": 52.92108917236328,
      "learning_rate": 6.26725212417576e-06,
      "loss": 2.1432,
      "step": 724
    },
    {
      "epoch": 0.4772876892692561,
      "grad_norm": 5.641218185424805,
      "learning_rate": 6.256133152624777e-06,
      "loss": 1.433,
      "step": 725
    },
    {
      "epoch": 0.477946017116524,
      "grad_norm": 31.32530403137207,
      "learning_rate": 6.245007546726488e-06,
      "loss": 1.8893,
      "step": 726
    },
    {
      "epoch": 0.47860434496379195,
      "grad_norm": 3.576063394546509,
      "learning_rate": 6.233875365241488e-06,
      "loss": 1.4267,
      "step": 727
    },
    {
      "epoch": 0.4792626728110599,
      "grad_norm": 5.478438854217529,
      "learning_rate": 6.222736666965105e-06,
      "loss": 1.4595,
      "step": 728
    },
    {
      "epoch": 0.4799210006583278,
      "grad_norm": 4.588358402252197,
      "learning_rate": 6.211591510727079e-06,
      "loss": 1.4305,
      "step": 729
    },
    {
      "epoch": 0.48057932850559576,
      "grad_norm": 48.810394287109375,
      "learning_rate": 6.20043995539127e-06,
      "loss": 1.8553,
      "step": 730
    },
    {
      "epoch": 0.48123765635286375,
      "grad_norm": 2.850536823272705,
      "learning_rate": 6.1892820598553224e-06,
      "loss": 1.4233,
      "step": 731
    },
    {
      "epoch": 0.4818959842001317,
      "grad_norm": 56.866580963134766,
      "learning_rate": 6.178117883050376e-06,
      "loss": 1.9881,
      "step": 732
    },
    {
      "epoch": 0.4825543120473996,
      "grad_norm": 42.585426330566406,
      "learning_rate": 6.166947483940742e-06,
      "loss": 1.8952,
      "step": 733
    },
    {
      "epoch": 0.48321263989466756,
      "grad_norm": 15.529403686523438,
      "learning_rate": 6.155770921523594e-06,
      "loss": 1.579,
      "step": 734
    },
    {
      "epoch": 0.4838709677419355,
      "grad_norm": 7.760002136230469,
      "learning_rate": 6.144588254828657e-06,
      "loss": 1.4998,
      "step": 735
    },
    {
      "epoch": 0.48452929558920343,
      "grad_norm": 23.01686668395996,
      "learning_rate": 6.1333995429179015e-06,
      "loss": 1.7757,
      "step": 736
    },
    {
      "epoch": 0.48518762343647137,
      "grad_norm": 6.262246608734131,
      "learning_rate": 6.122204844885222e-06,
      "loss": 1.4895,
      "step": 737
    },
    {
      "epoch": 0.4858459512837393,
      "grad_norm": 12.508829116821289,
      "learning_rate": 6.111004219856128e-06,
      "loss": 1.4986,
      "step": 738
    },
    {
      "epoch": 0.48650427913100724,
      "grad_norm": 91.99333190917969,
      "learning_rate": 6.099797726987436e-06,
      "loss": 1.7774,
      "step": 739
    },
    {
      "epoch": 0.4871626069782752,
      "grad_norm": 2.4698646068573,
      "learning_rate": 6.088585425466952e-06,
      "loss": 1.4297,
      "step": 740
    },
    {
      "epoch": 0.4878209348255431,
      "grad_norm": 3.2048466205596924,
      "learning_rate": 6.07736737451316e-06,
      "loss": 1.4411,
      "step": 741
    },
    {
      "epoch": 0.48847926267281105,
      "grad_norm": 205.7655792236328,
      "learning_rate": 6.066143633374912e-06,
      "loss": 1.7271,
      "step": 742
    },
    {
      "epoch": 0.489137590520079,
      "grad_norm": 69.28873443603516,
      "learning_rate": 6.054914261331112e-06,
      "loss": 2.6783,
      "step": 743
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 3.3991024494171143,
      "learning_rate": 6.043679317690404e-06,
      "loss": 1.4303,
      "step": 744
    },
    {
      "epoch": 0.49045424621461486,
      "grad_norm": 47.941890716552734,
      "learning_rate": 6.032438861790859e-06,
      "loss": 1.8486,
      "step": 745
    },
    {
      "epoch": 0.4911125740618828,
      "grad_norm": 86.59538269042969,
      "learning_rate": 6.021192952999662e-06,
      "loss": 1.84,
      "step": 746
    },
    {
      "epoch": 0.49177090190915074,
      "grad_norm": 66.75851440429688,
      "learning_rate": 6.009941650712798e-06,
      "loss": 2.5907,
      "step": 747
    },
    {
      "epoch": 0.4924292297564187,
      "grad_norm": 159.54360961914062,
      "learning_rate": 5.998685014354731e-06,
      "loss": 2.2187,
      "step": 748
    },
    {
      "epoch": 0.4930875576036866,
      "grad_norm": 4.303661346435547,
      "learning_rate": 5.987423103378113e-06,
      "loss": 1.4306,
      "step": 749
    },
    {
      "epoch": 0.4937458854509546,
      "grad_norm": 16.646657943725586,
      "learning_rate": 5.976155977263438e-06,
      "loss": 1.6295,
      "step": 750
    },
    {
      "epoch": 0.49440421329822254,
      "grad_norm": 2.313730478286743,
      "learning_rate": 5.964883695518752e-06,
      "loss": 1.4317,
      "step": 751
    },
    {
      "epoch": 0.4950625411454905,
      "grad_norm": 1.5052971839904785,
      "learning_rate": 5.953606317679328e-06,
      "loss": 1.4163,
      "step": 752
    },
    {
      "epoch": 0.4957208689927584,
      "grad_norm": 40.75640106201172,
      "learning_rate": 5.942323903307361e-06,
      "loss": 1.7274,
      "step": 753
    },
    {
      "epoch": 0.49637919684002635,
      "grad_norm": 27.576156616210938,
      "learning_rate": 5.931036511991637e-06,
      "loss": 1.6746,
      "step": 754
    },
    {
      "epoch": 0.4970375246872943,
      "grad_norm": 45.47671890258789,
      "learning_rate": 5.919744203347233e-06,
      "loss": 1.8223,
      "step": 755
    },
    {
      "epoch": 0.4976958525345622,
      "grad_norm": 45.701698303222656,
      "learning_rate": 5.908447037015198e-06,
      "loss": 1.7704,
      "step": 756
    },
    {
      "epoch": 0.49835418038183016,
      "grad_norm": 25.370426177978516,
      "learning_rate": 5.897145072662237e-06,
      "loss": 1.8348,
      "step": 757
    },
    {
      "epoch": 0.4990125082290981,
      "grad_norm": 74.06967163085938,
      "learning_rate": 5.8858383699803926e-06,
      "loss": 2.8901,
      "step": 758
    },
    {
      "epoch": 0.49967083607636603,
      "grad_norm": 3.6806156635284424,
      "learning_rate": 5.8745269886867365e-06,
      "loss": 1.4401,
      "step": 759
    },
    {
      "epoch": 0.500329163923634,
      "grad_norm": 33.0919189453125,
      "learning_rate": 5.863210988523052e-06,
      "loss": 1.8292,
      "step": 760
    },
    {
      "epoch": 0.500987491770902,
      "grad_norm": 12.373385429382324,
      "learning_rate": 5.851890429255513e-06,
      "loss": 1.4477,
      "step": 761
    },
    {
      "epoch": 0.5016458196181699,
      "grad_norm": 2.974787712097168,
      "learning_rate": 5.840565370674375e-06,
      "loss": 1.4369,
      "step": 762
    },
    {
      "epoch": 0.5023041474654378,
      "grad_norm": 8.506070137023926,
      "learning_rate": 5.829235872593657e-06,
      "loss": 1.4374,
      "step": 763
    },
    {
      "epoch": 0.5029624753127058,
      "grad_norm": 12.077237129211426,
      "learning_rate": 5.817901994850825e-06,
      "loss": 1.5689,
      "step": 764
    },
    {
      "epoch": 0.5036208031599737,
      "grad_norm": 27.6811466217041,
      "learning_rate": 5.806563797306478e-06,
      "loss": 1.7629,
      "step": 765
    },
    {
      "epoch": 0.5042791310072416,
      "grad_norm": 118.4969482421875,
      "learning_rate": 5.795221339844028e-06,
      "loss": 2.2629,
      "step": 766
    },
    {
      "epoch": 0.5049374588545096,
      "grad_norm": 37.20933532714844,
      "learning_rate": 5.783874682369386e-06,
      "loss": 1.8146,
      "step": 767
    },
    {
      "epoch": 0.5055957867017775,
      "grad_norm": 1.861580729484558,
      "learning_rate": 5.772523884810649e-06,
      "loss": 1.4252,
      "step": 768
    },
    {
      "epoch": 0.5062541145490455,
      "grad_norm": 11.454998970031738,
      "learning_rate": 5.761169007117775e-06,
      "loss": 1.5139,
      "step": 769
    },
    {
      "epoch": 0.5069124423963134,
      "grad_norm": 1.523589849472046,
      "learning_rate": 5.7498101092622775e-06,
      "loss": 1.4164,
      "step": 770
    },
    {
      "epoch": 0.5075707702435813,
      "grad_norm": 1.8741952180862427,
      "learning_rate": 5.738447251236895e-06,
      "loss": 1.4214,
      "step": 771
    },
    {
      "epoch": 0.5082290980908493,
      "grad_norm": 48.79371643066406,
      "learning_rate": 5.72708049305529e-06,
      "loss": 2.0262,
      "step": 772
    },
    {
      "epoch": 0.5088874259381172,
      "grad_norm": 46.7109375,
      "learning_rate": 5.715709894751721e-06,
      "loss": 1.8282,
      "step": 773
    },
    {
      "epoch": 0.5095457537853851,
      "grad_norm": 216.79690551757812,
      "learning_rate": 5.7043355163807255e-06,
      "loss": 2.657,
      "step": 774
    },
    {
      "epoch": 0.5102040816326531,
      "grad_norm": 22.49699592590332,
      "learning_rate": 5.692957418016806e-06,
      "loss": 1.6291,
      "step": 775
    },
    {
      "epoch": 0.510862409479921,
      "grad_norm": 2.313462018966675,
      "learning_rate": 5.681575659754119e-06,
      "loss": 1.4175,
      "step": 776
    },
    {
      "epoch": 0.511520737327189,
      "grad_norm": 33.02191925048828,
      "learning_rate": 5.670190301706143e-06,
      "loss": 1.6669,
      "step": 777
    },
    {
      "epoch": 0.5121790651744569,
      "grad_norm": 36.53487777709961,
      "learning_rate": 5.658801404005373e-06,
      "loss": 1.601,
      "step": 778
    },
    {
      "epoch": 0.5128373930217248,
      "grad_norm": 42.239864349365234,
      "learning_rate": 5.647409026802998e-06,
      "loss": 1.777,
      "step": 779
    },
    {
      "epoch": 0.5134957208689928,
      "grad_norm": 38.9553337097168,
      "learning_rate": 5.636013230268585e-06,
      "loss": 1.6029,
      "step": 780
    },
    {
      "epoch": 0.5141540487162607,
      "grad_norm": 41.33475875854492,
      "learning_rate": 5.624614074589759e-06,
      "loss": 1.6982,
      "step": 781
    },
    {
      "epoch": 0.5148123765635286,
      "grad_norm": 3.5714569091796875,
      "learning_rate": 5.613211619971885e-06,
      "loss": 1.4296,
      "step": 782
    },
    {
      "epoch": 0.5154707044107966,
      "grad_norm": 136.7360382080078,
      "learning_rate": 5.601805926637759e-06,
      "loss": 3.5173,
      "step": 783
    },
    {
      "epoch": 0.5161290322580645,
      "grad_norm": 11.184842109680176,
      "learning_rate": 5.590397054827275e-06,
      "loss": 1.4623,
      "step": 784
    },
    {
      "epoch": 0.5167873601053324,
      "grad_norm": 2.3280551433563232,
      "learning_rate": 5.578985064797113e-06,
      "loss": 1.4146,
      "step": 785
    },
    {
      "epoch": 0.5174456879526004,
      "grad_norm": 9.13265609741211,
      "learning_rate": 5.5675700168204305e-06,
      "loss": 1.4252,
      "step": 786
    },
    {
      "epoch": 0.5181040157998683,
      "grad_norm": 18.943389892578125,
      "learning_rate": 5.55615197118653e-06,
      "loss": 1.5977,
      "step": 787
    },
    {
      "epoch": 0.5187623436471362,
      "grad_norm": 55.900760650634766,
      "learning_rate": 5.544730988200546e-06,
      "loss": 2.4156,
      "step": 788
    },
    {
      "epoch": 0.5194206714944042,
      "grad_norm": 53.277164459228516,
      "learning_rate": 5.533307128183127e-06,
      "loss": 2.202,
      "step": 789
    },
    {
      "epoch": 0.5200789993416721,
      "grad_norm": 14.16682243347168,
      "learning_rate": 5.521880451470121e-06,
      "loss": 1.4807,
      "step": 790
    },
    {
      "epoch": 0.5207373271889401,
      "grad_norm": 2.3056797981262207,
      "learning_rate": 5.5104510184122475e-06,
      "loss": 1.4376,
      "step": 791
    },
    {
      "epoch": 0.521395655036208,
      "grad_norm": 43.55756378173828,
      "learning_rate": 5.499018889374785e-06,
      "loss": 2.363,
      "step": 792
    },
    {
      "epoch": 0.5220539828834759,
      "grad_norm": 24.586313247680664,
      "learning_rate": 5.487584124737254e-06,
      "loss": 1.6838,
      "step": 793
    },
    {
      "epoch": 0.5227123107307439,
      "grad_norm": 10.203619956970215,
      "learning_rate": 5.476146784893091e-06,
      "loss": 1.486,
      "step": 794
    },
    {
      "epoch": 0.5233706385780118,
      "grad_norm": 1.4927797317504883,
      "learning_rate": 5.464706930249335e-06,
      "loss": 1.4185,
      "step": 795
    },
    {
      "epoch": 0.5240289664252797,
      "grad_norm": 1.6574357748031616,
      "learning_rate": 5.453264621226309e-06,
      "loss": 1.4201,
      "step": 796
    },
    {
      "epoch": 0.5246872942725477,
      "grad_norm": 34.17439270019531,
      "learning_rate": 5.441819918257296e-06,
      "loss": 1.4759,
      "step": 797
    },
    {
      "epoch": 0.5253456221198156,
      "grad_norm": 38.61821746826172,
      "learning_rate": 5.430372881788224e-06,
      "loss": 1.7841,
      "step": 798
    },
    {
      "epoch": 0.5260039499670837,
      "grad_norm": 26.243940353393555,
      "learning_rate": 5.418923572277346e-06,
      "loss": 1.9243,
      "step": 799
    },
    {
      "epoch": 0.5266622778143516,
      "grad_norm": 82.84605407714844,
      "learning_rate": 5.407472050194922e-06,
      "loss": 2.6568,
      "step": 800
    },
    {
      "epoch": 0.5273206056616195,
      "grad_norm": 35.07868957519531,
      "learning_rate": 5.396018376022894e-06,
      "loss": 2.0966,
      "step": 801
    },
    {
      "epoch": 0.5279789335088875,
      "grad_norm": 125.05119323730469,
      "learning_rate": 5.3845626102545704e-06,
      "loss": 1.6221,
      "step": 802
    },
    {
      "epoch": 0.5286372613561554,
      "grad_norm": 140.93162536621094,
      "learning_rate": 5.373104813394311e-06,
      "loss": 3.1787,
      "step": 803
    },
    {
      "epoch": 0.5292955892034233,
      "grad_norm": 5.182455539703369,
      "learning_rate": 5.3616450459572e-06,
      "loss": 1.441,
      "step": 804
    },
    {
      "epoch": 0.5299539170506913,
      "grad_norm": 6.27827262878418,
      "learning_rate": 5.3501833684687275e-06,
      "loss": 1.45,
      "step": 805
    },
    {
      "epoch": 0.5306122448979592,
      "grad_norm": 19.032882690429688,
      "learning_rate": 5.338719841464476e-06,
      "loss": 1.689,
      "step": 806
    },
    {
      "epoch": 0.5312705727452272,
      "grad_norm": 2.0947365760803223,
      "learning_rate": 5.327254525489793e-06,
      "loss": 1.4192,
      "step": 807
    },
    {
      "epoch": 0.5319289005924951,
      "grad_norm": 56.273006439208984,
      "learning_rate": 5.315787481099476e-06,
      "loss": 2.2506,
      "step": 808
    },
    {
      "epoch": 0.532587228439763,
      "grad_norm": 1.8010061979293823,
      "learning_rate": 5.3043187688574486e-06,
      "loss": 1.4162,
      "step": 809
    },
    {
      "epoch": 0.533245556287031,
      "grad_norm": 2.3851335048675537,
      "learning_rate": 5.2928484493364485e-06,
      "loss": 1.4295,
      "step": 810
    },
    {
      "epoch": 0.5339038841342989,
      "grad_norm": 1.1117370128631592,
      "learning_rate": 5.281376583117699e-06,
      "loss": 1.4094,
      "step": 811
    },
    {
      "epoch": 0.5345622119815668,
      "grad_norm": 14.504744529724121,
      "learning_rate": 5.2699032307905894e-06,
      "loss": 1.5499,
      "step": 812
    },
    {
      "epoch": 0.5352205398288348,
      "grad_norm": 2.1706430912017822,
      "learning_rate": 5.258428452952365e-06,
      "loss": 1.4118,
      "step": 813
    },
    {
      "epoch": 0.5358788676761027,
      "grad_norm": 53.78148651123047,
      "learning_rate": 5.246952310207796e-06,
      "loss": 2.46,
      "step": 814
    },
    {
      "epoch": 0.5365371955233706,
      "grad_norm": 3.542602062225342,
      "learning_rate": 5.23547486316886e-06,
      "loss": 1.4315,
      "step": 815
    },
    {
      "epoch": 0.5371955233706386,
      "grad_norm": 1.9462716579437256,
      "learning_rate": 5.223996172454425e-06,
      "loss": 1.4157,
      "step": 816
    },
    {
      "epoch": 0.5378538512179065,
      "grad_norm": 43.11045837402344,
      "learning_rate": 5.212516298689929e-06,
      "loss": 1.9916,
      "step": 817
    },
    {
      "epoch": 0.5385121790651745,
      "grad_norm": 36.56045150756836,
      "learning_rate": 5.201035302507057e-06,
      "loss": 2.0805,
      "step": 818
    },
    {
      "epoch": 0.5391705069124424,
      "grad_norm": 1.7985022068023682,
      "learning_rate": 5.18955324454342e-06,
      "loss": 1.4103,
      "step": 819
    },
    {
      "epoch": 0.5398288347597103,
      "grad_norm": 48.33369827270508,
      "learning_rate": 5.178070185442242e-06,
      "loss": 2.0539,
      "step": 820
    },
    {
      "epoch": 0.5404871626069783,
      "grad_norm": 1.8295447826385498,
      "learning_rate": 5.1665861858520296e-06,
      "loss": 1.4127,
      "step": 821
    },
    {
      "epoch": 0.5411454904542462,
      "grad_norm": 39.35807800292969,
      "learning_rate": 5.15510130642626e-06,
      "loss": 1.6,
      "step": 822
    },
    {
      "epoch": 0.5418038183015141,
      "grad_norm": 32.019290924072266,
      "learning_rate": 5.143615607823052e-06,
      "loss": 1.658,
      "step": 823
    },
    {
      "epoch": 0.5424621461487821,
      "grad_norm": 16.448415756225586,
      "learning_rate": 5.132129150704861e-06,
      "loss": 1.5575,
      "step": 824
    },
    {
      "epoch": 0.54312047399605,
      "grad_norm": 32.65312194824219,
      "learning_rate": 5.120641995738137e-06,
      "loss": 1.8561,
      "step": 825
    },
    {
      "epoch": 0.543778801843318,
      "grad_norm": 35.234703063964844,
      "learning_rate": 5.109154203593023e-06,
      "loss": 1.5574,
      "step": 826
    },
    {
      "epoch": 0.5444371296905859,
      "grad_norm": 72.4262466430664,
      "learning_rate": 5.097665834943027e-06,
      "loss": 1.992,
      "step": 827
    },
    {
      "epoch": 0.5450954575378538,
      "grad_norm": 2.2335803508758545,
      "learning_rate": 5.0861769504646985e-06,
      "loss": 1.4108,
      "step": 828
    },
    {
      "epoch": 0.5457537853851218,
      "grad_norm": 59.86720657348633,
      "learning_rate": 5.074687610837314e-06,
      "loss": 2.1934,
      "step": 829
    },
    {
      "epoch": 0.5464121132323897,
      "grad_norm": 3.0745368003845215,
      "learning_rate": 5.063197876742552e-06,
      "loss": 1.4213,
      "step": 830
    },
    {
      "epoch": 0.5470704410796576,
      "grad_norm": 16.640031814575195,
      "learning_rate": 5.051707808864176e-06,
      "loss": 1.4597,
      "step": 831
    },
    {
      "epoch": 0.5477287689269256,
      "grad_norm": 26.555967330932617,
      "learning_rate": 5.040217467887711e-06,
      "loss": 1.5602,
      "step": 832
    },
    {
      "epoch": 0.5483870967741935,
      "grad_norm": 47.167293548583984,
      "learning_rate": 5.028726914500126e-06,
      "loss": 2.0,
      "step": 833
    },
    {
      "epoch": 0.5490454246214614,
      "grad_norm": 4.488686561584473,
      "learning_rate": 5.0172362093895136e-06,
      "loss": 1.4119,
      "step": 834
    },
    {
      "epoch": 0.5497037524687294,
      "grad_norm": 237.36863708496094,
      "learning_rate": 5.005745413244762e-06,
      "loss": 2.4584,
      "step": 835
    },
    {
      "epoch": 0.5503620803159973,
      "grad_norm": 30.601701736450195,
      "learning_rate": 4.994254586755242e-06,
      "loss": 1.6621,
      "step": 836
    },
    {
      "epoch": 0.5510204081632653,
      "grad_norm": 32.4807014465332,
      "learning_rate": 4.982763790610489e-06,
      "loss": 1.7605,
      "step": 837
    },
    {
      "epoch": 0.5516787360105333,
      "grad_norm": 2.7792320251464844,
      "learning_rate": 4.971273085499874e-06,
      "loss": 1.4083,
      "step": 838
    },
    {
      "epoch": 0.5523370638578012,
      "grad_norm": 46.70012283325195,
      "learning_rate": 4.95978253211229e-06,
      "loss": 1.8997,
      "step": 839
    },
    {
      "epoch": 0.5529953917050692,
      "grad_norm": 67.20664978027344,
      "learning_rate": 4.948292191135826e-06,
      "loss": 2.3835,
      "step": 840
    },
    {
      "epoch": 0.5536537195523371,
      "grad_norm": 3.810805082321167,
      "learning_rate": 4.936802123257451e-06,
      "loss": 1.4137,
      "step": 841
    },
    {
      "epoch": 0.554312047399605,
      "grad_norm": 3.4050979614257812,
      "learning_rate": 4.925312389162689e-06,
      "loss": 1.411,
      "step": 842
    },
    {
      "epoch": 0.554970375246873,
      "grad_norm": 33.53762435913086,
      "learning_rate": 4.9138230495353015e-06,
      "loss": 1.9399,
      "step": 843
    },
    {
      "epoch": 0.5556287030941409,
      "grad_norm": 1.8173846006393433,
      "learning_rate": 4.902334165056975e-06,
      "loss": 1.4081,
      "step": 844
    },
    {
      "epoch": 0.5562870309414089,
      "grad_norm": 159.2578582763672,
      "learning_rate": 4.890845796406978e-06,
      "loss": 1.8489,
      "step": 845
    },
    {
      "epoch": 0.5569453587886768,
      "grad_norm": 1.1298421621322632,
      "learning_rate": 4.879358004261864e-06,
      "loss": 1.4051,
      "step": 846
    },
    {
      "epoch": 0.5576036866359447,
      "grad_norm": 1.6303741931915283,
      "learning_rate": 4.867870849295142e-06,
      "loss": 1.4074,
      "step": 847
    },
    {
      "epoch": 0.5582620144832127,
      "grad_norm": 1.4655779600143433,
      "learning_rate": 4.8563843921769485e-06,
      "loss": 1.4084,
      "step": 848
    },
    {
      "epoch": 0.5589203423304806,
      "grad_norm": 15.49639892578125,
      "learning_rate": 4.844898693573742e-06,
      "loss": 1.5085,
      "step": 849
    },
    {
      "epoch": 0.5595786701777485,
      "grad_norm": 99.51223754882812,
      "learning_rate": 4.833413814147972e-06,
      "loss": 1.6712,
      "step": 850
    },
    {
      "epoch": 0.5602369980250165,
      "grad_norm": 35.99299621582031,
      "learning_rate": 4.821929814557759e-06,
      "loss": 1.7501,
      "step": 851
    },
    {
      "epoch": 0.5608953258722844,
      "grad_norm": 2.2921812534332275,
      "learning_rate": 4.810446755456581e-06,
      "loss": 1.4067,
      "step": 852
    },
    {
      "epoch": 0.5615536537195523,
      "grad_norm": 31.351993560791016,
      "learning_rate": 4.798964697492945e-06,
      "loss": 1.9616,
      "step": 853
    },
    {
      "epoch": 0.5622119815668203,
      "grad_norm": 6.3562116622924805,
      "learning_rate": 4.787483701310072e-06,
      "loss": 1.4214,
      "step": 854
    },
    {
      "epoch": 0.5628703094140882,
      "grad_norm": 46.10549545288086,
      "learning_rate": 4.776003827545577e-06,
      "loss": 2.0723,
      "step": 855
    },
    {
      "epoch": 0.5635286372613562,
      "grad_norm": 53.42036056518555,
      "learning_rate": 4.764525136831141e-06,
      "loss": 1.5068,
      "step": 856
    },
    {
      "epoch": 0.5641869651086241,
      "grad_norm": 26.707307815551758,
      "learning_rate": 4.753047689792204e-06,
      "loss": 1.8503,
      "step": 857
    },
    {
      "epoch": 0.564845292955892,
      "grad_norm": 3.139026403427124,
      "learning_rate": 4.741571547047636e-06,
      "loss": 1.4105,
      "step": 858
    },
    {
      "epoch": 0.56550362080316,
      "grad_norm": 82.0301742553711,
      "learning_rate": 4.730096769209412e-06,
      "loss": 2.2158,
      "step": 859
    },
    {
      "epoch": 0.5661619486504279,
      "grad_norm": 12.804985046386719,
      "learning_rate": 4.7186234168823045e-06,
      "loss": 1.4499,
      "step": 860
    },
    {
      "epoch": 0.5668202764976958,
      "grad_norm": 2.135085105895996,
      "learning_rate": 4.707151550663553e-06,
      "loss": 1.4054,
      "step": 861
    },
    {
      "epoch": 0.5674786043449638,
      "grad_norm": 4.3563008308410645,
      "learning_rate": 4.695681231142552e-06,
      "loss": 1.4148,
      "step": 862
    },
    {
      "epoch": 0.5681369321922317,
      "grad_norm": 21.771757125854492,
      "learning_rate": 4.684212518900526e-06,
      "loss": 1.6757,
      "step": 863
    },
    {
      "epoch": 0.5687952600394997,
      "grad_norm": 31.20212745666504,
      "learning_rate": 4.672745474510208e-06,
      "loss": 1.9209,
      "step": 864
    },
    {
      "epoch": 0.5694535878867676,
      "grad_norm": 28.841760635375977,
      "learning_rate": 4.661280158535525e-06,
      "loss": 1.7054,
      "step": 865
    },
    {
      "epoch": 0.5701119157340355,
      "grad_norm": 6.3955159187316895,
      "learning_rate": 4.649816631531273e-06,
      "loss": 1.4313,
      "step": 866
    },
    {
      "epoch": 0.5707702435813035,
      "grad_norm": 421.71173095703125,
      "learning_rate": 4.638354954042801e-06,
      "loss": 3.6714,
      "step": 867
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 46.51030349731445,
      "learning_rate": 4.626895186605691e-06,
      "loss": 2.0282,
      "step": 868
    },
    {
      "epoch": 0.5720868992758393,
      "grad_norm": 17.064058303833008,
      "learning_rate": 4.615437389745431e-06,
      "loss": 1.5062,
      "step": 869
    },
    {
      "epoch": 0.5727452271231073,
      "grad_norm": 2.031435489654541,
      "learning_rate": 4.603981623977107e-06,
      "loss": 1.4068,
      "step": 870
    },
    {
      "epoch": 0.5734035549703752,
      "grad_norm": 196.8194122314453,
      "learning_rate": 4.59252794980508e-06,
      "loss": 2.5429,
      "step": 871
    },
    {
      "epoch": 0.5740618828176431,
      "grad_norm": 31.973037719726562,
      "learning_rate": 4.581076427722655e-06,
      "loss": 1.8867,
      "step": 872
    },
    {
      "epoch": 0.5747202106649111,
      "grad_norm": 2.445131778717041,
      "learning_rate": 4.5696271182117775e-06,
      "loss": 1.4087,
      "step": 873
    },
    {
      "epoch": 0.575378538512179,
      "grad_norm": 21.2620849609375,
      "learning_rate": 4.558180081742707e-06,
      "loss": 1.5241,
      "step": 874
    },
    {
      "epoch": 0.576036866359447,
      "grad_norm": 1.7320877313613892,
      "learning_rate": 4.546735378773694e-06,
      "loss": 1.4044,
      "step": 875
    },
    {
      "epoch": 0.5766951942067149,
      "grad_norm": 106.62492370605469,
      "learning_rate": 4.535293069750666e-06,
      "loss": 3.6113,
      "step": 876
    },
    {
      "epoch": 0.5773535220539829,
      "grad_norm": 23.174753189086914,
      "learning_rate": 4.523853215106911e-06,
      "loss": 1.5024,
      "step": 877
    },
    {
      "epoch": 0.5780118499012509,
      "grad_norm": 20.077611923217773,
      "learning_rate": 4.512415875262747e-06,
      "loss": 1.699,
      "step": 878
    },
    {
      "epoch": 0.5786701777485188,
      "grad_norm": 1.5587633848190308,
      "learning_rate": 4.500981110625216e-06,
      "loss": 1.4044,
      "step": 879
    },
    {
      "epoch": 0.5793285055957867,
      "grad_norm": 80.48184967041016,
      "learning_rate": 4.489548981587753e-06,
      "loss": 1.9422,
      "step": 880
    },
    {
      "epoch": 0.5799868334430547,
      "grad_norm": 27.397253036499023,
      "learning_rate": 4.47811954852988e-06,
      "loss": 1.9425,
      "step": 881
    },
    {
      "epoch": 0.5806451612903226,
      "grad_norm": 1.7962990999221802,
      "learning_rate": 4.466692871816875e-06,
      "loss": 1.4074,
      "step": 882
    },
    {
      "epoch": 0.5813034891375906,
      "grad_norm": 31.792110443115234,
      "learning_rate": 4.455269011799456e-06,
      "loss": 1.8899,
      "step": 883
    },
    {
      "epoch": 0.5819618169848585,
      "grad_norm": 8.816000938415527,
      "learning_rate": 4.443848028813471e-06,
      "loss": 1.4332,
      "step": 884
    },
    {
      "epoch": 0.5826201448321264,
      "grad_norm": 10.415184020996094,
      "learning_rate": 4.432429983179571e-06,
      "loss": 1.4201,
      "step": 885
    },
    {
      "epoch": 0.5832784726793944,
      "grad_norm": 37.302860260009766,
      "learning_rate": 4.421014935202888e-06,
      "loss": 1.8977,
      "step": 886
    },
    {
      "epoch": 0.5839368005266623,
      "grad_norm": 22.269790649414062,
      "learning_rate": 4.409602945172729e-06,
      "loss": 1.6397,
      "step": 887
    },
    {
      "epoch": 0.5845951283739302,
      "grad_norm": 51.06690216064453,
      "learning_rate": 4.398194073362243e-06,
      "loss": 1.8104,
      "step": 888
    },
    {
      "epoch": 0.5852534562211982,
      "grad_norm": 2.206472396850586,
      "learning_rate": 4.386788380028115e-06,
      "loss": 1.4033,
      "step": 889
    },
    {
      "epoch": 0.5859117840684661,
      "grad_norm": 150.2822723388672,
      "learning_rate": 4.375385925410244e-06,
      "loss": 2.1344,
      "step": 890
    },
    {
      "epoch": 0.586570111915734,
      "grad_norm": 2.8541715145111084,
      "learning_rate": 4.363986769731416e-06,
      "loss": 1.4059,
      "step": 891
    },
    {
      "epoch": 0.587228439763002,
      "grad_norm": 105.74165344238281,
      "learning_rate": 4.352590973197003e-06,
      "loss": 1.7447,
      "step": 892
    },
    {
      "epoch": 0.5878867676102699,
      "grad_norm": 2.3814942836761475,
      "learning_rate": 4.3411985959946286e-06,
      "loss": 1.4008,
      "step": 893
    },
    {
      "epoch": 0.5885450954575379,
      "grad_norm": 1.6948471069335938,
      "learning_rate": 4.3298096982938584e-06,
      "loss": 1.4019,
      "step": 894
    },
    {
      "epoch": 0.5892034233048058,
      "grad_norm": 46.49648666381836,
      "learning_rate": 4.318424340245882e-06,
      "loss": 1.617,
      "step": 895
    },
    {
      "epoch": 0.5898617511520737,
      "grad_norm": 100.79780578613281,
      "learning_rate": 4.307042581983196e-06,
      "loss": 2.4537,
      "step": 896
    },
    {
      "epoch": 0.5905200789993417,
      "grad_norm": 6.3727521896362305,
      "learning_rate": 4.295664483619276e-06,
      "loss": 1.4498,
      "step": 897
    },
    {
      "epoch": 0.5911784068466096,
      "grad_norm": 31.274288177490234,
      "learning_rate": 4.284290105248282e-06,
      "loss": 1.659,
      "step": 898
    },
    {
      "epoch": 0.5918367346938775,
      "grad_norm": 66.16429138183594,
      "learning_rate": 4.272919506944711e-06,
      "loss": 2.0357,
      "step": 899
    },
    {
      "epoch": 0.5924950625411455,
      "grad_norm": 54.85755920410156,
      "learning_rate": 4.261552748763106e-06,
      "loss": 2.0583,
      "step": 900
    },
    {
      "epoch": 0.5931533903884134,
      "grad_norm": 87.87657165527344,
      "learning_rate": 4.250189890737726e-06,
      "loss": 2.3549,
      "step": 901
    },
    {
      "epoch": 0.5938117182356814,
      "grad_norm": 5.132814407348633,
      "learning_rate": 4.238830992882226e-06,
      "loss": 1.4093,
      "step": 902
    },
    {
      "epoch": 0.5944700460829493,
      "grad_norm": 43.03925704956055,
      "learning_rate": 4.227476115189353e-06,
      "loss": 1.9797,
      "step": 903
    },
    {
      "epoch": 0.5951283739302172,
      "grad_norm": 214.93087768554688,
      "learning_rate": 4.216125317630616e-06,
      "loss": 2.5436,
      "step": 904
    },
    {
      "epoch": 0.5957867017774852,
      "grad_norm": 21.69081687927246,
      "learning_rate": 4.2047786601559735e-06,
      "loss": 1.6732,
      "step": 905
    },
    {
      "epoch": 0.5964450296247531,
      "grad_norm": 42.47734069824219,
      "learning_rate": 4.193436202693524e-06,
      "loss": 2.2306,
      "step": 906
    },
    {
      "epoch": 0.597103357472021,
      "grad_norm": 2.432072401046753,
      "learning_rate": 4.182098005149176e-06,
      "loss": 1.4085,
      "step": 907
    },
    {
      "epoch": 0.597761685319289,
      "grad_norm": 49.14797592163086,
      "learning_rate": 4.170764127406344e-06,
      "loss": 1.9765,
      "step": 908
    },
    {
      "epoch": 0.5984200131665569,
      "grad_norm": 3.9710512161254883,
      "learning_rate": 4.159434629325628e-06,
      "loss": 1.4231,
      "step": 909
    },
    {
      "epoch": 0.5990783410138248,
      "grad_norm": 7.8699421882629395,
      "learning_rate": 4.1481095707444885e-06,
      "loss": 1.4523,
      "step": 910
    },
    {
      "epoch": 0.5997366688610928,
      "grad_norm": 42.802310943603516,
      "learning_rate": 4.136789011476949e-06,
      "loss": 2.1351,
      "step": 911
    },
    {
      "epoch": 0.6003949967083607,
      "grad_norm": 32.77323532104492,
      "learning_rate": 4.125473011313265e-06,
      "loss": 1.7164,
      "step": 912
    },
    {
      "epoch": 0.6010533245556287,
      "grad_norm": 23.140178680419922,
      "learning_rate": 4.114161630019609e-06,
      "loss": 1.6129,
      "step": 913
    },
    {
      "epoch": 0.6017116524028966,
      "grad_norm": 33.039329528808594,
      "learning_rate": 4.1028549273377635e-06,
      "loss": 1.8802,
      "step": 914
    },
    {
      "epoch": 0.6023699802501645,
      "grad_norm": 1.4942359924316406,
      "learning_rate": 4.091552962984804e-06,
      "loss": 1.4028,
      "step": 915
    },
    {
      "epoch": 0.6030283080974326,
      "grad_norm": 34.478267669677734,
      "learning_rate": 4.080255796652768e-06,
      "loss": 1.7155,
      "step": 916
    },
    {
      "epoch": 0.6036866359447005,
      "grad_norm": 127.77173614501953,
      "learning_rate": 4.068963488008365e-06,
      "loss": 2.3571,
      "step": 917
    },
    {
      "epoch": 0.6043449637919684,
      "grad_norm": 1.678113341331482,
      "learning_rate": 4.057676096692641e-06,
      "loss": 1.4006,
      "step": 918
    },
    {
      "epoch": 0.6050032916392364,
      "grad_norm": 19.095722198486328,
      "learning_rate": 4.046393682320672e-06,
      "loss": 1.7232,
      "step": 919
    },
    {
      "epoch": 0.6056616194865043,
      "grad_norm": 6.160574913024902,
      "learning_rate": 4.035116304481251e-06,
      "loss": 1.4355,
      "step": 920
    },
    {
      "epoch": 0.6063199473337723,
      "grad_norm": 47.3324089050293,
      "learning_rate": 4.023844022736564e-06,
      "loss": 2.1772,
      "step": 921
    },
    {
      "epoch": 0.6069782751810402,
      "grad_norm": 1.4909039735794067,
      "learning_rate": 4.012576896621888e-06,
      "loss": 1.4095,
      "step": 922
    },
    {
      "epoch": 0.6076366030283081,
      "grad_norm": 2.4706246852874756,
      "learning_rate": 4.0013149856452694e-06,
      "loss": 1.4044,
      "step": 923
    },
    {
      "epoch": 0.6082949308755761,
      "grad_norm": 2.1402218341827393,
      "learning_rate": 3.990058349287203e-06,
      "loss": 1.4122,
      "step": 924
    },
    {
      "epoch": 0.608953258722844,
      "grad_norm": 2.8751108646392822,
      "learning_rate": 3.978807047000339e-06,
      "loss": 1.3995,
      "step": 925
    },
    {
      "epoch": 0.6096115865701119,
      "grad_norm": 2.1580309867858887,
      "learning_rate": 3.967561138209142e-06,
      "loss": 1.4011,
      "step": 926
    },
    {
      "epoch": 0.6102699144173799,
      "grad_norm": 2.118558168411255,
      "learning_rate": 3.956320682309597e-06,
      "loss": 1.4031,
      "step": 927
    },
    {
      "epoch": 0.6109282422646478,
      "grad_norm": 34.146202087402344,
      "learning_rate": 3.94508573866889e-06,
      "loss": 1.8092,
      "step": 928
    },
    {
      "epoch": 0.6115865701119158,
      "grad_norm": 96.53311157226562,
      "learning_rate": 3.933856366625089e-06,
      "loss": 2.5023,
      "step": 929
    },
    {
      "epoch": 0.6122448979591837,
      "grad_norm": 22.77658462524414,
      "learning_rate": 3.922632625486841e-06,
      "loss": 1.6227,
      "step": 930
    },
    {
      "epoch": 0.6129032258064516,
      "grad_norm": 4.536901473999023,
      "learning_rate": 3.91141457453305e-06,
      "loss": 1.4346,
      "step": 931
    },
    {
      "epoch": 0.6135615536537196,
      "grad_norm": 3.029078960418701,
      "learning_rate": 3.900202273012565e-06,
      "loss": 1.4059,
      "step": 932
    },
    {
      "epoch": 0.6142198815009875,
      "grad_norm": 3.358987331390381,
      "learning_rate": 3.888995780143873e-06,
      "loss": 1.4012,
      "step": 933
    },
    {
      "epoch": 0.6148782093482554,
      "grad_norm": 4.827092170715332,
      "learning_rate": 3.877795155114779e-06,
      "loss": 1.4092,
      "step": 934
    },
    {
      "epoch": 0.6155365371955234,
      "grad_norm": 4.630693435668945,
      "learning_rate": 3.8666004570820985e-06,
      "loss": 1.4069,
      "step": 935
    },
    {
      "epoch": 0.6161948650427913,
      "grad_norm": 46.155906677246094,
      "learning_rate": 3.855411745171345e-06,
      "loss": 1.832,
      "step": 936
    },
    {
      "epoch": 0.6168531928900592,
      "grad_norm": 1.925965428352356,
      "learning_rate": 3.844229078476408e-06,
      "loss": 1.3968,
      "step": 937
    },
    {
      "epoch": 0.6175115207373272,
      "grad_norm": 76.66065216064453,
      "learning_rate": 3.833052516059259e-06,
      "loss": 2.0656,
      "step": 938
    },
    {
      "epoch": 0.6181698485845951,
      "grad_norm": 55.11357116699219,
      "learning_rate": 3.821882116949625e-06,
      "loss": 1.8776,
      "step": 939
    },
    {
      "epoch": 0.618828176431863,
      "grad_norm": 54.72166061401367,
      "learning_rate": 3.810717940144679e-06,
      "loss": 2.1585,
      "step": 940
    },
    {
      "epoch": 0.619486504279131,
      "grad_norm": 4.303915023803711,
      "learning_rate": 3.7995600446087303e-06,
      "loss": 1.4368,
      "step": 941
    },
    {
      "epoch": 0.6201448321263989,
      "grad_norm": 54.50958251953125,
      "learning_rate": 3.788408489272922e-06,
      "loss": 1.934,
      "step": 942
    },
    {
      "epoch": 0.6208031599736669,
      "grad_norm": 2.389320135116577,
      "learning_rate": 3.7772633330348974e-06,
      "loss": 1.4034,
      "step": 943
    },
    {
      "epoch": 0.6214614878209348,
      "grad_norm": 51.475067138671875,
      "learning_rate": 3.7661246347585134e-06,
      "loss": 1.8398,
      "step": 944
    },
    {
      "epoch": 0.6221198156682027,
      "grad_norm": 2.872058868408203,
      "learning_rate": 3.7549924532735125e-06,
      "loss": 1.4145,
      "step": 945
    },
    {
      "epoch": 0.6227781435154707,
      "grad_norm": 46.15674591064453,
      "learning_rate": 3.743866847375223e-06,
      "loss": 2.0189,
      "step": 946
    },
    {
      "epoch": 0.6234364713627386,
      "grad_norm": 5.641069412231445,
      "learning_rate": 3.732747875824241e-06,
      "loss": 1.403,
      "step": 947
    },
    {
      "epoch": 0.6240947992100065,
      "grad_norm": 22.601123809814453,
      "learning_rate": 3.7216355973461217e-06,
      "loss": 1.5645,
      "step": 948
    },
    {
      "epoch": 0.6247531270572745,
      "grad_norm": 32.06928253173828,
      "learning_rate": 3.710530070631074e-06,
      "loss": 1.7449,
      "step": 949
    },
    {
      "epoch": 0.6254114549045424,
      "grad_norm": 26.77996253967285,
      "learning_rate": 3.6994313543336456e-06,
      "loss": 1.6239,
      "step": 950
    },
    {
      "epoch": 0.6260697827518104,
      "grad_norm": 2.8897624015808105,
      "learning_rate": 3.6883395070724113e-06,
      "loss": 1.4048,
      "step": 951
    },
    {
      "epoch": 0.6267281105990783,
      "grad_norm": 1.315128207206726,
      "learning_rate": 3.677254587429674e-06,
      "loss": 1.4001,
      "step": 952
    },
    {
      "epoch": 0.6273864384463462,
      "grad_norm": 44.99141311645508,
      "learning_rate": 3.666176653951138e-06,
      "loss": 1.9417,
      "step": 953
    },
    {
      "epoch": 0.6280447662936142,
      "grad_norm": 46.823978424072266,
      "learning_rate": 3.655105765145617e-06,
      "loss": 1.9989,
      "step": 954
    },
    {
      "epoch": 0.6287030941408822,
      "grad_norm": 1.5472817420959473,
      "learning_rate": 3.6440419794847172e-06,
      "loss": 1.4088,
      "step": 955
    },
    {
      "epoch": 0.6293614219881501,
      "grad_norm": 22.657339096069336,
      "learning_rate": 3.6329853554025263e-06,
      "loss": 1.8255,
      "step": 956
    },
    {
      "epoch": 0.6300197498354181,
      "grad_norm": 133.67520141601562,
      "learning_rate": 3.621935951295309e-06,
      "loss": 2.1443,
      "step": 957
    },
    {
      "epoch": 0.630678077682686,
      "grad_norm": 1.5212199687957764,
      "learning_rate": 3.610893825521199e-06,
      "loss": 1.3986,
      "step": 958
    },
    {
      "epoch": 0.631336405529954,
      "grad_norm": 4.962665557861328,
      "learning_rate": 3.599859036399885e-06,
      "loss": 1.4087,
      "step": 959
    },
    {
      "epoch": 0.6319947333772219,
      "grad_norm": 1.9076162576675415,
      "learning_rate": 3.5888316422123114e-06,
      "loss": 1.4081,
      "step": 960
    },
    {
      "epoch": 0.6326530612244898,
      "grad_norm": 46.827144622802734,
      "learning_rate": 3.5778117012003625e-06,
      "loss": 1.9636,
      "step": 961
    },
    {
      "epoch": 0.6333113890717578,
      "grad_norm": 1.6447924375534058,
      "learning_rate": 3.566799271566558e-06,
      "loss": 1.3966,
      "step": 962
    },
    {
      "epoch": 0.6339697169190257,
      "grad_norm": 37.113792419433594,
      "learning_rate": 3.5557944114737507e-06,
      "loss": 1.6004,
      "step": 963
    },
    {
      "epoch": 0.6346280447662936,
      "grad_norm": 2.6248950958251953,
      "learning_rate": 3.544797179044804e-06,
      "loss": 1.3962,
      "step": 964
    },
    {
      "epoch": 0.6352863726135616,
      "grad_norm": 2.9417378902435303,
      "learning_rate": 3.533807632362307e-06,
      "loss": 1.398,
      "step": 965
    },
    {
      "epoch": 0.6359447004608295,
      "grad_norm": 50.10569763183594,
      "learning_rate": 3.5228258294682516e-06,
      "loss": 2.1323,
      "step": 966
    },
    {
      "epoch": 0.6366030283080975,
      "grad_norm": 32.435691833496094,
      "learning_rate": 3.511851828363726e-06,
      "loss": 1.7769,
      "step": 967
    },
    {
      "epoch": 0.6372613561553654,
      "grad_norm": 210.4340362548828,
      "learning_rate": 3.500885687008616e-06,
      "loss": 2.2402,
      "step": 968
    },
    {
      "epoch": 0.6379196840026333,
      "grad_norm": 29.27447509765625,
      "learning_rate": 3.4899274633213033e-06,
      "loss": 1.7328,
      "step": 969
    },
    {
      "epoch": 0.6385780118499013,
      "grad_norm": 60.04541778564453,
      "learning_rate": 3.478977215178336e-06,
      "loss": 2.0252,
      "step": 970
    },
    {
      "epoch": 0.6392363396971692,
      "grad_norm": 1.9046796560287476,
      "learning_rate": 3.4680350004141534e-06,
      "loss": 1.3929,
      "step": 971
    },
    {
      "epoch": 0.6398946675444371,
      "grad_norm": 146.32186889648438,
      "learning_rate": 3.457100876820757e-06,
      "loss": 2.1533,
      "step": 972
    },
    {
      "epoch": 0.6405529953917051,
      "grad_norm": 2.208871364593506,
      "learning_rate": 3.446174902147419e-06,
      "loss": 1.4024,
      "step": 973
    },
    {
      "epoch": 0.641211323238973,
      "grad_norm": 36.61041259765625,
      "learning_rate": 3.4352571341003705e-06,
      "loss": 1.8628,
      "step": 974
    },
    {
      "epoch": 0.6418696510862409,
      "grad_norm": 42.90276336669922,
      "learning_rate": 3.4243476303424992e-06,
      "loss": 1.815,
      "step": 975
    },
    {
      "epoch": 0.6425279789335089,
      "grad_norm": 2.8709559440612793,
      "learning_rate": 3.4134464484930453e-06,
      "loss": 1.3929,
      "step": 976
    },
    {
      "epoch": 0.6431863067807768,
      "grad_norm": 1.931951642036438,
      "learning_rate": 3.4025536461272955e-06,
      "loss": 1.3933,
      "step": 977
    },
    {
      "epoch": 0.6438446346280448,
      "grad_norm": 27.55235481262207,
      "learning_rate": 3.3916692807762786e-06,
      "loss": 1.5795,
      "step": 978
    },
    {
      "epoch": 0.6445029624753127,
      "grad_norm": 5.678911209106445,
      "learning_rate": 3.380793409926466e-06,
      "loss": 1.4153,
      "step": 979
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 34.89347839355469,
      "learning_rate": 3.3699260910194597e-06,
      "loss": 1.7526,
      "step": 980
    },
    {
      "epoch": 0.6458196181698486,
      "grad_norm": 2.090827226638794,
      "learning_rate": 3.3590673814516993e-06,
      "loss": 1.3953,
      "step": 981
    },
    {
      "epoch": 0.6464779460171165,
      "grad_norm": 14.854339599609375,
      "learning_rate": 3.348217338574151e-06,
      "loss": 1.5367,
      "step": 982
    },
    {
      "epoch": 0.6471362738643844,
      "grad_norm": 14.023796081542969,
      "learning_rate": 3.337376019692008e-06,
      "loss": 1.4606,
      "step": 983
    },
    {
      "epoch": 0.6477946017116524,
      "grad_norm": 38.71376037597656,
      "learning_rate": 3.326543482064386e-06,
      "loss": 1.824,
      "step": 984
    },
    {
      "epoch": 0.6484529295589203,
      "grad_norm": 33.99492645263672,
      "learning_rate": 3.3157197829040254e-06,
      "loss": 1.9611,
      "step": 985
    },
    {
      "epoch": 0.6491112574061882,
      "grad_norm": 27.794395446777344,
      "learning_rate": 3.3049049793769803e-06,
      "loss": 1.6168,
      "step": 986
    },
    {
      "epoch": 0.6497695852534562,
      "grad_norm": 59.9862060546875,
      "learning_rate": 3.294099128602326e-06,
      "loss": 2.1923,
      "step": 987
    },
    {
      "epoch": 0.6504279131007241,
      "grad_norm": 1.7101515531539917,
      "learning_rate": 3.2833022876518527e-06,
      "loss": 1.3942,
      "step": 988
    },
    {
      "epoch": 0.6510862409479921,
      "grad_norm": 61.26365280151367,
      "learning_rate": 3.2725145135497625e-06,
      "loss": 2.0589,
      "step": 989
    },
    {
      "epoch": 0.65174456879526,
      "grad_norm": 2.4097256660461426,
      "learning_rate": 3.2617358632723755e-06,
      "loss": 1.4012,
      "step": 990
    },
    {
      "epoch": 0.6524028966425279,
      "grad_norm": 4.808211803436279,
      "learning_rate": 3.2509663937478143e-06,
      "loss": 1.4229,
      "step": 991
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 56.713966369628906,
      "learning_rate": 3.2402061618557224e-06,
      "loss": 2.3573,
      "step": 992
    },
    {
      "epoch": 0.6537195523370638,
      "grad_norm": 1.8471179008483887,
      "learning_rate": 3.2294552244269516e-06,
      "loss": 1.3954,
      "step": 993
    },
    {
      "epoch": 0.6543778801843319,
      "grad_norm": 97.46768951416016,
      "learning_rate": 3.2187136382432606e-06,
      "loss": 2.8836,
      "step": 994
    },
    {
      "epoch": 0.6550362080315998,
      "grad_norm": 1.9358832836151123,
      "learning_rate": 3.2079814600370197e-06,
      "loss": 1.3976,
      "step": 995
    },
    {
      "epoch": 0.6556945358788677,
      "grad_norm": 119.82242584228516,
      "learning_rate": 3.1972587464909178e-06,
      "loss": 3.2858,
      "step": 996
    },
    {
      "epoch": 0.6563528637261357,
      "grad_norm": 132.34274291992188,
      "learning_rate": 3.1865455542376454e-06,
      "loss": 2.0316,
      "step": 997
    },
    {
      "epoch": 0.6570111915734036,
      "grad_norm": 1.71378755569458,
      "learning_rate": 3.1758419398596117e-06,
      "loss": 1.3937,
      "step": 998
    },
    {
      "epoch": 0.6576695194206715,
      "grad_norm": 30.267282485961914,
      "learning_rate": 3.165147959888638e-06,
      "loss": 1.654,
      "step": 999
    },
    {
      "epoch": 0.6583278472679395,
      "grad_norm": 56.25429153442383,
      "learning_rate": 3.1544636708056602e-06,
      "loss": 1.5855,
      "step": 1000
    },
    {
      "epoch": 0.6589861751152074,
      "grad_norm": 26.982288360595703,
      "learning_rate": 3.1437891290404345e-06,
      "loss": 1.6801,
      "step": 1001
    },
    {
      "epoch": 0.6596445029624753,
      "grad_norm": 2.149207592010498,
      "learning_rate": 3.133124390971231e-06,
      "loss": 1.3977,
      "step": 1002
    },
    {
      "epoch": 0.6603028308097433,
      "grad_norm": 1.293017864227295,
      "learning_rate": 3.122469512924545e-06,
      "loss": 1.3897,
      "step": 1003
    },
    {
      "epoch": 0.6609611586570112,
      "grad_norm": 33.679420471191406,
      "learning_rate": 3.111824551174795e-06,
      "loss": 1.866,
      "step": 1004
    },
    {
      "epoch": 0.6616194865042792,
      "grad_norm": 2.2781550884246826,
      "learning_rate": 3.1011895619440214e-06,
      "loss": 1.403,
      "step": 1005
    },
    {
      "epoch": 0.6622778143515471,
      "grad_norm": 29.383935928344727,
      "learning_rate": 3.090564601401601e-06,
      "loss": 1.7561,
      "step": 1006
    },
    {
      "epoch": 0.662936142198815,
      "grad_norm": 1.3678462505340576,
      "learning_rate": 3.0799497256639406e-06,
      "loss": 1.3923,
      "step": 1007
    },
    {
      "epoch": 0.663594470046083,
      "grad_norm": 1.7018545866012573,
      "learning_rate": 3.0693449907941784e-06,
      "loss": 1.3887,
      "step": 1008
    },
    {
      "epoch": 0.6642527978933509,
      "grad_norm": 2.1555495262145996,
      "learning_rate": 3.0587504528019035e-06,
      "loss": 1.4012,
      "step": 1009
    },
    {
      "epoch": 0.6649111257406188,
      "grad_norm": 132.76351928710938,
      "learning_rate": 3.048166167642839e-06,
      "loss": 2.623,
      "step": 1010
    },
    {
      "epoch": 0.6655694535878868,
      "grad_norm": 1.4144763946533203,
      "learning_rate": 3.0375921912185625e-06,
      "loss": 1.3876,
      "step": 1011
    },
    {
      "epoch": 0.6662277814351547,
      "grad_norm": 33.99701690673828,
      "learning_rate": 3.0270285793762054e-06,
      "loss": 1.832,
      "step": 1012
    },
    {
      "epoch": 0.6668861092824226,
      "grad_norm": 181.3500213623047,
      "learning_rate": 3.016475387908157e-06,
      "loss": 2.2489,
      "step": 1013
    },
    {
      "epoch": 0.6675444371296906,
      "grad_norm": 29.47447395324707,
      "learning_rate": 3.0059326725517694e-06,
      "loss": 1.9253,
      "step": 1014
    },
    {
      "epoch": 0.6682027649769585,
      "grad_norm": 1.9191011190414429,
      "learning_rate": 2.995400488989068e-06,
      "loss": 1.3916,
      "step": 1015
    },
    {
      "epoch": 0.6688610928242265,
      "grad_norm": 22.94038963317871,
      "learning_rate": 2.984878892846449e-06,
      "loss": 1.5105,
      "step": 1016
    },
    {
      "epoch": 0.6695194206714944,
      "grad_norm": 9.635405540466309,
      "learning_rate": 2.9743679396943965e-06,
      "loss": 1.4663,
      "step": 1017
    },
    {
      "epoch": 0.6701777485187623,
      "grad_norm": 12.007926940917969,
      "learning_rate": 2.963867685047174e-06,
      "loss": 1.423,
      "step": 1018
    },
    {
      "epoch": 0.6708360763660303,
      "grad_norm": 2.296846389770508,
      "learning_rate": 2.9533781843625496e-06,
      "loss": 1.3976,
      "step": 1019
    },
    {
      "epoch": 0.6714944042132982,
      "grad_norm": 43.3607063293457,
      "learning_rate": 2.942899493041491e-06,
      "loss": 2.0886,
      "step": 1020
    },
    {
      "epoch": 0.6721527320605661,
      "grad_norm": 15.482626914978027,
      "learning_rate": 2.9324316664278706e-06,
      "loss": 1.4413,
      "step": 1021
    },
    {
      "epoch": 0.6728110599078341,
      "grad_norm": 40.48617172241211,
      "learning_rate": 2.9219747598081823e-06,
      "loss": 1.9282,
      "step": 1022
    },
    {
      "epoch": 0.673469387755102,
      "grad_norm": 1.9068964719772339,
      "learning_rate": 2.9115288284112476e-06,
      "loss": 1.3907,
      "step": 1023
    },
    {
      "epoch": 0.67412771560237,
      "grad_norm": 69.89031219482422,
      "learning_rate": 2.901093927407918e-06,
      "loss": 1.7044,
      "step": 1024
    },
    {
      "epoch": 0.6747860434496379,
      "grad_norm": 61.5449104309082,
      "learning_rate": 2.8906701119107877e-06,
      "loss": 1.5897,
      "step": 1025
    },
    {
      "epoch": 0.6754443712969058,
      "grad_norm": 50.76947784423828,
      "learning_rate": 2.8802574369739023e-06,
      "loss": 1.827,
      "step": 1026
    },
    {
      "epoch": 0.6761026991441738,
      "grad_norm": 48.749900817871094,
      "learning_rate": 2.8698559575924666e-06,
      "loss": 2.0915,
      "step": 1027
    },
    {
      "epoch": 0.6767610269914417,
      "grad_norm": 5.825866222381592,
      "learning_rate": 2.859465728702562e-06,
      "loss": 1.4159,
      "step": 1028
    },
    {
      "epoch": 0.6774193548387096,
      "grad_norm": 2.9007978439331055,
      "learning_rate": 2.8490868051808397e-06,
      "loss": 1.3927,
      "step": 1029
    },
    {
      "epoch": 0.6780776826859776,
      "grad_norm": 1.6009446382522583,
      "learning_rate": 2.838719241844246e-06,
      "loss": 1.3898,
      "step": 1030
    },
    {
      "epoch": 0.6787360105332455,
      "grad_norm": 1.8745179176330566,
      "learning_rate": 2.8283630934497306e-06,
      "loss": 1.3904,
      "step": 1031
    },
    {
      "epoch": 0.6793943383805134,
      "grad_norm": 35.168949127197266,
      "learning_rate": 2.8180184146939504e-06,
      "loss": 1.9525,
      "step": 1032
    },
    {
      "epoch": 0.6800526662277815,
      "grad_norm": 4.2070393562316895,
      "learning_rate": 2.8076852602129828e-06,
      "loss": 1.3944,
      "step": 1033
    },
    {
      "epoch": 0.6807109940750494,
      "grad_norm": 2.0233516693115234,
      "learning_rate": 2.7973636845820455e-06,
      "loss": 1.3944,
      "step": 1034
    },
    {
      "epoch": 0.6813693219223174,
      "grad_norm": 3.2545299530029297,
      "learning_rate": 2.7870537423151967e-06,
      "loss": 1.3892,
      "step": 1035
    },
    {
      "epoch": 0.6820276497695853,
      "grad_norm": 2.158686876296997,
      "learning_rate": 2.7767554878650525e-06,
      "loss": 1.3879,
      "step": 1036
    },
    {
      "epoch": 0.6826859776168532,
      "grad_norm": 1.8129771947860718,
      "learning_rate": 2.7664689756225e-06,
      "loss": 1.3891,
      "step": 1037
    },
    {
      "epoch": 0.6833443054641212,
      "grad_norm": 1.3785724639892578,
      "learning_rate": 2.7561942599164117e-06,
      "loss": 1.3883,
      "step": 1038
    },
    {
      "epoch": 0.6840026333113891,
      "grad_norm": 173.37539672851562,
      "learning_rate": 2.7459313950133517e-06,
      "loss": 2.376,
      "step": 1039
    },
    {
      "epoch": 0.684660961158657,
      "grad_norm": 16.0750675201416,
      "learning_rate": 2.735680435117294e-06,
      "loss": 1.5707,
      "step": 1040
    },
    {
      "epoch": 0.685319289005925,
      "grad_norm": 43.8872184753418,
      "learning_rate": 2.7254414343693393e-06,
      "loss": 1.8742,
      "step": 1041
    },
    {
      "epoch": 0.6859776168531929,
      "grad_norm": 48.62205505371094,
      "learning_rate": 2.7152144468474217e-06,
      "loss": 1.9208,
      "step": 1042
    },
    {
      "epoch": 0.6866359447004609,
      "grad_norm": 37.46039962768555,
      "learning_rate": 2.7049995265660273e-06,
      "loss": 1.8172,
      "step": 1043
    },
    {
      "epoch": 0.6872942725477288,
      "grad_norm": 59.66940689086914,
      "learning_rate": 2.6947967274759085e-06,
      "loss": 2.0909,
      "step": 1044
    },
    {
      "epoch": 0.6879526003949967,
      "grad_norm": 18.60275650024414,
      "learning_rate": 2.684606103463797e-06,
      "loss": 1.5519,
      "step": 1045
    },
    {
      "epoch": 0.6886109282422647,
      "grad_norm": 52.59329605102539,
      "learning_rate": 2.6744277083521276e-06,
      "loss": 2.0852,
      "step": 1046
    },
    {
      "epoch": 0.6892692560895326,
      "grad_norm": 29.295434951782227,
      "learning_rate": 2.6642615958987394e-06,
      "loss": 1.8158,
      "step": 1047
    },
    {
      "epoch": 0.6899275839368005,
      "grad_norm": 138.39598083496094,
      "learning_rate": 2.654107819796603e-06,
      "loss": 2.1006,
      "step": 1048
    },
    {
      "epoch": 0.6905859117840685,
      "grad_norm": 34.82219696044922,
      "learning_rate": 2.643966433673537e-06,
      "loss": 1.7153,
      "step": 1049
    },
    {
      "epoch": 0.6912442396313364,
      "grad_norm": 38.210601806640625,
      "learning_rate": 2.6338374910919173e-06,
      "loss": 1.8003,
      "step": 1050
    },
    {
      "epoch": 0.6919025674786043,
      "grad_norm": 30.72583770751953,
      "learning_rate": 2.623721045548397e-06,
      "loss": 1.6166,
      "step": 1051
    },
    {
      "epoch": 0.6925608953258723,
      "grad_norm": 12.391952514648438,
      "learning_rate": 2.6136171504736325e-06,
      "loss": 1.4993,
      "step": 1052
    },
    {
      "epoch": 0.6932192231731402,
      "grad_norm": 32.51580047607422,
      "learning_rate": 2.6035258592319867e-06,
      "loss": 1.5743,
      "step": 1053
    },
    {
      "epoch": 0.6938775510204082,
      "grad_norm": 26.853885650634766,
      "learning_rate": 2.593447225121257e-06,
      "loss": 1.4997,
      "step": 1054
    },
    {
      "epoch": 0.6945358788676761,
      "grad_norm": 24.095247268676758,
      "learning_rate": 2.5833813013723967e-06,
      "loss": 1.5898,
      "step": 1055
    },
    {
      "epoch": 0.695194206714944,
      "grad_norm": 2.7738723754882812,
      "learning_rate": 2.573328141149216e-06,
      "loss": 1.3927,
      "step": 1056
    },
    {
      "epoch": 0.695852534562212,
      "grad_norm": 138.6599578857422,
      "learning_rate": 2.5632877975481275e-06,
      "loss": 1.7631,
      "step": 1057
    },
    {
      "epoch": 0.6965108624094799,
      "grad_norm": 2.7564120292663574,
      "learning_rate": 2.553260323597844e-06,
      "loss": 1.3916,
      "step": 1058
    },
    {
      "epoch": 0.6971691902567478,
      "grad_norm": 2.6091039180755615,
      "learning_rate": 2.5432457722591053e-06,
      "loss": 1.3881,
      "step": 1059
    },
    {
      "epoch": 0.6978275181040158,
      "grad_norm": 50.3702278137207,
      "learning_rate": 2.533244196424408e-06,
      "loss": 2.3333,
      "step": 1060
    },
    {
      "epoch": 0.6984858459512837,
      "grad_norm": 38.55958557128906,
      "learning_rate": 2.5232556489177095e-06,
      "loss": 1.844,
      "step": 1061
    },
    {
      "epoch": 0.6991441737985516,
      "grad_norm": 42.197593688964844,
      "learning_rate": 2.513280182494158e-06,
      "loss": 2.0089,
      "step": 1062
    },
    {
      "epoch": 0.6998025016458196,
      "grad_norm": 19.248693466186523,
      "learning_rate": 2.5033178498398193e-06,
      "loss": 1.5453,
      "step": 1063
    },
    {
      "epoch": 0.7004608294930875,
      "grad_norm": 2.187680244445801,
      "learning_rate": 2.4933687035713866e-06,
      "loss": 1.3885,
      "step": 1064
    },
    {
      "epoch": 0.7011191573403555,
      "grad_norm": 2.1829395294189453,
      "learning_rate": 2.4834327962359078e-06,
      "loss": 1.3981,
      "step": 1065
    },
    {
      "epoch": 0.7017774851876234,
      "grad_norm": 54.75776672363281,
      "learning_rate": 2.473510180310517e-06,
      "loss": 2.2854,
      "step": 1066
    },
    {
      "epoch": 0.7024358130348913,
      "grad_norm": 49.076560974121094,
      "learning_rate": 2.4636009082021346e-06,
      "loss": 2.1923,
      "step": 1067
    },
    {
      "epoch": 0.7030941408821593,
      "grad_norm": 1.3020837306976318,
      "learning_rate": 2.4537050322472167e-06,
      "loss": 1.395,
      "step": 1068
    },
    {
      "epoch": 0.7037524687294272,
      "grad_norm": 30.39332389831543,
      "learning_rate": 2.4438226047114666e-06,
      "loss": 1.8792,
      "step": 1069
    },
    {
      "epoch": 0.7044107965766951,
      "grad_norm": 2.5373141765594482,
      "learning_rate": 2.4339536777895477e-06,
      "loss": 1.3991,
      "step": 1070
    },
    {
      "epoch": 0.7050691244239631,
      "grad_norm": 3.745762348175049,
      "learning_rate": 2.4240983036048303e-06,
      "loss": 1.3954,
      "step": 1071
    },
    {
      "epoch": 0.7057274522712311,
      "grad_norm": 2.8331024646759033,
      "learning_rate": 2.414256534209105e-06,
      "loss": 1.4006,
      "step": 1072
    },
    {
      "epoch": 0.7063857801184991,
      "grad_norm": 22.6929988861084,
      "learning_rate": 2.4044284215822965e-06,
      "loss": 1.7905,
      "step": 1073
    },
    {
      "epoch": 0.707044107965767,
      "grad_norm": 2.946481704711914,
      "learning_rate": 2.3946140176322146e-06,
      "loss": 1.3899,
      "step": 1074
    },
    {
      "epoch": 0.7077024358130349,
      "grad_norm": 27.920021057128906,
      "learning_rate": 2.3848133741942565e-06,
      "loss": 1.7959,
      "step": 1075
    },
    {
      "epoch": 0.7083607636603029,
      "grad_norm": 40.943416595458984,
      "learning_rate": 2.3750265430311437e-06,
      "loss": 2.1481,
      "step": 1076
    },
    {
      "epoch": 0.7090190915075708,
      "grad_norm": 3.1212105751037598,
      "learning_rate": 2.365253575832652e-06,
      "loss": 1.3919,
      "step": 1077
    },
    {
      "epoch": 0.7096774193548387,
      "grad_norm": 26.92620277404785,
      "learning_rate": 2.3554945242153287e-06,
      "loss": 1.5357,
      "step": 1078
    },
    {
      "epoch": 0.7103357472021067,
      "grad_norm": 17.617740631103516,
      "learning_rate": 2.3457494397222257e-06,
      "loss": 1.5682,
      "step": 1079
    },
    {
      "epoch": 0.7109940750493746,
      "grad_norm": 22.225128173828125,
      "learning_rate": 2.336018373822632e-06,
      "loss": 1.7731,
      "step": 1080
    },
    {
      "epoch": 0.7116524028966426,
      "grad_norm": 34.83110427856445,
      "learning_rate": 2.326301377911789e-06,
      "loss": 1.7168,
      "step": 1081
    },
    {
      "epoch": 0.7123107307439105,
      "grad_norm": 2.4391355514526367,
      "learning_rate": 2.316598503310631e-06,
      "loss": 1.3862,
      "step": 1082
    },
    {
      "epoch": 0.7129690585911784,
      "grad_norm": 18.674654006958008,
      "learning_rate": 2.3069098012655077e-06,
      "loss": 1.5923,
      "step": 1083
    },
    {
      "epoch": 0.7136273864384464,
      "grad_norm": 13.100321769714355,
      "learning_rate": 2.297235322947916e-06,
      "loss": 1.4096,
      "step": 1084
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 1.8988432884216309,
      "learning_rate": 2.287575119454233e-06,
      "loss": 1.3862,
      "step": 1085
    },
    {
      "epoch": 0.7149440421329822,
      "grad_norm": 31.40330696105957,
      "learning_rate": 2.277929241805436e-06,
      "loss": 1.5636,
      "step": 1086
    },
    {
      "epoch": 0.7156023699802502,
      "grad_norm": 32.75732421875,
      "learning_rate": 2.2682977409468424e-06,
      "loss": 1.6578,
      "step": 1087
    },
    {
      "epoch": 0.7162606978275181,
      "grad_norm": 26.85803985595703,
      "learning_rate": 2.258680667747841e-06,
      "loss": 1.8817,
      "step": 1088
    },
    {
      "epoch": 0.716919025674786,
      "grad_norm": 3.207603931427002,
      "learning_rate": 2.249078073001615e-06,
      "loss": 1.4058,
      "step": 1089
    },
    {
      "epoch": 0.717577353522054,
      "grad_norm": 3.3912060260772705,
      "learning_rate": 2.2394900074248804e-06,
      "loss": 1.3995,
      "step": 1090
    },
    {
      "epoch": 0.7182356813693219,
      "grad_norm": 38.408470153808594,
      "learning_rate": 2.2299165216576153e-06,
      "loss": 1.5421,
      "step": 1091
    },
    {
      "epoch": 0.7188940092165899,
      "grad_norm": 2.2135322093963623,
      "learning_rate": 2.220357666262798e-06,
      "loss": 1.3946,
      "step": 1092
    },
    {
      "epoch": 0.7195523370638578,
      "grad_norm": 4.880965709686279,
      "learning_rate": 2.210813491726131e-06,
      "loss": 1.3929,
      "step": 1093
    },
    {
      "epoch": 0.7202106649111257,
      "grad_norm": 1.8325717449188232,
      "learning_rate": 2.2012840484557784e-06,
      "loss": 1.384,
      "step": 1094
    },
    {
      "epoch": 0.7208689927583937,
      "grad_norm": 51.20682144165039,
      "learning_rate": 2.1917693867821062e-06,
      "loss": 2.2478,
      "step": 1095
    },
    {
      "epoch": 0.7215273206056616,
      "grad_norm": 53.995880126953125,
      "learning_rate": 2.1822695569574042e-06,
      "loss": 1.9308,
      "step": 1096
    },
    {
      "epoch": 0.7221856484529295,
      "grad_norm": 4.2330451011657715,
      "learning_rate": 2.1727846091556298e-06,
      "loss": 1.4006,
      "step": 1097
    },
    {
      "epoch": 0.7228439763001975,
      "grad_norm": 139.20559692382812,
      "learning_rate": 2.1633145934721366e-06,
      "loss": 3.9136,
      "step": 1098
    },
    {
      "epoch": 0.7235023041474654,
      "grad_norm": 2.0698904991149902,
      "learning_rate": 2.1538595599234207e-06,
      "loss": 1.3853,
      "step": 1099
    },
    {
      "epoch": 0.7241606319947334,
      "grad_norm": 1.9027677774429321,
      "learning_rate": 2.144419558446842e-06,
      "loss": 1.3883,
      "step": 1100
    },
    {
      "epoch": 0.7248189598420013,
      "grad_norm": 1.6193921566009521,
      "learning_rate": 2.13499463890037e-06,
      "loss": 1.3855,
      "step": 1101
    },
    {
      "epoch": 0.7254772876892692,
      "grad_norm": 233.53311157226562,
      "learning_rate": 2.125584851062316e-06,
      "loss": 2.2751,
      "step": 1102
    },
    {
      "epoch": 0.7261356155365372,
      "grad_norm": 56.87468719482422,
      "learning_rate": 2.116190244631078e-06,
      "loss": 2.0274,
      "step": 1103
    },
    {
      "epoch": 0.7267939433838051,
      "grad_norm": 25.882205963134766,
      "learning_rate": 2.106810869224867e-06,
      "loss": 1.7263,
      "step": 1104
    },
    {
      "epoch": 0.727452271231073,
      "grad_norm": 41.23339080810547,
      "learning_rate": 2.0974467743814504e-06,
      "loss": 2.0798,
      "step": 1105
    },
    {
      "epoch": 0.728110599078341,
      "grad_norm": 69.99002075195312,
      "learning_rate": 2.088098009557895e-06,
      "loss": 1.9337,
      "step": 1106
    },
    {
      "epoch": 0.7287689269256089,
      "grad_norm": 1.861536979675293,
      "learning_rate": 2.078764624130297e-06,
      "loss": 1.3847,
      "step": 1107
    },
    {
      "epoch": 0.7294272547728768,
      "grad_norm": 11.279483795166016,
      "learning_rate": 2.069446667393523e-06,
      "loss": 1.421,
      "step": 1108
    },
    {
      "epoch": 0.7300855826201448,
      "grad_norm": 16.475914001464844,
      "learning_rate": 2.060144188560963e-06,
      "loss": 1.5443,
      "step": 1109
    },
    {
      "epoch": 0.7307439104674127,
      "grad_norm": 3.8704020977020264,
      "learning_rate": 2.050857236764243e-06,
      "loss": 1.4028,
      "step": 1110
    },
    {
      "epoch": 0.7314022383146808,
      "grad_norm": 33.90293884277344,
      "learning_rate": 2.0415858610529954e-06,
      "loss": 1.8075,
      "step": 1111
    },
    {
      "epoch": 0.7320605661619487,
      "grad_norm": 1.840522289276123,
      "learning_rate": 2.0323301103945845e-06,
      "loss": 1.3893,
      "step": 1112
    },
    {
      "epoch": 0.7327188940092166,
      "grad_norm": 33.06235885620117,
      "learning_rate": 2.0230900336738417e-06,
      "loss": 1.7071,
      "step": 1113
    },
    {
      "epoch": 0.7333772218564846,
      "grad_norm": 1.8192265033721924,
      "learning_rate": 2.013865679692825e-06,
      "loss": 1.3858,
      "step": 1114
    },
    {
      "epoch": 0.7340355497037525,
      "grad_norm": 2.116513252258301,
      "learning_rate": 2.0046570971705477e-06,
      "loss": 1.3865,
      "step": 1115
    },
    {
      "epoch": 0.7346938775510204,
      "grad_norm": 13.945723533630371,
      "learning_rate": 1.9954643347427215e-06,
      "loss": 1.468,
      "step": 1116
    },
    {
      "epoch": 0.7353522053982884,
      "grad_norm": 39.22065353393555,
      "learning_rate": 1.9862874409615125e-06,
      "loss": 1.7305,
      "step": 1117
    },
    {
      "epoch": 0.7360105332455563,
      "grad_norm": 77.50081634521484,
      "learning_rate": 1.977126464295267e-06,
      "loss": 2.1828,
      "step": 1118
    },
    {
      "epoch": 0.7366688610928243,
      "grad_norm": 33.972877502441406,
      "learning_rate": 1.9679814531282658e-06,
      "loss": 1.7377,
      "step": 1119
    },
    {
      "epoch": 0.7373271889400922,
      "grad_norm": 93.63334655761719,
      "learning_rate": 1.958852455760474e-06,
      "loss": 1.8529,
      "step": 1120
    },
    {
      "epoch": 0.7379855167873601,
      "grad_norm": 130.69967651367188,
      "learning_rate": 1.949739520407265e-06,
      "loss": 3.7688,
      "step": 1121
    },
    {
      "epoch": 0.7386438446346281,
      "grad_norm": 45.82351303100586,
      "learning_rate": 1.9406426951991926e-06,
      "loss": 1.9855,
      "step": 1122
    },
    {
      "epoch": 0.739302172481896,
      "grad_norm": 92.78215026855469,
      "learning_rate": 1.9315620281817214e-06,
      "loss": 1.9579,
      "step": 1123
    },
    {
      "epoch": 0.7399605003291639,
      "grad_norm": 88.72393035888672,
      "learning_rate": 1.9224975673149666e-06,
      "loss": 2.1456,
      "step": 1124
    },
    {
      "epoch": 0.7406188281764319,
      "grad_norm": 22.419498443603516,
      "learning_rate": 1.913449360473459e-06,
      "loss": 1.5472,
      "step": 1125
    },
    {
      "epoch": 0.7412771560236998,
      "grad_norm": 2.5150716304779053,
      "learning_rate": 1.9044174554458833e-06,
      "loss": 1.397,
      "step": 1126
    },
    {
      "epoch": 0.7419354838709677,
      "grad_norm": 23.594242095947266,
      "learning_rate": 1.895401899934815e-06,
      "loss": 1.5897,
      "step": 1127
    },
    {
      "epoch": 0.7425938117182357,
      "grad_norm": 16.241283416748047,
      "learning_rate": 1.88640274155649e-06,
      "loss": 1.4057,
      "step": 1128
    },
    {
      "epoch": 0.7432521395655036,
      "grad_norm": 16.386932373046875,
      "learning_rate": 1.8774200278405353e-06,
      "loss": 1.5229,
      "step": 1129
    },
    {
      "epoch": 0.7439104674127716,
      "grad_norm": 42.248878479003906,
      "learning_rate": 1.8684538062297235e-06,
      "loss": 1.8607,
      "step": 1130
    },
    {
      "epoch": 0.7445687952600395,
      "grad_norm": 26.678325653076172,
      "learning_rate": 1.8595041240797307e-06,
      "loss": 1.7607,
      "step": 1131
    },
    {
      "epoch": 0.7452271231073074,
      "grad_norm": 39.70442581176758,
      "learning_rate": 1.8505710286588701e-06,
      "loss": 1.9913,
      "step": 1132
    },
    {
      "epoch": 0.7458854509545754,
      "grad_norm": 14.41952133178711,
      "learning_rate": 1.8416545671478535e-06,
      "loss": 1.4251,
      "step": 1133
    },
    {
      "epoch": 0.7465437788018433,
      "grad_norm": 14.022274017333984,
      "learning_rate": 1.832754786639544e-06,
      "loss": 1.5166,
      "step": 1134
    },
    {
      "epoch": 0.7472021066491112,
      "grad_norm": 3.4914774894714355,
      "learning_rate": 1.823871734138698e-06,
      "loss": 1.392,
      "step": 1135
    },
    {
      "epoch": 0.7478604344963792,
      "grad_norm": 2.943895101547241,
      "learning_rate": 1.8150054565617237e-06,
      "loss": 1.3897,
      "step": 1136
    },
    {
      "epoch": 0.7485187623436471,
      "grad_norm": 8.700444221496582,
      "learning_rate": 1.8061560007364304e-06,
      "loss": 1.3972,
      "step": 1137
    },
    {
      "epoch": 0.749177090190915,
      "grad_norm": 35.471561431884766,
      "learning_rate": 1.7973234134017825e-06,
      "loss": 1.8693,
      "step": 1138
    },
    {
      "epoch": 0.749835418038183,
      "grad_norm": 21.5732421875,
      "learning_rate": 1.7885077412076558e-06,
      "loss": 1.6184,
      "step": 1139
    },
    {
      "epoch": 0.7504937458854509,
      "grad_norm": 217.99618530273438,
      "learning_rate": 1.7797090307145826e-06,
      "loss": 3.1818,
      "step": 1140
    },
    {
      "epoch": 0.7511520737327189,
      "grad_norm": 2.322974443435669,
      "learning_rate": 1.7709273283935125e-06,
      "loss": 1.4008,
      "step": 1141
    },
    {
      "epoch": 0.7518104015799868,
      "grad_norm": 18.8515625,
      "learning_rate": 1.7621626806255688e-06,
      "loss": 1.6124,
      "step": 1142
    },
    {
      "epoch": 0.7524687294272547,
      "grad_norm": 1.3747808933258057,
      "learning_rate": 1.7534151337017951e-06,
      "loss": 1.3828,
      "step": 1143
    },
    {
      "epoch": 0.7531270572745227,
      "grad_norm": 77.51075744628906,
      "learning_rate": 1.744684733822918e-06,
      "loss": 2.0772,
      "step": 1144
    },
    {
      "epoch": 0.7537853851217906,
      "grad_norm": 1.4990075826644897,
      "learning_rate": 1.7359715270991028e-06,
      "loss": 1.3795,
      "step": 1145
    },
    {
      "epoch": 0.7544437129690585,
      "grad_norm": 128.12918090820312,
      "learning_rate": 1.7272755595497048e-06,
      "loss": 2.8753,
      "step": 1146
    },
    {
      "epoch": 0.7551020408163265,
      "grad_norm": 63.641754150390625,
      "learning_rate": 1.7185968771030315e-06,
      "loss": 2.3097,
      "step": 1147
    },
    {
      "epoch": 0.7557603686635944,
      "grad_norm": 31.749460220336914,
      "learning_rate": 1.7099355255960947e-06,
      "loss": 1.4367,
      "step": 1148
    },
    {
      "epoch": 0.7564186965108625,
      "grad_norm": 1.609816074371338,
      "learning_rate": 1.7012915507743789e-06,
      "loss": 1.3801,
      "step": 1149
    },
    {
      "epoch": 0.7570770243581304,
      "grad_norm": 33.85536193847656,
      "learning_rate": 1.692664998291585e-06,
      "loss": 1.807,
      "step": 1150
    },
    {
      "epoch": 0.7577353522053983,
      "grad_norm": 52.48042297363281,
      "learning_rate": 1.6840559137094008e-06,
      "loss": 1.6964,
      "step": 1151
    },
    {
      "epoch": 0.7583936800526663,
      "grad_norm": 1.7403502464294434,
      "learning_rate": 1.6754643424972538e-06,
      "loss": 1.384,
      "step": 1152
    },
    {
      "epoch": 0.7590520078999342,
      "grad_norm": 67.27982330322266,
      "learning_rate": 1.666890330032077e-06,
      "loss": 1.8089,
      "step": 1153
    },
    {
      "epoch": 0.7597103357472021,
      "grad_norm": 82.0757827758789,
      "learning_rate": 1.6583339215980637e-06,
      "loss": 2.0937,
      "step": 1154
    },
    {
      "epoch": 0.7603686635944701,
      "grad_norm": 28.768667221069336,
      "learning_rate": 1.6497951623864288e-06,
      "loss": 1.6473,
      "step": 1155
    },
    {
      "epoch": 0.761026991441738,
      "grad_norm": 2.4855470657348633,
      "learning_rate": 1.641274097495173e-06,
      "loss": 1.3843,
      "step": 1156
    },
    {
      "epoch": 0.761685319289006,
      "grad_norm": 0.9967201948165894,
      "learning_rate": 1.632770771928846e-06,
      "loss": 1.3825,
      "step": 1157
    },
    {
      "epoch": 0.7623436471362739,
      "grad_norm": 43.1859016418457,
      "learning_rate": 1.6242852305983026e-06,
      "loss": 1.8609,
      "step": 1158
    },
    {
      "epoch": 0.7630019749835418,
      "grad_norm": 19.96113395690918,
      "learning_rate": 1.6158175183204682e-06,
      "loss": 1.5901,
      "step": 1159
    },
    {
      "epoch": 0.7636603028308098,
      "grad_norm": 33.21387481689453,
      "learning_rate": 1.607367679818108e-06,
      "loss": 1.7072,
      "step": 1160
    },
    {
      "epoch": 0.7643186306780777,
      "grad_norm": 12.150626182556152,
      "learning_rate": 1.598935759719582e-06,
      "loss": 1.5238,
      "step": 1161
    },
    {
      "epoch": 0.7649769585253456,
      "grad_norm": 32.23988723754883,
      "learning_rate": 1.5905218025586112e-06,
      "loss": 1.7525,
      "step": 1162
    },
    {
      "epoch": 0.7656352863726136,
      "grad_norm": 1.2432304620742798,
      "learning_rate": 1.5821258527740497e-06,
      "loss": 1.3801,
      "step": 1163
    },
    {
      "epoch": 0.7662936142198815,
      "grad_norm": 1.5840959548950195,
      "learning_rate": 1.5737479547096401e-06,
      "loss": 1.3876,
      "step": 1164
    },
    {
      "epoch": 0.7669519420671495,
      "grad_norm": 38.48316955566406,
      "learning_rate": 1.5653881526137832e-06,
      "loss": 1.9105,
      "step": 1165
    },
    {
      "epoch": 0.7676102699144174,
      "grad_norm": 4.981957912445068,
      "learning_rate": 1.5570464906393118e-06,
      "loss": 1.3971,
      "step": 1166
    },
    {
      "epoch": 0.7682685977616853,
      "grad_norm": 35.71418762207031,
      "learning_rate": 1.5487230128432384e-06,
      "loss": 1.9222,
      "step": 1167
    },
    {
      "epoch": 0.7689269256089533,
      "grad_norm": 2.8314576148986816,
      "learning_rate": 1.540417763186547e-06,
      "loss": 1.4075,
      "step": 1168
    },
    {
      "epoch": 0.7695852534562212,
      "grad_norm": 1.6789603233337402,
      "learning_rate": 1.5321307855339406e-06,
      "loss": 1.3834,
      "step": 1169
    },
    {
      "epoch": 0.7702435813034891,
      "grad_norm": 126.46034240722656,
      "learning_rate": 1.5238621236536188e-06,
      "loss": 3.4585,
      "step": 1170
    },
    {
      "epoch": 0.7709019091507571,
      "grad_norm": 1.4052273035049438,
      "learning_rate": 1.5156118212170496e-06,
      "loss": 1.3811,
      "step": 1171
    },
    {
      "epoch": 0.771560236998025,
      "grad_norm": 22.609983444213867,
      "learning_rate": 1.5073799217987288e-06,
      "loss": 1.5932,
      "step": 1172
    },
    {
      "epoch": 0.7722185648452929,
      "grad_norm": 101.55943298339844,
      "learning_rate": 1.4991664688759571e-06,
      "loss": 1.5984,
      "step": 1173
    },
    {
      "epoch": 0.7728768926925609,
      "grad_norm": 69.73072052001953,
      "learning_rate": 1.4909715058286139e-06,
      "loss": 2.0045,
      "step": 1174
    },
    {
      "epoch": 0.7735352205398288,
      "grad_norm": 16.814620971679688,
      "learning_rate": 1.4827950759389126e-06,
      "loss": 1.516,
      "step": 1175
    },
    {
      "epoch": 0.7741935483870968,
      "grad_norm": 27.948299407958984,
      "learning_rate": 1.474637222391192e-06,
      "loss": 1.6203,
      "step": 1176
    },
    {
      "epoch": 0.7748518762343647,
      "grad_norm": 46.789920806884766,
      "learning_rate": 1.466497988271679e-06,
      "loss": 1.4704,
      "step": 1177
    },
    {
      "epoch": 0.7755102040816326,
      "grad_norm": 26.26358985900879,
      "learning_rate": 1.4583774165682508e-06,
      "loss": 1.6351,
      "step": 1178
    },
    {
      "epoch": 0.7761685319289006,
      "grad_norm": 8.555994987487793,
      "learning_rate": 1.4502755501702275e-06,
      "loss": 1.4198,
      "step": 1179
    },
    {
      "epoch": 0.7768268597761685,
      "grad_norm": 39.812095642089844,
      "learning_rate": 1.4421924318681358e-06,
      "loss": 2.0684,
      "step": 1180
    },
    {
      "epoch": 0.7774851876234364,
      "grad_norm": 2.5865745544433594,
      "learning_rate": 1.4341281043534743e-06,
      "loss": 1.3869,
      "step": 1181
    },
    {
      "epoch": 0.7781435154707044,
      "grad_norm": 8.458815574645996,
      "learning_rate": 1.4260826102185076e-06,
      "loss": 1.4612,
      "step": 1182
    },
    {
      "epoch": 0.7788018433179723,
      "grad_norm": 1.5754876136779785,
      "learning_rate": 1.4180559919560244e-06,
      "loss": 1.3851,
      "step": 1183
    },
    {
      "epoch": 0.7794601711652402,
      "grad_norm": 37.663848876953125,
      "learning_rate": 1.410048291959118e-06,
      "loss": 1.9543,
      "step": 1184
    },
    {
      "epoch": 0.7801184990125082,
      "grad_norm": 1.8258507251739502,
      "learning_rate": 1.40205955252097e-06,
      "loss": 1.3783,
      "step": 1185
    },
    {
      "epoch": 0.7807768268597761,
      "grad_norm": 40.49237060546875,
      "learning_rate": 1.3940898158346162e-06,
      "loss": 1.8667,
      "step": 1186
    },
    {
      "epoch": 0.781435154707044,
      "grad_norm": 50.70589065551758,
      "learning_rate": 1.3861391239927257e-06,
      "loss": 1.8248,
      "step": 1187
    },
    {
      "epoch": 0.7820934825543121,
      "grad_norm": 39.1597900390625,
      "learning_rate": 1.378207518987389e-06,
      "loss": 1.7718,
      "step": 1188
    },
    {
      "epoch": 0.78275181040158,
      "grad_norm": 2.6741368770599365,
      "learning_rate": 1.3702950427098804e-06,
      "loss": 1.3804,
      "step": 1189
    },
    {
      "epoch": 0.783410138248848,
      "grad_norm": 2.39133358001709,
      "learning_rate": 1.3624017369504472e-06,
      "loss": 1.3816,
      "step": 1190
    },
    {
      "epoch": 0.7840684660961159,
      "grad_norm": 57.49946975708008,
      "learning_rate": 1.3545276433980908e-06,
      "loss": 2.0097,
      "step": 1191
    },
    {
      "epoch": 0.7847267939433838,
      "grad_norm": 46.08671188354492,
      "learning_rate": 1.3466728036403326e-06,
      "loss": 1.6179,
      "step": 1192
    },
    {
      "epoch": 0.7853851217906518,
      "grad_norm": 5.993505001068115,
      "learning_rate": 1.338837259163014e-06,
      "loss": 1.4092,
      "step": 1193
    },
    {
      "epoch": 0.7860434496379197,
      "grad_norm": 2.2454302310943604,
      "learning_rate": 1.3310210513500604e-06,
      "loss": 1.38,
      "step": 1194
    },
    {
      "epoch": 0.7867017774851877,
      "grad_norm": 2.4485719203948975,
      "learning_rate": 1.3232242214832702e-06,
      "loss": 1.39,
      "step": 1195
    },
    {
      "epoch": 0.7873601053324556,
      "grad_norm": 57.94138717651367,
      "learning_rate": 1.315446810742101e-06,
      "loss": 2.2253,
      "step": 1196
    },
    {
      "epoch": 0.7880184331797235,
      "grad_norm": 26.744680404663086,
      "learning_rate": 1.307688860203441e-06,
      "loss": 1.7889,
      "step": 1197
    },
    {
      "epoch": 0.7886767610269915,
      "grad_norm": 34.330047607421875,
      "learning_rate": 1.2999504108413997e-06,
      "loss": 1.4606,
      "step": 1198
    },
    {
      "epoch": 0.7893350888742594,
      "grad_norm": 8.085688591003418,
      "learning_rate": 1.2922315035270933e-06,
      "loss": 1.4615,
      "step": 1199
    },
    {
      "epoch": 0.7899934167215273,
      "grad_norm": 1.9180716276168823,
      "learning_rate": 1.2845321790284204e-06,
      "loss": 1.3795,
      "step": 1200
    },
    {
      "epoch": 0.7906517445687953,
      "grad_norm": 36.66376495361328,
      "learning_rate": 1.2768524780098556e-06,
      "loss": 1.6202,
      "step": 1201
    },
    {
      "epoch": 0.7913100724160632,
      "grad_norm": 3.1258039474487305,
      "learning_rate": 1.2691924410322265e-06,
      "loss": 1.4032,
      "step": 1202
    },
    {
      "epoch": 0.7919684002633312,
      "grad_norm": 4.294305801391602,
      "learning_rate": 1.2615521085525107e-06,
      "loss": 1.4035,
      "step": 1203
    },
    {
      "epoch": 0.7926267281105991,
      "grad_norm": 37.27045822143555,
      "learning_rate": 1.2539315209236085e-06,
      "loss": 1.7503,
      "step": 1204
    },
    {
      "epoch": 0.793285055957867,
      "grad_norm": 40.07862091064453,
      "learning_rate": 1.2463307183941404e-06,
      "loss": 1.7462,
      "step": 1205
    },
    {
      "epoch": 0.793943383805135,
      "grad_norm": 2.455453395843506,
      "learning_rate": 1.2387497411082267e-06,
      "loss": 1.3851,
      "step": 1206
    },
    {
      "epoch": 0.7946017116524029,
      "grad_norm": 1.2184044122695923,
      "learning_rate": 1.231188629105286e-06,
      "loss": 1.3794,
      "step": 1207
    },
    {
      "epoch": 0.7952600394996708,
      "grad_norm": 2.3760530948638916,
      "learning_rate": 1.223647422319812e-06,
      "loss": 1.3945,
      "step": 1208
    },
    {
      "epoch": 0.7959183673469388,
      "grad_norm": 2.6858296394348145,
      "learning_rate": 1.2161261605811664e-06,
      "loss": 1.3811,
      "step": 1209
    },
    {
      "epoch": 0.7965766951942067,
      "grad_norm": 47.371971130371094,
      "learning_rate": 1.2086248836133756e-06,
      "loss": 1.9429,
      "step": 1210
    },
    {
      "epoch": 0.7972350230414746,
      "grad_norm": 33.89539337158203,
      "learning_rate": 1.2011436310349117e-06,
      "loss": 1.8067,
      "step": 1211
    },
    {
      "epoch": 0.7978933508887426,
      "grad_norm": 2.217621088027954,
      "learning_rate": 1.1936824423584859e-06,
      "loss": 1.3821,
      "step": 1212
    },
    {
      "epoch": 0.7985516787360105,
      "grad_norm": 2.822589635848999,
      "learning_rate": 1.186241356990841e-06,
      "loss": 1.3965,
      "step": 1213
    },
    {
      "epoch": 0.7992100065832785,
      "grad_norm": 51.985042572021484,
      "learning_rate": 1.178820414232546e-06,
      "loss": 1.8369,
      "step": 1214
    },
    {
      "epoch": 0.7998683344305464,
      "grad_norm": 17.843124389648438,
      "learning_rate": 1.1714196532777817e-06,
      "loss": 1.5333,
      "step": 1215
    },
    {
      "epoch": 0.8005266622778143,
      "grad_norm": 5.541134357452393,
      "learning_rate": 1.1640391132141377e-06,
      "loss": 1.3951,
      "step": 1216
    },
    {
      "epoch": 0.8011849901250823,
      "grad_norm": 2.186779022216797,
      "learning_rate": 1.1566788330224087e-06,
      "loss": 1.3832,
      "step": 1217
    },
    {
      "epoch": 0.8018433179723502,
      "grad_norm": 62.36914825439453,
      "learning_rate": 1.1493388515763836e-06,
      "loss": 1.8459,
      "step": 1218
    },
    {
      "epoch": 0.8025016458196181,
      "grad_norm": 52.356300354003906,
      "learning_rate": 1.14201920764264e-06,
      "loss": 2.2029,
      "step": 1219
    },
    {
      "epoch": 0.8031599736668861,
      "grad_norm": 1.6602932214736938,
      "learning_rate": 1.1347199398803477e-06,
      "loss": 1.3784,
      "step": 1220
    },
    {
      "epoch": 0.803818301514154,
      "grad_norm": 46.90974807739258,
      "learning_rate": 1.12744108684105e-06,
      "loss": 2.1546,
      "step": 1221
    },
    {
      "epoch": 0.804476629361422,
      "grad_norm": 51.03591537475586,
      "learning_rate": 1.1201826869684767e-06,
      "loss": 1.9826,
      "step": 1222
    },
    {
      "epoch": 0.8051349572086899,
      "grad_norm": 2.2050364017486572,
      "learning_rate": 1.1129447785983284e-06,
      "loss": 1.3798,
      "step": 1223
    },
    {
      "epoch": 0.8057932850559578,
      "grad_norm": 92.7001953125,
      "learning_rate": 1.1057273999580775e-06,
      "loss": 1.7373,
      "step": 1224
    },
    {
      "epoch": 0.8064516129032258,
      "grad_norm": 3.326306104660034,
      "learning_rate": 1.0985305891667724e-06,
      "loss": 1.3837,
      "step": 1225
    },
    {
      "epoch": 0.8071099407504937,
      "grad_norm": 24.329111099243164,
      "learning_rate": 1.0913543842348263e-06,
      "loss": 1.4221,
      "step": 1226
    },
    {
      "epoch": 0.8077682685977617,
      "grad_norm": 17.340068817138672,
      "learning_rate": 1.0841988230638222e-06,
      "loss": 1.5463,
      "step": 1227
    },
    {
      "epoch": 0.8084265964450297,
      "grad_norm": 62.40840530395508,
      "learning_rate": 1.0770639434463148e-06,
      "loss": 1.8618,
      "step": 1228
    },
    {
      "epoch": 0.8090849242922976,
      "grad_norm": 17.85041618347168,
      "learning_rate": 1.069949783065624e-06,
      "loss": 1.4112,
      "step": 1229
    },
    {
      "epoch": 0.8097432521395656,
      "grad_norm": 33.18306350708008,
      "learning_rate": 1.0628563794956403e-06,
      "loss": 1.7191,
      "step": 1230
    },
    {
      "epoch": 0.8104015799868335,
      "grad_norm": 56.851497650146484,
      "learning_rate": 1.0557837702006313e-06,
      "loss": 1.7137,
      "step": 1231
    },
    {
      "epoch": 0.8110599078341014,
      "grad_norm": 1.795941948890686,
      "learning_rate": 1.048731992535027e-06,
      "loss": 1.3817,
      "step": 1232
    },
    {
      "epoch": 0.8117182356813694,
      "grad_norm": 62.513084411621094,
      "learning_rate": 1.0417010837432444e-06,
      "loss": 2.0271,
      "step": 1233
    },
    {
      "epoch": 0.8123765635286373,
      "grad_norm": 4.2196149826049805,
      "learning_rate": 1.034691080959479e-06,
      "loss": 1.4006,
      "step": 1234
    },
    {
      "epoch": 0.8130348913759052,
      "grad_norm": 1.6041866540908813,
      "learning_rate": 1.0277020212075012e-06,
      "loss": 1.378,
      "step": 1235
    },
    {
      "epoch": 0.8136932192231732,
      "grad_norm": 2.1239194869995117,
      "learning_rate": 1.0207339414004808e-06,
      "loss": 1.3885,
      "step": 1236
    },
    {
      "epoch": 0.8143515470704411,
      "grad_norm": 54.16850280761719,
      "learning_rate": 1.0137868783407728e-06,
      "loss": 1.7506,
      "step": 1237
    },
    {
      "epoch": 0.815009874917709,
      "grad_norm": 2.29703688621521,
      "learning_rate": 1.006860868719733e-06,
      "loss": 1.3866,
      "step": 1238
    },
    {
      "epoch": 0.815668202764977,
      "grad_norm": 1.2792041301727295,
      "learning_rate": 9.999559491175243e-07,
      "loss": 1.3744,
      "step": 1239
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 2.2448766231536865,
      "learning_rate": 9.9307215600292e-07,
      "loss": 1.3791,
      "step": 1240
    },
    {
      "epoch": 0.8169848584595129,
      "grad_norm": 60.000267028808594,
      "learning_rate": 9.862095257331095e-07,
      "loss": 1.8149,
      "step": 1241
    },
    {
      "epoch": 0.8176431863067808,
      "grad_norm": 2.339176893234253,
      "learning_rate": 9.79368094553515e-07,
      "loss": 1.3771,
      "step": 1242
    },
    {
      "epoch": 0.8183015141540487,
      "grad_norm": 13.633628845214844,
      "learning_rate": 9.725478985975905e-07,
      "loss": 1.5072,
      "step": 1243
    },
    {
      "epoch": 0.8189598420013167,
      "grad_norm": 152.42333984375,
      "learning_rate": 9.657489738866339e-07,
      "loss": 2.502,
      "step": 1244
    },
    {
      "epoch": 0.8196181698485846,
      "grad_norm": 52.500858306884766,
      "learning_rate": 9.589713563296027e-07,
      "loss": 1.7262,
      "step": 1245
    },
    {
      "epoch": 0.8202764976958525,
      "grad_norm": 1.7250319719314575,
      "learning_rate": 9.52215081722912e-07,
      "loss": 1.3748,
      "step": 1246
    },
    {
      "epoch": 0.8209348255431205,
      "grad_norm": 27.407928466796875,
      "learning_rate": 9.454801857502599e-07,
      "loss": 1.6021,
      "step": 1247
    },
    {
      "epoch": 0.8215931533903884,
      "grad_norm": 1.453079104423523,
      "learning_rate": 9.387667039824277e-07,
      "loss": 1.3827,
      "step": 1248
    },
    {
      "epoch": 0.8222514812376563,
      "grad_norm": 61.62173843383789,
      "learning_rate": 9.320746718770957e-07,
      "loss": 1.8875,
      "step": 1249
    },
    {
      "epoch": 0.8229098090849243,
      "grad_norm": 202.4427032470703,
      "learning_rate": 9.254041247786611e-07,
      "loss": 2.9729,
      "step": 1250
    },
    {
      "epoch": 0.8235681369321922,
      "grad_norm": 2.478040933609009,
      "learning_rate": 9.187550979180415e-07,
      "loss": 1.3794,
      "step": 1251
    },
    {
      "epoch": 0.8242264647794602,
      "grad_norm": 113.37527465820312,
      "learning_rate": 9.121276264124951e-07,
      "loss": 1.5343,
      "step": 1252
    },
    {
      "epoch": 0.8248847926267281,
      "grad_norm": 47.421756744384766,
      "learning_rate": 9.05521745265438e-07,
      "loss": 1.7088,
      "step": 1253
    },
    {
      "epoch": 0.825543120473996,
      "grad_norm": 2.924802780151367,
      "learning_rate": 8.989374893662511e-07,
      "loss": 1.3837,
      "step": 1254
    },
    {
      "epoch": 0.826201448321264,
      "grad_norm": 57.86833572387695,
      "learning_rate": 8.923748934900999e-07,
      "loss": 1.895,
      "step": 1255
    },
    {
      "epoch": 0.8268597761685319,
      "grad_norm": 59.01033020019531,
      "learning_rate": 8.858339922977555e-07,
      "loss": 2.3446,
      "step": 1256
    },
    {
      "epoch": 0.8275181040157998,
      "grad_norm": 1.6333149671554565,
      "learning_rate": 8.793148203354035e-07,
      "loss": 1.3778,
      "step": 1257
    },
    {
      "epoch": 0.8281764318630678,
      "grad_norm": 12.972888946533203,
      "learning_rate": 8.728174120344663e-07,
      "loss": 1.508,
      "step": 1258
    },
    {
      "epoch": 0.8288347597103357,
      "grad_norm": 144.8201446533203,
      "learning_rate": 8.663418017114189e-07,
      "loss": 1.7705,
      "step": 1259
    },
    {
      "epoch": 0.8294930875576036,
      "grad_norm": 2.312288284301758,
      "learning_rate": 8.598880235676094e-07,
      "loss": 1.3857,
      "step": 1260
    },
    {
      "epoch": 0.8301514154048716,
      "grad_norm": 310.9079895019531,
      "learning_rate": 8.534561116890811e-07,
      "loss": 2.3769,
      "step": 1261
    },
    {
      "epoch": 0.8308097432521395,
      "grad_norm": 1.4461802244186401,
      "learning_rate": 8.470461000463854e-07,
      "loss": 1.3761,
      "step": 1262
    },
    {
      "epoch": 0.8314680710994075,
      "grad_norm": 51.69458770751953,
      "learning_rate": 8.40658022494405e-07,
      "loss": 2.1079,
      "step": 1263
    },
    {
      "epoch": 0.8321263989466754,
      "grad_norm": 40.43217849731445,
      "learning_rate": 8.342919127721826e-07,
      "loss": 1.6526,
      "step": 1264
    },
    {
      "epoch": 0.8327847267939433,
      "grad_norm": 2.5769567489624023,
      "learning_rate": 8.279478045027301e-07,
      "loss": 1.3772,
      "step": 1265
    },
    {
      "epoch": 0.8334430546412114,
      "grad_norm": 50.21409606933594,
      "learning_rate": 8.216257311928599e-07,
      "loss": 1.9449,
      "step": 1266
    },
    {
      "epoch": 0.8341013824884793,
      "grad_norm": 39.65702819824219,
      "learning_rate": 8.153257262330045e-07,
      "loss": 1.9312,
      "step": 1267
    },
    {
      "epoch": 0.8347597103357473,
      "grad_norm": 65.25142669677734,
      "learning_rate": 8.090478228970433e-07,
      "loss": 2.11,
      "step": 1268
    },
    {
      "epoch": 0.8354180381830152,
      "grad_norm": 29.61642837524414,
      "learning_rate": 8.027920543421225e-07,
      "loss": 1.4694,
      "step": 1269
    },
    {
      "epoch": 0.8360763660302831,
      "grad_norm": 27.88981819152832,
      "learning_rate": 7.965584536084808e-07,
      "loss": 1.6787,
      "step": 1270
    },
    {
      "epoch": 0.8367346938775511,
      "grad_norm": 207.697265625,
      "learning_rate": 7.903470536192797e-07,
      "loss": 1.9395,
      "step": 1271
    },
    {
      "epoch": 0.837393021724819,
      "grad_norm": 112.05683898925781,
      "learning_rate": 7.841578871804239e-07,
      "loss": 2.2332,
      "step": 1272
    },
    {
      "epoch": 0.8380513495720869,
      "grad_norm": 2.1945509910583496,
      "learning_rate": 7.779909869803881e-07,
      "loss": 1.3771,
      "step": 1273
    },
    {
      "epoch": 0.8387096774193549,
      "grad_norm": 45.27144241333008,
      "learning_rate": 7.718463855900521e-07,
      "loss": 2.1896,
      "step": 1274
    },
    {
      "epoch": 0.8393680052666228,
      "grad_norm": 2.43808650970459,
      "learning_rate": 7.657241154625184e-07,
      "loss": 1.3922,
      "step": 1275
    },
    {
      "epoch": 0.8400263331138907,
      "grad_norm": 46.80513381958008,
      "learning_rate": 7.59624208932947e-07,
      "loss": 1.757,
      "step": 1276
    },
    {
      "epoch": 0.8406846609611587,
      "grad_norm": 51.63554763793945,
      "learning_rate": 7.535466982183825e-07,
      "loss": 1.8446,
      "step": 1277
    },
    {
      "epoch": 0.8413429888084266,
      "grad_norm": 1.8084814548492432,
      "learning_rate": 7.474916154175843e-07,
      "loss": 1.3823,
      "step": 1278
    },
    {
      "epoch": 0.8420013166556946,
      "grad_norm": 2.586010217666626,
      "learning_rate": 7.414589925108601e-07,
      "loss": 1.386,
      "step": 1279
    },
    {
      "epoch": 0.8426596445029625,
      "grad_norm": 5.7132649421691895,
      "learning_rate": 7.354488613598915e-07,
      "loss": 1.3843,
      "step": 1280
    },
    {
      "epoch": 0.8433179723502304,
      "grad_norm": 1.386013388633728,
      "learning_rate": 7.294612537075679e-07,
      "loss": 1.3809,
      "step": 1281
    },
    {
      "epoch": 0.8439763001974984,
      "grad_norm": 2.0573153495788574,
      "learning_rate": 7.234962011778235e-07,
      "loss": 1.3789,
      "step": 1282
    },
    {
      "epoch": 0.8446346280447663,
      "grad_norm": 24.073034286499023,
      "learning_rate": 7.175537352754619e-07,
      "loss": 1.6645,
      "step": 1283
    },
    {
      "epoch": 0.8452929558920342,
      "grad_norm": 36.62663269042969,
      "learning_rate": 7.116338873859941e-07,
      "loss": 1.6127,
      "step": 1284
    },
    {
      "epoch": 0.8459512837393022,
      "grad_norm": 4.4223785400390625,
      "learning_rate": 7.05736688775478e-07,
      "loss": 1.395,
      "step": 1285
    },
    {
      "epoch": 0.8466096115865701,
      "grad_norm": 182.0355682373047,
      "learning_rate": 6.998621705903391e-07,
      "loss": 3.3231,
      "step": 1286
    },
    {
      "epoch": 0.847267939433838,
      "grad_norm": 3.5289058685302734,
      "learning_rate": 6.940103638572215e-07,
      "loss": 1.3828,
      "step": 1287
    },
    {
      "epoch": 0.847926267281106,
      "grad_norm": 66.3415298461914,
      "learning_rate": 6.881812994828185e-07,
      "loss": 2.2193,
      "step": 1288
    },
    {
      "epoch": 0.8485845951283739,
      "grad_norm": 3.578678846359253,
      "learning_rate": 6.823750082537e-07,
      "loss": 1.3813,
      "step": 1289
    },
    {
      "epoch": 0.8492429229756419,
      "grad_norm": 2.366495370864868,
      "learning_rate": 6.765915208361673e-07,
      "loss": 1.3823,
      "step": 1290
    },
    {
      "epoch": 0.8499012508229098,
      "grad_norm": 63.2597770690918,
      "learning_rate": 6.708308677760756e-07,
      "loss": 2.1068,
      "step": 1291
    },
    {
      "epoch": 0.8505595786701777,
      "grad_norm": 32.950172424316406,
      "learning_rate": 6.65093079498681e-07,
      "loss": 1.9297,
      "step": 1292
    },
    {
      "epoch": 0.8512179065174457,
      "grad_norm": 2.0833966732025146,
      "learning_rate": 6.593781863084797e-07,
      "loss": 1.3798,
      "step": 1293
    },
    {
      "epoch": 0.8518762343647136,
      "grad_norm": 22.56699562072754,
      "learning_rate": 6.536862183890436e-07,
      "loss": 1.4749,
      "step": 1294
    },
    {
      "epoch": 0.8525345622119815,
      "grad_norm": 225.6930389404297,
      "learning_rate": 6.480172058028627e-07,
      "loss": 3.5713,
      "step": 1295
    },
    {
      "epoch": 0.8531928900592495,
      "grad_norm": 1.7320035696029663,
      "learning_rate": 6.423711784911906e-07,
      "loss": 1.378,
      "step": 1296
    },
    {
      "epoch": 0.8538512179065174,
      "grad_norm": 7.715714931488037,
      "learning_rate": 6.367481662738789e-07,
      "loss": 1.3876,
      "step": 1297
    },
    {
      "epoch": 0.8545095457537853,
      "grad_norm": 100.32140350341797,
      "learning_rate": 6.311481988492247e-07,
      "loss": 1.4957,
      "step": 1298
    },
    {
      "epoch": 0.8551678736010533,
      "grad_norm": 1.8960990905761719,
      "learning_rate": 6.255713057938156e-07,
      "loss": 1.3817,
      "step": 1299
    },
    {
      "epoch": 0.8558262014483212,
      "grad_norm": 1.505463719367981,
      "learning_rate": 6.200175165623634e-07,
      "loss": 1.3772,
      "step": 1300
    },
    {
      "epoch": 0.8564845292955892,
      "grad_norm": 1.8539563417434692,
      "learning_rate": 6.14486860487562e-07,
      "loss": 1.3788,
      "step": 1301
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 2.798017978668213,
      "learning_rate": 6.089793667799266e-07,
      "loss": 1.3887,
      "step": 1302
    },
    {
      "epoch": 0.857801184990125,
      "grad_norm": 1.5923138856887817,
      "learning_rate": 6.034950645276317e-07,
      "loss": 1.3777,
      "step": 1303
    },
    {
      "epoch": 0.858459512837393,
      "grad_norm": 2.322511911392212,
      "learning_rate": 5.980339826963721e-07,
      "loss": 1.3863,
      "step": 1304
    },
    {
      "epoch": 0.859117840684661,
      "grad_norm": 30.8601016998291,
      "learning_rate": 5.925961501291994e-07,
      "loss": 1.7341,
      "step": 1305
    },
    {
      "epoch": 0.859776168531929,
      "grad_norm": 1.1499168872833252,
      "learning_rate": 5.871815955463706e-07,
      "loss": 1.3722,
      "step": 1306
    },
    {
      "epoch": 0.8604344963791969,
      "grad_norm": 44.86933135986328,
      "learning_rate": 5.817903475452036e-07,
      "loss": 2.0428,
      "step": 1307
    },
    {
      "epoch": 0.8610928242264648,
      "grad_norm": 81.04059600830078,
      "learning_rate": 5.76422434599917e-07,
      "loss": 2.0968,
      "step": 1308
    },
    {
      "epoch": 0.8617511520737328,
      "grad_norm": 15.026147842407227,
      "learning_rate": 5.710778850614845e-07,
      "loss": 1.5182,
      "step": 1309
    },
    {
      "epoch": 0.8624094799210007,
      "grad_norm": 25.395095825195312,
      "learning_rate": 5.657567271574871e-07,
      "loss": 1.6376,
      "step": 1310
    },
    {
      "epoch": 0.8630678077682686,
      "grad_norm": 25.019704818725586,
      "learning_rate": 5.604589889919588e-07,
      "loss": 1.5603,
      "step": 1311
    },
    {
      "epoch": 0.8637261356155366,
      "grad_norm": 1.7854557037353516,
      "learning_rate": 5.551846985452403e-07,
      "loss": 1.3748,
      "step": 1312
    },
    {
      "epoch": 0.8643844634628045,
      "grad_norm": 144.6929473876953,
      "learning_rate": 5.499338836738322e-07,
      "loss": 1.6949,
      "step": 1313
    },
    {
      "epoch": 0.8650427913100724,
      "grad_norm": 16.647340774536133,
      "learning_rate": 5.447065721102501e-07,
      "loss": 1.5636,
      "step": 1314
    },
    {
      "epoch": 0.8657011191573404,
      "grad_norm": 5.242038726806641,
      "learning_rate": 5.395027914628725e-07,
      "loss": 1.3767,
      "step": 1315
    },
    {
      "epoch": 0.8663594470046083,
      "grad_norm": 35.95319747924805,
      "learning_rate": 5.343225692157983e-07,
      "loss": 1.4769,
      "step": 1316
    },
    {
      "epoch": 0.8670177748518763,
      "grad_norm": 20.459501266479492,
      "learning_rate": 5.291659327287003e-07,
      "loss": 1.5683,
      "step": 1317
    },
    {
      "epoch": 0.8676761026991442,
      "grad_norm": 5.068058490753174,
      "learning_rate": 5.240329092366848e-07,
      "loss": 1.3878,
      "step": 1318
    },
    {
      "epoch": 0.8683344305464121,
      "grad_norm": 4.707389831542969,
      "learning_rate": 5.189235258501413e-07,
      "loss": 1.3832,
      "step": 1319
    },
    {
      "epoch": 0.8689927583936801,
      "grad_norm": 20.434877395629883,
      "learning_rate": 5.138378095546032e-07,
      "loss": 1.4724,
      "step": 1320
    },
    {
      "epoch": 0.869651086240948,
      "grad_norm": 34.984317779541016,
      "learning_rate": 5.087757872106075e-07,
      "loss": 1.6083,
      "step": 1321
    },
    {
      "epoch": 0.8703094140882159,
      "grad_norm": 136.11231994628906,
      "learning_rate": 5.03737485553546e-07,
      "loss": 1.7521,
      "step": 1322
    },
    {
      "epoch": 0.8709677419354839,
      "grad_norm": 1.7989280223846436,
      "learning_rate": 4.98722931193531e-07,
      "loss": 1.3832,
      "step": 1323
    },
    {
      "epoch": 0.8716260697827518,
      "grad_norm": 1.0729292631149292,
      "learning_rate": 4.937321506152492e-07,
      "loss": 1.3717,
      "step": 1324
    },
    {
      "epoch": 0.8722843976300197,
      "grad_norm": 1.5060887336730957,
      "learning_rate": 4.88765170177829e-07,
      "loss": 1.3757,
      "step": 1325
    },
    {
      "epoch": 0.8729427254772877,
      "grad_norm": 1.2933990955352783,
      "learning_rate": 4.838220161146922e-07,
      "loss": 1.3762,
      "step": 1326
    },
    {
      "epoch": 0.8736010533245556,
      "grad_norm": 64.99469757080078,
      "learning_rate": 4.789027145334224e-07,
      "loss": 2.0253,
      "step": 1327
    },
    {
      "epoch": 0.8742593811718236,
      "grad_norm": 1.9371899366378784,
      "learning_rate": 4.7400729141562516e-07,
      "loss": 1.3753,
      "step": 1328
    },
    {
      "epoch": 0.8749177090190915,
      "grad_norm": 2.3212263584136963,
      "learning_rate": 4.691357726167889e-07,
      "loss": 1.3806,
      "step": 1329
    },
    {
      "epoch": 0.8755760368663594,
      "grad_norm": 22.691116333007812,
      "learning_rate": 4.642881838661517e-07,
      "loss": 1.5881,
      "step": 1330
    },
    {
      "epoch": 0.8762343647136274,
      "grad_norm": 56.79160690307617,
      "learning_rate": 4.5946455076656127e-07,
      "loss": 1.6845,
      "step": 1331
    },
    {
      "epoch": 0.8768926925608953,
      "grad_norm": 3.3770928382873535,
      "learning_rate": 4.5466489879434305e-07,
      "loss": 1.3901,
      "step": 1332
    },
    {
      "epoch": 0.8775510204081632,
      "grad_norm": 55.4163703918457,
      "learning_rate": 4.498892532991661e-07,
      "loss": 1.7637,
      "step": 1333
    },
    {
      "epoch": 0.8782093482554312,
      "grad_norm": 1.7119579315185547,
      "learning_rate": 4.4513763950390587e-07,
      "loss": 1.3808,
      "step": 1334
    },
    {
      "epoch": 0.8788676761026991,
      "grad_norm": 2.314962387084961,
      "learning_rate": 4.4041008250451325e-07,
      "loss": 1.3831,
      "step": 1335
    },
    {
      "epoch": 0.879526003949967,
      "grad_norm": 37.876121520996094,
      "learning_rate": 4.3570660726988233e-07,
      "loss": 1.8135,
      "step": 1336
    },
    {
      "epoch": 0.880184331797235,
      "grad_norm": 47.549007415771484,
      "learning_rate": 4.3102723864171804e-07,
      "loss": 2.0272,
      "step": 1337
    },
    {
      "epoch": 0.8808426596445029,
      "grad_norm": 2.744859218597412,
      "learning_rate": 4.263720013344014e-07,
      "loss": 1.3815,
      "step": 1338
    },
    {
      "epoch": 0.8815009874917709,
      "grad_norm": 142.32086181640625,
      "learning_rate": 4.2174091993486874e-07,
      "loss": 2.688,
      "step": 1339
    },
    {
      "epoch": 0.8821593153390388,
      "grad_norm": 3.3279199600219727,
      "learning_rate": 4.1713401890246807e-07,
      "loss": 1.3765,
      "step": 1340
    },
    {
      "epoch": 0.8828176431863067,
      "grad_norm": 1.3863815069198608,
      "learning_rate": 4.1255132256884213e-07,
      "loss": 1.3728,
      "step": 1341
    },
    {
      "epoch": 0.8834759710335747,
      "grad_norm": 62.611812591552734,
      "learning_rate": 4.07992855137796e-07,
      "loss": 1.7214,
      "step": 1342
    },
    {
      "epoch": 0.8841342988808426,
      "grad_norm": 45.31715393066406,
      "learning_rate": 4.034586406851615e-07,
      "loss": 1.5994,
      "step": 1343
    },
    {
      "epoch": 0.8847926267281107,
      "grad_norm": 24.349756240844727,
      "learning_rate": 3.989487031586842e-07,
      "loss": 1.6125,
      "step": 1344
    },
    {
      "epoch": 0.8854509545753786,
      "grad_norm": 33.508445739746094,
      "learning_rate": 3.9446306637788755e-07,
      "loss": 1.5752,
      "step": 1345
    },
    {
      "epoch": 0.8861092824226465,
      "grad_norm": 10.297689437866211,
      "learning_rate": 3.9000175403394456e-07,
      "loss": 1.398,
      "step": 1346
    },
    {
      "epoch": 0.8867676102699145,
      "grad_norm": 44.78404235839844,
      "learning_rate": 3.8556478968956426e-07,
      "loss": 1.9281,
      "step": 1347
    },
    {
      "epoch": 0.8874259381171824,
      "grad_norm": 2.0178720951080322,
      "learning_rate": 3.811521967788545e-07,
      "loss": 1.3818,
      "step": 1348
    },
    {
      "epoch": 0.8880842659644503,
      "grad_norm": 1.9513123035430908,
      "learning_rate": 3.7676399860720583e-07,
      "loss": 1.3786,
      "step": 1349
    },
    {
      "epoch": 0.8887425938117183,
      "grad_norm": 401.4014587402344,
      "learning_rate": 3.724002183511688e-07,
      "loss": 2.3296,
      "step": 1350
    },
    {
      "epoch": 0.8894009216589862,
      "grad_norm": 25.935894012451172,
      "learning_rate": 3.6806087905832456e-07,
      "loss": 1.6104,
      "step": 1351
    },
    {
      "epoch": 0.8900592495062541,
      "grad_norm": 36.56901168823242,
      "learning_rate": 3.6374600364717125e-07,
      "loss": 1.9025,
      "step": 1352
    },
    {
      "epoch": 0.8907175773535221,
      "grad_norm": 67.1872787475586,
      "learning_rate": 3.5945561490699943e-07,
      "loss": 2.3161,
      "step": 1353
    },
    {
      "epoch": 0.89137590520079,
      "grad_norm": 1.2260066270828247,
      "learning_rate": 3.5518973549776737e-07,
      "loss": 1.3719,
      "step": 1354
    },
    {
      "epoch": 0.892034233048058,
      "grad_norm": 2.2303528785705566,
      "learning_rate": 3.50948387949992e-07,
      "loss": 1.3864,
      "step": 1355
    },
    {
      "epoch": 0.8926925608953259,
      "grad_norm": 2.1850709915161133,
      "learning_rate": 3.46731594664621e-07,
      "loss": 1.378,
      "step": 1356
    },
    {
      "epoch": 0.8933508887425938,
      "grad_norm": 34.233734130859375,
      "learning_rate": 3.425393779129144e-07,
      "loss": 1.7804,
      "step": 1357
    },
    {
      "epoch": 0.8940092165898618,
      "grad_norm": 56.83019256591797,
      "learning_rate": 3.38371759836334e-07,
      "loss": 2.0281,
      "step": 1358
    },
    {
      "epoch": 0.8946675444371297,
      "grad_norm": 40.93299865722656,
      "learning_rate": 3.342287624464202e-07,
      "loss": 1.8336,
      "step": 1359
    },
    {
      "epoch": 0.8953258722843976,
      "grad_norm": 139.44004821777344,
      "learning_rate": 3.301104076246764e-07,
      "loss": 3.5296,
      "step": 1360
    },
    {
      "epoch": 0.8959842001316656,
      "grad_norm": 70.79949188232422,
      "learning_rate": 3.260167171224582e-07,
      "loss": 1.7303,
      "step": 1361
    },
    {
      "epoch": 0.8966425279789335,
      "grad_norm": 2.0079946517944336,
      "learning_rate": 3.2194771256085165e-07,
      "loss": 1.3775,
      "step": 1362
    },
    {
      "epoch": 0.8973008558262014,
      "grad_norm": 5.323785781860352,
      "learning_rate": 3.1790341543056327e-07,
      "loss": 1.398,
      "step": 1363
    },
    {
      "epoch": 0.8979591836734694,
      "grad_norm": 46.78853225708008,
      "learning_rate": 3.138838470918071e-07,
      "loss": 1.7225,
      "step": 1364
    },
    {
      "epoch": 0.8986175115207373,
      "grad_norm": 70.24716186523438,
      "learning_rate": 3.09889028774189e-07,
      "loss": 2.0374,
      "step": 1365
    },
    {
      "epoch": 0.8992758393680053,
      "grad_norm": 2.9884097576141357,
      "learning_rate": 3.0591898157659617e-07,
      "loss": 1.3796,
      "step": 1366
    },
    {
      "epoch": 0.8999341672152732,
      "grad_norm": 1.3013149499893188,
      "learning_rate": 3.019737264670869e-07,
      "loss": 1.376,
      "step": 1367
    },
    {
      "epoch": 0.9005924950625411,
      "grad_norm": 27.049148559570312,
      "learning_rate": 2.9805328428277713e-07,
      "loss": 1.6273,
      "step": 1368
    },
    {
      "epoch": 0.9012508229098091,
      "grad_norm": 64.72850036621094,
      "learning_rate": 2.941576757297321e-07,
      "loss": 1.8333,
      "step": 1369
    },
    {
      "epoch": 0.901909150757077,
      "grad_norm": 1.691245675086975,
      "learning_rate": 2.9028692138285687e-07,
      "loss": 1.3714,
      "step": 1370
    },
    {
      "epoch": 0.9025674786043449,
      "grad_norm": 39.331260681152344,
      "learning_rate": 2.8644104168578544e-07,
      "loss": 1.718,
      "step": 1371
    },
    {
      "epoch": 0.9032258064516129,
      "grad_norm": 51.0649299621582,
      "learning_rate": 2.826200569507792e-07,
      "loss": 1.9375,
      "step": 1372
    },
    {
      "epoch": 0.9038841342988808,
      "grad_norm": 223.95751953125,
      "learning_rate": 2.7882398735861127e-07,
      "loss": 3.6662,
      "step": 1373
    },
    {
      "epoch": 0.9045424621461488,
      "grad_norm": 23.491966247558594,
      "learning_rate": 2.7505285295846504e-07,
      "loss": 1.4047,
      "step": 1374
    },
    {
      "epoch": 0.9052007899934167,
      "grad_norm": 41.46273422241211,
      "learning_rate": 2.713066736678277e-07,
      "loss": 1.8663,
      "step": 1375
    },
    {
      "epoch": 0.9058591178406846,
      "grad_norm": 171.9471435546875,
      "learning_rate": 2.6758546927238394e-07,
      "loss": 1.602,
      "step": 1376
    },
    {
      "epoch": 0.9065174456879526,
      "grad_norm": 13.912371635437012,
      "learning_rate": 2.638892594259113e-07,
      "loss": 1.404,
      "step": 1377
    },
    {
      "epoch": 0.9071757735352205,
      "grad_norm": 272.4838562011719,
      "learning_rate": 2.602180636501778e-07,
      "loss": 2.0953,
      "step": 1378
    },
    {
      "epoch": 0.9078341013824884,
      "grad_norm": 131.7509765625,
      "learning_rate": 2.565719013348389e-07,
      "loss": 3.5511,
      "step": 1379
    },
    {
      "epoch": 0.9084924292297564,
      "grad_norm": 32.12971496582031,
      "learning_rate": 2.5295079173733197e-07,
      "loss": 1.8121,
      "step": 1380
    },
    {
      "epoch": 0.9091507570770243,
      "grad_norm": 1.6420260667800903,
      "learning_rate": 2.493547539827784e-07,
      "loss": 1.3762,
      "step": 1381
    },
    {
      "epoch": 0.9098090849242922,
      "grad_norm": 47.0679817199707,
      "learning_rate": 2.457838070638818e-07,
      "loss": 1.9572,
      "step": 1382
    },
    {
      "epoch": 0.9104674127715603,
      "grad_norm": 62.138389587402344,
      "learning_rate": 2.422379698408256e-07,
      "loss": 2.226,
      "step": 1383
    },
    {
      "epoch": 0.9111257406188282,
      "grad_norm": 34.001251220703125,
      "learning_rate": 2.387172610411742e-07,
      "loss": 1.6395,
      "step": 1384
    },
    {
      "epoch": 0.9117840684660962,
      "grad_norm": 58.20603942871094,
      "learning_rate": 2.3522169925977556e-07,
      "loss": 1.784,
      "step": 1385
    },
    {
      "epoch": 0.9124423963133641,
      "grad_norm": 2.9929146766662598,
      "learning_rate": 2.317513029586632e-07,
      "loss": 1.3781,
      "step": 1386
    },
    {
      "epoch": 0.913100724160632,
      "grad_norm": 30.435100555419922,
      "learning_rate": 2.2830609046695596e-07,
      "loss": 1.6491,
      "step": 1387
    },
    {
      "epoch": 0.9137590520079,
      "grad_norm": 41.260108947753906,
      "learning_rate": 2.2488607998076284e-07,
      "loss": 1.9408,
      "step": 1388
    },
    {
      "epoch": 0.9144173798551679,
      "grad_norm": 10.369584083557129,
      "learning_rate": 2.2149128956308664e-07,
      "loss": 1.3977,
      "step": 1389
    },
    {
      "epoch": 0.9150757077024358,
      "grad_norm": 160.6278533935547,
      "learning_rate": 2.1812173714372997e-07,
      "loss": 3.5321,
      "step": 1390
    },
    {
      "epoch": 0.9157340355497038,
      "grad_norm": 37.19264602661133,
      "learning_rate": 2.1477744051919935e-07,
      "loss": 1.7812,
      "step": 1391
    },
    {
      "epoch": 0.9163923633969717,
      "grad_norm": 147.31640625,
      "learning_rate": 2.1145841735260909e-07,
      "loss": 3.435,
      "step": 1392
    },
    {
      "epoch": 0.9170506912442397,
      "grad_norm": 96.03667449951172,
      "learning_rate": 2.0816468517359356e-07,
      "loss": 1.6422,
      "step": 1393
    },
    {
      "epoch": 0.9177090190915076,
      "grad_norm": 164.21551513671875,
      "learning_rate": 2.048962613782074e-07,
      "loss": 2.3703,
      "step": 1394
    },
    {
      "epoch": 0.9183673469387755,
      "grad_norm": 1.537845253944397,
      "learning_rate": 2.016531632288393e-07,
      "loss": 1.3782,
      "step": 1395
    },
    {
      "epoch": 0.9190256747860435,
      "grad_norm": 29.86785125732422,
      "learning_rate": 1.9843540785412064e-07,
      "loss": 1.6258,
      "step": 1396
    },
    {
      "epoch": 0.9196840026333114,
      "grad_norm": 42.434261322021484,
      "learning_rate": 1.9524301224882912e-07,
      "loss": 1.9157,
      "step": 1397
    },
    {
      "epoch": 0.9203423304805793,
      "grad_norm": 52.62324523925781,
      "learning_rate": 1.9207599327380698e-07,
      "loss": 1.7638,
      "step": 1398
    },
    {
      "epoch": 0.9210006583278473,
      "grad_norm": 179.69827270507812,
      "learning_rate": 1.8893436765586748e-07,
      "loss": 3.0214,
      "step": 1399
    },
    {
      "epoch": 0.9216589861751152,
      "grad_norm": 61.69736862182617,
      "learning_rate": 1.8581815198770447e-07,
      "loss": 2.1954,
      "step": 1400
    },
    {
      "epoch": 0.9223173140223832,
      "grad_norm": 24.118183135986328,
      "learning_rate": 1.8272736272781145e-07,
      "loss": 1.4205,
      "step": 1401
    },
    {
      "epoch": 0.9229756418696511,
      "grad_norm": 23.822893142700195,
      "learning_rate": 1.796620162003887e-07,
      "loss": 1.5876,
      "step": 1402
    },
    {
      "epoch": 0.923633969716919,
      "grad_norm": 56.479644775390625,
      "learning_rate": 1.766221285952574e-07,
      "loss": 1.9543,
      "step": 1403
    },
    {
      "epoch": 0.924292297564187,
      "grad_norm": 51.355857849121094,
      "learning_rate": 1.7360771596778014e-07,
      "loss": 1.7267,
      "step": 1404
    },
    {
      "epoch": 0.9249506254114549,
      "grad_norm": 36.657073974609375,
      "learning_rate": 1.7061879423876826e-07,
      "loss": 1.5951,
      "step": 1405
    },
    {
      "epoch": 0.9256089532587228,
      "grad_norm": 34.23673629760742,
      "learning_rate": 1.6765537919440134e-07,
      "loss": 1.6154,
      "step": 1406
    },
    {
      "epoch": 0.9262672811059908,
      "grad_norm": 51.846012115478516,
      "learning_rate": 1.6471748648614783e-07,
      "loss": 1.794,
      "step": 1407
    },
    {
      "epoch": 0.9269256089532587,
      "grad_norm": 85.41478729248047,
      "learning_rate": 1.6180513163067179e-07,
      "loss": 1.6137,
      "step": 1408
    },
    {
      "epoch": 0.9275839368005266,
      "grad_norm": 44.00346374511719,
      "learning_rate": 1.5891833000976243e-07,
      "loss": 1.5504,
      "step": 1409
    },
    {
      "epoch": 0.9282422646477946,
      "grad_norm": 45.720821380615234,
      "learning_rate": 1.5605709687024796e-07,
      "loss": 2.0474,
      "step": 1410
    },
    {
      "epoch": 0.9289005924950625,
      "grad_norm": 2.591735601425171,
      "learning_rate": 1.532214473239113e-07,
      "loss": 1.3798,
      "step": 1411
    },
    {
      "epoch": 0.9295589203423305,
      "grad_norm": 17.913326263427734,
      "learning_rate": 1.504113963474174e-07,
      "loss": 1.4287,
      "step": 1412
    },
    {
      "epoch": 0.9302172481895984,
      "grad_norm": 1.327878713607788,
      "learning_rate": 1.4762695878223044e-07,
      "loss": 1.3742,
      "step": 1413
    },
    {
      "epoch": 0.9308755760368663,
      "grad_norm": 2.0191221237182617,
      "learning_rate": 1.4486814933453276e-07,
      "loss": 1.3774,
      "step": 1414
    },
    {
      "epoch": 0.9315339038841343,
      "grad_norm": 37.325042724609375,
      "learning_rate": 1.4213498257515335e-07,
      "loss": 1.7857,
      "step": 1415
    },
    {
      "epoch": 0.9321922317314022,
      "grad_norm": 1.623129963874817,
      "learning_rate": 1.3942747293948732e-07,
      "loss": 1.3732,
      "step": 1416
    },
    {
      "epoch": 0.9328505595786701,
      "grad_norm": 2.9298181533813477,
      "learning_rate": 1.367456347274182e-07,
      "loss": 1.3818,
      "step": 1417
    },
    {
      "epoch": 0.9335088874259381,
      "grad_norm": 1.9942799806594849,
      "learning_rate": 1.3408948210324678e-07,
      "loss": 1.3872,
      "step": 1418
    },
    {
      "epoch": 0.934167215273206,
      "grad_norm": 138.20123291015625,
      "learning_rate": 1.3145902909561137e-07,
      "loss": 1.834,
      "step": 1419
    },
    {
      "epoch": 0.934825543120474,
      "grad_norm": 48.410091400146484,
      "learning_rate": 1.288542895974171e-07,
      "loss": 1.7375,
      "step": 1420
    },
    {
      "epoch": 0.9354838709677419,
      "grad_norm": 65.22760009765625,
      "learning_rate": 1.2627527736576273e-07,
      "loss": 2.0975,
      "step": 1421
    },
    {
      "epoch": 0.9361421988150099,
      "grad_norm": 45.432613372802734,
      "learning_rate": 1.2372200602186467e-07,
      "loss": 1.8856,
      "step": 1422
    },
    {
      "epoch": 0.9368005266622779,
      "grad_norm": 1.6287323236465454,
      "learning_rate": 1.2119448905098862e-07,
      "loss": 1.3776,
      "step": 1423
    },
    {
      "epoch": 0.9374588545095458,
      "grad_norm": 44.085411071777344,
      "learning_rate": 1.1869273980237684e-07,
      "loss": 1.8289,
      "step": 1424
    },
    {
      "epoch": 0.9381171823568137,
      "grad_norm": 129.4268798828125,
      "learning_rate": 1.162167714891771e-07,
      "loss": 3.1927,
      "step": 1425
    },
    {
      "epoch": 0.9387755102040817,
      "grad_norm": 22.906103134155273,
      "learning_rate": 1.137665971883739e-07,
      "loss": 1.5934,
      "step": 1426
    },
    {
      "epoch": 0.9394338380513496,
      "grad_norm": 25.922935485839844,
      "learning_rate": 1.1134222984071907e-07,
      "loss": 1.6303,
      "step": 1427
    },
    {
      "epoch": 0.9400921658986175,
      "grad_norm": 1.6683893203735352,
      "learning_rate": 1.0894368225066288e-07,
      "loss": 1.3734,
      "step": 1428
    },
    {
      "epoch": 0.9407504937458855,
      "grad_norm": 3.206143379211426,
      "learning_rate": 1.0657096708628745e-07,
      "loss": 1.3765,
      "step": 1429
    },
    {
      "epoch": 0.9414088215931534,
      "grad_norm": 1.6517951488494873,
      "learning_rate": 1.0422409687924018e-07,
      "loss": 1.3775,
      "step": 1430
    },
    {
      "epoch": 0.9420671494404214,
      "grad_norm": 35.2164421081543,
      "learning_rate": 1.0190308402466376e-07,
      "loss": 1.7833,
      "step": 1431
    },
    {
      "epoch": 0.9427254772876893,
      "grad_norm": 34.769859313964844,
      "learning_rate": 9.960794078113734e-08,
      "loss": 1.7962,
      "step": 1432
    },
    {
      "epoch": 0.9433838051349572,
      "grad_norm": 1.9954984188079834,
      "learning_rate": 9.733867927060548e-08,
      "loss": 1.3753,
      "step": 1433
    },
    {
      "epoch": 0.9440421329822252,
      "grad_norm": 37.48027801513672,
      "learning_rate": 9.509531147831763e-08,
      "loss": 1.726,
      "step": 1434
    },
    {
      "epoch": 0.9447004608294931,
      "grad_norm": 1.7138782739639282,
      "learning_rate": 9.287784925276322e-08,
      "loss": 1.3757,
      "step": 1435
    },
    {
      "epoch": 0.945358788676761,
      "grad_norm": 2.0937001705169678,
      "learning_rate": 9.068630430561055e-08,
      "loss": 1.3762,
      "step": 1436
    },
    {
      "epoch": 0.946017116524029,
      "grad_norm": 2.111534833908081,
      "learning_rate": 8.852068821164461e-08,
      "loss": 1.3757,
      "step": 1437
    },
    {
      "epoch": 0.9466754443712969,
      "grad_norm": 1.9269344806671143,
      "learning_rate": 8.63810124087039e-08,
      "loss": 1.3828,
      "step": 1438
    },
    {
      "epoch": 0.9473337722185649,
      "grad_norm": 1.825760006904602,
      "learning_rate": 8.426728819762309e-08,
      "loss": 1.3724,
      "step": 1439
    },
    {
      "epoch": 0.9479921000658328,
      "grad_norm": 2.6564524173736572,
      "learning_rate": 8.217952674217155e-08,
      "loss": 1.3922,
      "step": 1440
    },
    {
      "epoch": 0.9486504279131007,
      "grad_norm": 16.48324203491211,
      "learning_rate": 8.01177390689939e-08,
      "loss": 1.4048,
      "step": 1441
    },
    {
      "epoch": 0.9493087557603687,
      "grad_norm": 1.8181655406951904,
      "learning_rate": 7.808193606755332e-08,
      "loss": 1.3779,
      "step": 1442
    },
    {
      "epoch": 0.9499670836076366,
      "grad_norm": 132.927734375,
      "learning_rate": 7.607212849007284e-08,
      "loss": 2.8566,
      "step": 1443
    },
    {
      "epoch": 0.9506254114549045,
      "grad_norm": 15.912672996520996,
      "learning_rate": 7.408832695148028e-08,
      "loss": 1.4301,
      "step": 1444
    },
    {
      "epoch": 0.9512837393021725,
      "grad_norm": 36.5748291015625,
      "learning_rate": 7.213054192934887e-08,
      "loss": 1.869,
      "step": 1445
    },
    {
      "epoch": 0.9519420671494404,
      "grad_norm": 1.7790108919143677,
      "learning_rate": 7.019878376384515e-08,
      "loss": 1.3816,
      "step": 1446
    },
    {
      "epoch": 0.9526003949967083,
      "grad_norm": 19.975482940673828,
      "learning_rate": 6.829306265767332e-08,
      "loss": 1.5993,
      "step": 1447
    },
    {
      "epoch": 0.9532587228439763,
      "grad_norm": 50.00921630859375,
      "learning_rate": 6.641338867602098e-08,
      "loss": 1.9583,
      "step": 1448
    },
    {
      "epoch": 0.9539170506912442,
      "grad_norm": 36.957550048828125,
      "learning_rate": 6.455977174650573e-08,
      "loss": 1.661,
      "step": 1449
    },
    {
      "epoch": 0.9545753785385122,
      "grad_norm": 164.21849060058594,
      "learning_rate": 6.273222165912418e-08,
      "loss": 1.9274,
      "step": 1450
    },
    {
      "epoch": 0.9552337063857801,
      "grad_norm": 62.729854583740234,
      "learning_rate": 6.093074806619693e-08,
      "loss": 2.0495,
      "step": 1451
    },
    {
      "epoch": 0.955892034233048,
      "grad_norm": 36.03814697265625,
      "learning_rate": 5.9155360482323064e-08,
      "loss": 1.5562,
      "step": 1452
    },
    {
      "epoch": 0.956550362080316,
      "grad_norm": 40.931087493896484,
      "learning_rate": 5.740606828432582e-08,
      "loss": 1.7988,
      "step": 1453
    },
    {
      "epoch": 0.9572086899275839,
      "grad_norm": 12.480756759643555,
      "learning_rate": 5.5682880711201425e-08,
      "loss": 1.3985,
      "step": 1454
    },
    {
      "epoch": 0.9578670177748518,
      "grad_norm": 52.252845764160156,
      "learning_rate": 5.39858068640764e-08,
      "loss": 1.7126,
      "step": 1455
    },
    {
      "epoch": 0.9585253456221198,
      "grad_norm": 2.8048839569091797,
      "learning_rate": 5.231485570615424e-08,
      "loss": 1.3801,
      "step": 1456
    },
    {
      "epoch": 0.9591836734693877,
      "grad_norm": 30.86437225341797,
      "learning_rate": 5.0670036062668846e-08,
      "loss": 1.5746,
      "step": 1457
    },
    {
      "epoch": 0.9598420013166556,
      "grad_norm": 42.25654983520508,
      "learning_rate": 4.905135662084171e-08,
      "loss": 1.4677,
      "step": 1458
    },
    {
      "epoch": 0.9605003291639236,
      "grad_norm": 26.06846809387207,
      "learning_rate": 4.7458825929830863e-08,
      "loss": 1.6071,
      "step": 1459
    },
    {
      "epoch": 0.9611586570111915,
      "grad_norm": 131.4384765625,
      "learning_rate": 4.589245240068763e-08,
      "loss": 2.1465,
      "step": 1460
    },
    {
      "epoch": 0.9618169848584596,
      "grad_norm": 1.1618742942810059,
      "learning_rate": 4.435224430631546e-08,
      "loss": 1.3734,
      "step": 1461
    },
    {
      "epoch": 0.9624753127057275,
      "grad_norm": 31.36115074157715,
      "learning_rate": 4.2838209781419505e-08,
      "loss": 1.696,
      "step": 1462
    },
    {
      "epoch": 0.9631336405529954,
      "grad_norm": 33.11177062988281,
      "learning_rate": 4.1350356822471014e-08,
      "loss": 1.5839,
      "step": 1463
    },
    {
      "epoch": 0.9637919684002634,
      "grad_norm": 3.0610666275024414,
      "learning_rate": 3.9888693287660765e-08,
      "loss": 1.3919,
      "step": 1464
    },
    {
      "epoch": 0.9644502962475313,
      "grad_norm": 111.52664184570312,
      "learning_rate": 3.8453226896856265e-08,
      "loss": 2.6464,
      "step": 1465
    },
    {
      "epoch": 0.9651086240947993,
      "grad_norm": 2.0578484535217285,
      "learning_rate": 3.704396523156517e-08,
      "loss": 1.3823,
      "step": 1466
    },
    {
      "epoch": 0.9657669519420672,
      "grad_norm": 89.12710571289062,
      "learning_rate": 3.566091573489361e-08,
      "loss": 2.6723,
      "step": 1467
    },
    {
      "epoch": 0.9664252797893351,
      "grad_norm": 117.62042999267578,
      "learning_rate": 3.430408571150512e-08,
      "loss": 2.1057,
      "step": 1468
    },
    {
      "epoch": 0.9670836076366031,
      "grad_norm": 47.93045425415039,
      "learning_rate": 3.297348232758346e-08,
      "loss": 1.7006,
      "step": 1469
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 38.424560546875,
      "learning_rate": 3.1669112610796524e-08,
      "loss": 1.7858,
      "step": 1470
    },
    {
      "epoch": 0.9684002633311389,
      "grad_norm": 1.593186616897583,
      "learning_rate": 3.0390983450254705e-08,
      "loss": 1.3754,
      "step": 1471
    },
    {
      "epoch": 0.9690585911784069,
      "grad_norm": 37.99445724487305,
      "learning_rate": 2.9139101596479258e-08,
      "loss": 1.8696,
      "step": 1472
    },
    {
      "epoch": 0.9697169190256748,
      "grad_norm": 1.4369361400604248,
      "learning_rate": 2.791347366136399e-08,
      "loss": 1.3719,
      "step": 1473
    },
    {
      "epoch": 0.9703752468729427,
      "grad_norm": 1.0462650060653687,
      "learning_rate": 2.6714106118140848e-08,
      "loss": 1.3723,
      "step": 1474
    },
    {
      "epoch": 0.9710335747202107,
      "grad_norm": 26.547653198242188,
      "learning_rate": 2.554100530134662e-08,
      "loss": 1.5466,
      "step": 1475
    },
    {
      "epoch": 0.9716919025674786,
      "grad_norm": 36.52650451660156,
      "learning_rate": 2.4394177406786845e-08,
      "loss": 1.8712,
      "step": 1476
    },
    {
      "epoch": 0.9723502304147466,
      "grad_norm": 37.96826934814453,
      "learning_rate": 2.327362849150694e-08,
      "loss": 1.8648,
      "step": 1477
    },
    {
      "epoch": 0.9730085582620145,
      "grad_norm": 52.889400482177734,
      "learning_rate": 2.2179364473757237e-08,
      "loss": 1.8071,
      "step": 1478
    },
    {
      "epoch": 0.9736668861092824,
      "grad_norm": 1.9344404935836792,
      "learning_rate": 2.1111391132962456e-08,
      "loss": 1.3789,
      "step": 1479
    },
    {
      "epoch": 0.9743252139565504,
      "grad_norm": 43.122032165527344,
      "learning_rate": 2.006971410969283e-08,
      "loss": 1.8827,
      "step": 1480
    },
    {
      "epoch": 0.9749835418038183,
      "grad_norm": 137.5574188232422,
      "learning_rate": 1.9054338905631353e-08,
      "loss": 1.617,
      "step": 1481
    },
    {
      "epoch": 0.9756418696510862,
      "grad_norm": 1.8330485820770264,
      "learning_rate": 1.8065270883546592e-08,
      "loss": 1.3717,
      "step": 1482
    },
    {
      "epoch": 0.9763001974983542,
      "grad_norm": 78.87809753417969,
      "learning_rate": 1.710251526726492e-08,
      "loss": 2.1696,
      "step": 1483
    },
    {
      "epoch": 0.9769585253456221,
      "grad_norm": 12.268321990966797,
      "learning_rate": 1.6166077141641646e-08,
      "loss": 1.5024,
      "step": 1484
    },
    {
      "epoch": 0.97761685319289,
      "grad_norm": 3.1240689754486084,
      "learning_rate": 1.5255961452533273e-08,
      "loss": 1.3765,
      "step": 1485
    },
    {
      "epoch": 0.978275181040158,
      "grad_norm": 182.64169311523438,
      "learning_rate": 1.4372173006775293e-08,
      "loss": 4.1481,
      "step": 1486
    },
    {
      "epoch": 0.9789335088874259,
      "grad_norm": 243.14962768554688,
      "learning_rate": 1.3514716472151634e-08,
      "loss": 2.6059,
      "step": 1487
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 58.1439208984375,
      "learning_rate": 1.2683596377373598e-08,
      "loss": 1.7544,
      "step": 1488
    },
    {
      "epoch": 0.9802501645819618,
      "grad_norm": 1.9220765829086304,
      "learning_rate": 1.1878817112055962e-08,
      "loss": 1.3809,
      "step": 1489
    },
    {
      "epoch": 0.9809084924292297,
      "grad_norm": 41.95667266845703,
      "learning_rate": 1.1100382926690911e-08,
      "loss": 1.746,
      "step": 1490
    },
    {
      "epoch": 0.9815668202764977,
      "grad_norm": 28.368104934692383,
      "learning_rate": 1.0348297932628037e-08,
      "loss": 1.6673,
      "step": 1491
    },
    {
      "epoch": 0.9822251481237656,
      "grad_norm": 1.8040703535079956,
      "learning_rate": 9.6225661020527e-09,
      "loss": 1.3734,
      "step": 1492
    },
    {
      "epoch": 0.9828834759710335,
      "grad_norm": 29.489913940429688,
      "learning_rate": 8.923191267963816e-09,
      "loss": 1.5365,
      "step": 1493
    },
    {
      "epoch": 0.9835418038183015,
      "grad_norm": 1.2152013778686523,
      "learning_rate": 8.25017712415388e-09,
      "loss": 1.3722,
      "step": 1494
    },
    {
      "epoch": 0.9842001316655694,
      "grad_norm": 1.8509271144866943,
      "learning_rate": 7.603527225190088e-09,
      "loss": 1.3817,
      "step": 1495
    },
    {
      "epoch": 0.9848584595128373,
      "grad_norm": 34.09790802001953,
      "learning_rate": 6.98324498639491e-09,
      "loss": 1.6818,
      "step": 1496
    },
    {
      "epoch": 0.9855167873601053,
      "grad_norm": 1.5842331647872925,
      "learning_rate": 6.38933368382888e-09,
      "loss": 1.383,
      "step": 1497
    },
    {
      "epoch": 0.9861751152073732,
      "grad_norm": 43.56391143798828,
      "learning_rate": 5.8217964542722774e-09,
      "loss": 1.7739,
      "step": 1498
    },
    {
      "epoch": 0.9868334430546412,
      "grad_norm": 1.7416644096374512,
      "learning_rate": 5.28063629520903e-09,
      "loss": 1.3753,
      "step": 1499
    },
    {
      "epoch": 0.9874917709019092,
      "grad_norm": 1.7117242813110352,
      "learning_rate": 4.765856064810614e-09,
      "loss": 1.3739,
      "step": 1500
    },
    {
      "epoch": 0.9881500987491771,
      "grad_norm": 1.9684370756149292,
      "learning_rate": 4.277458481922736e-09,
      "loss": 1.3735,
      "step": 1501
    },
    {
      "epoch": 0.9888084265964451,
      "grad_norm": 38.23576736450195,
      "learning_rate": 3.8154461260475615e-09,
      "loss": 1.6833,
      "step": 1502
    },
    {
      "epoch": 0.989466754443713,
      "grad_norm": 2.546396255493164,
      "learning_rate": 3.37982143733262e-09,
      "loss": 1.3772,
      "step": 1503
    },
    {
      "epoch": 0.990125082290981,
      "grad_norm": 26.05986976623535,
      "learning_rate": 2.970586716558033e-09,
      "loss": 1.4201,
      "step": 1504
    },
    {
      "epoch": 0.9907834101382489,
      "grad_norm": 1.2589483261108398,
      "learning_rate": 2.5877441251226375e-09,
      "loss": 1.375,
      "step": 1505
    },
    {
      "epoch": 0.9914417379855168,
      "grad_norm": 40.474735260009766,
      "learning_rate": 2.2312956850345514e-09,
      "loss": 1.7896,
      "step": 1506
    },
    {
      "epoch": 0.9921000658327848,
      "grad_norm": 26.489261627197266,
      "learning_rate": 1.9012432788984015e-09,
      "loss": 1.6774,
      "step": 1507
    },
    {
      "epoch": 0.9927583936800527,
      "grad_norm": 1.1008522510528564,
      "learning_rate": 1.5975886499064453e-09,
      "loss": 1.3743,
      "step": 1508
    },
    {
      "epoch": 0.9934167215273206,
      "grad_norm": 1.2440948486328125,
      "learning_rate": 1.320333401831353e-09,
      "loss": 1.3729,
      "step": 1509
    },
    {
      "epoch": 0.9940750493745886,
      "grad_norm": 131.2242889404297,
      "learning_rate": 1.0694789990134403e-09,
      "loss": 2.6317,
      "step": 1510
    },
    {
      "epoch": 0.9947333772218565,
      "grad_norm": 47.43233108520508,
      "learning_rate": 8.450267663567824e-10,
      "loss": 1.8621,
      "step": 1511
    },
    {
      "epoch": 0.9953917050691244,
      "grad_norm": 1.3271501064300537,
      "learning_rate": 6.469778893197775e-10,
      "loss": 1.3786,
      "step": 1512
    },
    {
      "epoch": 0.9960500329163924,
      "grad_norm": 56.707427978515625,
      "learning_rate": 4.753334139101506e-10,
      "loss": 1.9611,
      "step": 1513
    },
    {
      "epoch": 0.9967083607636603,
      "grad_norm": 1.2939485311508179,
      "learning_rate": 3.3009424667884705e-10,
      "loss": 1.3719,
      "step": 1514
    },
    {
      "epoch": 0.9973666886109283,
      "grad_norm": 1.2984050512313843,
      "learning_rate": 2.1126115471670204e-10,
      "loss": 1.3753,
      "step": 1515
    },
    {
      "epoch": 0.9980250164581962,
      "grad_norm": 1.388020634651184,
      "learning_rate": 1.188347656477795e-10,
      "loss": 1.3752,
      "step": 1516
    },
    {
      "epoch": 0.9986833443054641,
      "grad_norm": 2.08591890335083,
      "learning_rate": 5.281556762770645e-11,
      "loss": 1.384,
      "step": 1517
    },
    {
      "epoch": 0.9993416721527321,
      "grad_norm": 49.131507873535156,
      "learning_rate": 1.3203909341452659e-11,
      "loss": 1.9942,
      "step": 1518
    },
    {
      "epoch": 1.0,
      "grad_norm": 96.58551788330078,
      "learning_rate": 0.0,
      "loss": 1.914,
      "step": 1519
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.5798816568047337,
      "eval_loss": 1.71526300907135,
      "eval_runtime": 89.3334,
      "eval_samples_per_second": 1.892,
      "eval_steps_per_second": 1.892,
      "step": 1519
    }
  ],
  "logging_steps": 1,
  "max_steps": 1519,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.053606820675584e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
