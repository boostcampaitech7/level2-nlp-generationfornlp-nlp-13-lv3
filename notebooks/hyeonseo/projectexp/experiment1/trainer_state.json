{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 4557,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006583278472679394,
      "grad_norm": 2.131429433822632,
      "learning_rate": 1.9999997623643054e-05,
      "loss": 8.2211,
      "step": 1
    },
    {
      "epoch": 0.0013166556945358788,
      "grad_norm": 1.874447226524353,
      "learning_rate": 1.9999990494573347e-05,
      "loss": 8.4198,
      "step": 2
    },
    {
      "epoch": 0.0019749835418038184,
      "grad_norm": 2.2703394889831543,
      "learning_rate": 1.9999978612794268e-05,
      "loss": 8.9605,
      "step": 3
    },
    {
      "epoch": 0.0026333113890717576,
      "grad_norm": 1.8485312461853027,
      "learning_rate": 1.9999961978311458e-05,
      "loss": 8.5751,
      "step": 4
    },
    {
      "epoch": 0.0032916392363396972,
      "grad_norm": 1.8476485013961792,
      "learning_rate": 1.9999940591132826e-05,
      "loss": 8.5206,
      "step": 5
    },
    {
      "epoch": 0.003949967083607637,
      "grad_norm": 1.8567999601364136,
      "learning_rate": 1.999991445126854e-05,
      "loss": 7.9137,
      "step": 6
    },
    {
      "epoch": 0.004608294930875576,
      "grad_norm": 2.11399245262146,
      "learning_rate": 1.9999883558731018e-05,
      "loss": 8.6014,
      "step": 7
    },
    {
      "epoch": 0.005266622778143515,
      "grad_norm": 1.7252010107040405,
      "learning_rate": 1.999984791353495e-05,
      "loss": 8.1081,
      "step": 8
    },
    {
      "epoch": 0.005924950625411455,
      "grad_norm": 1.8572436571121216,
      "learning_rate": 1.999980751569727e-05,
      "loss": 8.4809,
      "step": 9
    },
    {
      "epoch": 0.0065832784726793945,
      "grad_norm": 2.0901010036468506,
      "learning_rate": 1.999976236523718e-05,
      "loss": 8.2253,
      "step": 10
    },
    {
      "epoch": 0.007241606319947334,
      "grad_norm": 2.128286361694336,
      "learning_rate": 1.999971246217614e-05,
      "loss": 8.4509,
      "step": 11
    },
    {
      "epoch": 0.007899934167215274,
      "grad_norm": 2.2189273834228516,
      "learning_rate": 1.9999657806537867e-05,
      "loss": 8.3134,
      "step": 12
    },
    {
      "epoch": 0.008558262014483212,
      "grad_norm": 2.007449150085449,
      "learning_rate": 1.9999598398348335e-05,
      "loss": 8.3925,
      "step": 13
    },
    {
      "epoch": 0.009216589861751152,
      "grad_norm": 2.3200554847717285,
      "learning_rate": 1.999953423763578e-05,
      "loss": 8.6073,
      "step": 14
    },
    {
      "epoch": 0.009874917709019092,
      "grad_norm": 2.177006959915161,
      "learning_rate": 1.9999465324430703e-05,
      "loss": 8.0826,
      "step": 15
    },
    {
      "epoch": 0.01053324555628703,
      "grad_norm": 1.8942843675613403,
      "learning_rate": 1.9999391658765847e-05,
      "loss": 7.9221,
      "step": 16
    },
    {
      "epoch": 0.01119157340355497,
      "grad_norm": 2.301038980484009,
      "learning_rate": 1.999931324067623e-05,
      "loss": 8.9059,
      "step": 17
    },
    {
      "epoch": 0.01184990125082291,
      "grad_norm": 2.6364622116088867,
      "learning_rate": 1.9999230070199108e-05,
      "loss": 8.908,
      "step": 18
    },
    {
      "epoch": 0.012508229098090849,
      "grad_norm": 2.1194939613342285,
      "learning_rate": 1.999914214737403e-05,
      "loss": 7.964,
      "step": 19
    },
    {
      "epoch": 0.013166556945358789,
      "grad_norm": 2.768700122833252,
      "learning_rate": 1.999904947224277e-05,
      "loss": 8.5084,
      "step": 20
    },
    {
      "epoch": 0.013824884792626729,
      "grad_norm": 2.1955950260162354,
      "learning_rate": 1.9998952044849376e-05,
      "loss": 8.3956,
      "step": 21
    },
    {
      "epoch": 0.014483212639894667,
      "grad_norm": 2.4403765201568604,
      "learning_rate": 1.9998849865240154e-05,
      "loss": 8.4748,
      "step": 22
    },
    {
      "epoch": 0.015141540487162607,
      "grad_norm": 2.4176599979400635,
      "learning_rate": 1.9998742933463668e-05,
      "loss": 8.0906,
      "step": 23
    },
    {
      "epoch": 0.015799868334430547,
      "grad_norm": 2.1145386695861816,
      "learning_rate": 1.9998631249570733e-05,
      "loss": 7.7036,
      "step": 24
    },
    {
      "epoch": 0.016458196181698487,
      "grad_norm": 2.708209753036499,
      "learning_rate": 1.9998514813614438e-05,
      "loss": 7.9774,
      "step": 25
    },
    {
      "epoch": 0.017116524028966424,
      "grad_norm": 2.6566216945648193,
      "learning_rate": 1.9998393625650118e-05,
      "loss": 8.0154,
      "step": 26
    },
    {
      "epoch": 0.017774851876234364,
      "grad_norm": 2.1627070903778076,
      "learning_rate": 1.9998267685735368e-05,
      "loss": 8.0382,
      "step": 27
    },
    {
      "epoch": 0.018433179723502304,
      "grad_norm": 2.339254140853882,
      "learning_rate": 1.9998136993930045e-05,
      "loss": 7.767,
      "step": 28
    },
    {
      "epoch": 0.019091507570770244,
      "grad_norm": 2.652705669403076,
      "learning_rate": 1.9998001550296266e-05,
      "loss": 7.8525,
      "step": 29
    },
    {
      "epoch": 0.019749835418038184,
      "grad_norm": 3.0964508056640625,
      "learning_rate": 1.9997861354898403e-05,
      "loss": 8.0887,
      "step": 30
    },
    {
      "epoch": 0.02040816326530612,
      "grad_norm": 2.475480556488037,
      "learning_rate": 1.999771640780308e-05,
      "loss": 8.1929,
      "step": 31
    },
    {
      "epoch": 0.02106649111257406,
      "grad_norm": 3.203205108642578,
      "learning_rate": 1.9997566709079195e-05,
      "loss": 7.8079,
      "step": 32
    },
    {
      "epoch": 0.021724818959842,
      "grad_norm": 3.8465611934661865,
      "learning_rate": 1.9997412258797892e-05,
      "loss": 7.9051,
      "step": 33
    },
    {
      "epoch": 0.02238314680710994,
      "grad_norm": 3.329432964324951,
      "learning_rate": 1.9997253057032572e-05,
      "loss": 7.6807,
      "step": 34
    },
    {
      "epoch": 0.02304147465437788,
      "grad_norm": 2.7468819618225098,
      "learning_rate": 1.999708910385891e-05,
      "loss": 7.9689,
      "step": 35
    },
    {
      "epoch": 0.02369980250164582,
      "grad_norm": 2.7694599628448486,
      "learning_rate": 1.9996920399354818e-05,
      "loss": 7.4663,
      "step": 36
    },
    {
      "epoch": 0.024358130348913758,
      "grad_norm": 3.278832197189331,
      "learning_rate": 1.9996746943600484e-05,
      "loss": 7.6584,
      "step": 37
    },
    {
      "epoch": 0.025016458196181698,
      "grad_norm": 2.727351665496826,
      "learning_rate": 1.9996568736678337e-05,
      "loss": 7.4638,
      "step": 38
    },
    {
      "epoch": 0.025674786043449638,
      "grad_norm": 3.449983596801758,
      "learning_rate": 1.9996385778673085e-05,
      "loss": 7.5807,
      "step": 39
    },
    {
      "epoch": 0.026333113890717578,
      "grad_norm": 3.3749680519104004,
      "learning_rate": 1.9996198069671677e-05,
      "loss": 7.8727,
      "step": 40
    },
    {
      "epoch": 0.026991441737985518,
      "grad_norm": 3.049487829208374,
      "learning_rate": 1.999600560976332e-05,
      "loss": 7.6239,
      "step": 41
    },
    {
      "epoch": 0.027649769585253458,
      "grad_norm": 3.3943774700164795,
      "learning_rate": 1.9995808399039496e-05,
      "loss": 7.4753,
      "step": 42
    },
    {
      "epoch": 0.028308097432521395,
      "grad_norm": 4.065996170043945,
      "learning_rate": 1.9995606437593926e-05,
      "loss": 8.0557,
      "step": 43
    },
    {
      "epoch": 0.028966425279789335,
      "grad_norm": 3.1930477619171143,
      "learning_rate": 1.99953997255226e-05,
      "loss": 7.4173,
      "step": 44
    },
    {
      "epoch": 0.029624753127057275,
      "grad_norm": 3.6218087673187256,
      "learning_rate": 1.9995188262923762e-05,
      "loss": 7.3965,
      "step": 45
    },
    {
      "epoch": 0.030283080974325215,
      "grad_norm": 3.7268083095550537,
      "learning_rate": 1.999497204989791e-05,
      "loss": 7.5649,
      "step": 46
    },
    {
      "epoch": 0.030941408821593155,
      "grad_norm": 3.132927656173706,
      "learning_rate": 1.9994751086547807e-05,
      "loss": 8.127,
      "step": 47
    },
    {
      "epoch": 0.031599736668861095,
      "grad_norm": 3.9159648418426514,
      "learning_rate": 1.9994525372978475e-05,
      "loss": 7.4368,
      "step": 48
    },
    {
      "epoch": 0.03225806451612903,
      "grad_norm": 4.159717082977295,
      "learning_rate": 1.999429490929718e-05,
      "loss": 7.1949,
      "step": 49
    },
    {
      "epoch": 0.032916392363396975,
      "grad_norm": 3.6679251194000244,
      "learning_rate": 1.9994059695613465e-05,
      "loss": 7.7799,
      "step": 50
    },
    {
      "epoch": 0.03357472021066491,
      "grad_norm": 4.097235202789307,
      "learning_rate": 1.999381973203911e-05,
      "loss": 7.4847,
      "step": 51
    },
    {
      "epoch": 0.03423304805793285,
      "grad_norm": 3.427018404006958,
      "learning_rate": 1.9993575018688172e-05,
      "loss": 7.6354,
      "step": 52
    },
    {
      "epoch": 0.03489137590520079,
      "grad_norm": 4.017534255981445,
      "learning_rate": 1.999332555567695e-05,
      "loss": 7.2401,
      "step": 53
    },
    {
      "epoch": 0.03554970375246873,
      "grad_norm": 4.688260078430176,
      "learning_rate": 1.9993071343124008e-05,
      "loss": 6.9096,
      "step": 54
    },
    {
      "epoch": 0.03620803159973667,
      "grad_norm": 3.891077756881714,
      "learning_rate": 1.9992812381150165e-05,
      "loss": 6.9904,
      "step": 55
    },
    {
      "epoch": 0.03686635944700461,
      "grad_norm": 5.140750408172607,
      "learning_rate": 1.9992548669878503e-05,
      "loss": 6.8471,
      "step": 56
    },
    {
      "epoch": 0.037524687294272545,
      "grad_norm": 5.4405598640441895,
      "learning_rate": 1.999228020943435e-05,
      "loss": 6.6756,
      "step": 57
    },
    {
      "epoch": 0.03818301514154049,
      "grad_norm": 5.329137802124023,
      "learning_rate": 1.9992006999945306e-05,
      "loss": 6.8097,
      "step": 58
    },
    {
      "epoch": 0.038841342988808425,
      "grad_norm": 5.3259172439575195,
      "learning_rate": 1.9991729041541213e-05,
      "loss": 6.724,
      "step": 59
    },
    {
      "epoch": 0.03949967083607637,
      "grad_norm": 6.377849578857422,
      "learning_rate": 1.9991446334354173e-05,
      "loss": 6.9969,
      "step": 60
    },
    {
      "epoch": 0.040157998683344305,
      "grad_norm": 5.741794586181641,
      "learning_rate": 1.999115887851856e-05,
      "loss": 6.5094,
      "step": 61
    },
    {
      "epoch": 0.04081632653061224,
      "grad_norm": 4.887935638427734,
      "learning_rate": 1.9990866674170984e-05,
      "loss": 7.1995,
      "step": 62
    },
    {
      "epoch": 0.041474654377880185,
      "grad_norm": 7.321499824523926,
      "learning_rate": 1.9990569721450324e-05,
      "loss": 6.3887,
      "step": 63
    },
    {
      "epoch": 0.04213298222514812,
      "grad_norm": 7.5880537033081055,
      "learning_rate": 1.999026802049772e-05,
      "loss": 6.2555,
      "step": 64
    },
    {
      "epoch": 0.042791310072416065,
      "grad_norm": 7.176050186157227,
      "learning_rate": 1.9989961571456548e-05,
      "loss": 6.3701,
      "step": 65
    },
    {
      "epoch": 0.043449637919684,
      "grad_norm": 4.008523941040039,
      "learning_rate": 1.9989650374472466e-05,
      "loss": 6.4863,
      "step": 66
    },
    {
      "epoch": 0.044107965766951945,
      "grad_norm": 6.208988666534424,
      "learning_rate": 1.998933442969337e-05,
      "loss": 6.4546,
      "step": 67
    },
    {
      "epoch": 0.04476629361421988,
      "grad_norm": 6.958259105682373,
      "learning_rate": 1.9989013737269424e-05,
      "loss": 6.1952,
      "step": 68
    },
    {
      "epoch": 0.04542462146148782,
      "grad_norm": 6.53776741027832,
      "learning_rate": 1.9988688297353045e-05,
      "loss": 7.0526,
      "step": 69
    },
    {
      "epoch": 0.04608294930875576,
      "grad_norm": 8.981145858764648,
      "learning_rate": 1.9988358110098903e-05,
      "loss": 6.0594,
      "step": 70
    },
    {
      "epoch": 0.0467412771560237,
      "grad_norm": 8.200749397277832,
      "learning_rate": 1.9988023175663925e-05,
      "loss": 6.7901,
      "step": 71
    },
    {
      "epoch": 0.04739960500329164,
      "grad_norm": 7.343729019165039,
      "learning_rate": 1.9987683494207294e-05,
      "loss": 6.5937,
      "step": 72
    },
    {
      "epoch": 0.04805793285055958,
      "grad_norm": 6.643872261047363,
      "learning_rate": 1.998733906589046e-05,
      "loss": 7.0166,
      "step": 73
    },
    {
      "epoch": 0.048716260697827515,
      "grad_norm": 6.171182632446289,
      "learning_rate": 1.998698989087711e-05,
      "loss": 5.6508,
      "step": 74
    },
    {
      "epoch": 0.04937458854509546,
      "grad_norm": 6.453584671020508,
      "learning_rate": 1.9986635969333204e-05,
      "loss": 6.0635,
      "step": 75
    },
    {
      "epoch": 0.050032916392363395,
      "grad_norm": 6.619903564453125,
      "learning_rate": 1.9986277301426943e-05,
      "loss": 5.6727,
      "step": 76
    },
    {
      "epoch": 0.05069124423963134,
      "grad_norm": 6.233262538909912,
      "learning_rate": 1.9985913887328802e-05,
      "loss": 5.4969,
      "step": 77
    },
    {
      "epoch": 0.051349572086899276,
      "grad_norm": 6.276513576507568,
      "learning_rate": 1.998554572721149e-05,
      "loss": 5.912,
      "step": 78
    },
    {
      "epoch": 0.05200789993416721,
      "grad_norm": 4.289707660675049,
      "learning_rate": 1.9985172821249993e-05,
      "loss": 5.9622,
      "step": 79
    },
    {
      "epoch": 0.052666227781435156,
      "grad_norm": 6.991359710693359,
      "learning_rate": 1.9984795169621534e-05,
      "loss": 5.6427,
      "step": 80
    },
    {
      "epoch": 0.05332455562870309,
      "grad_norm": 4.803580284118652,
      "learning_rate": 1.9984412772505606e-05,
      "loss": 5.2754,
      "step": 81
    },
    {
      "epoch": 0.053982883475971036,
      "grad_norm": 7.155777931213379,
      "learning_rate": 1.9984025630083952e-05,
      "loss": 6.0931,
      "step": 82
    },
    {
      "epoch": 0.05464121132323897,
      "grad_norm": 4.042895793914795,
      "learning_rate": 1.9983633742540564e-05,
      "loss": 5.1358,
      "step": 83
    },
    {
      "epoch": 0.055299539170506916,
      "grad_norm": 4.877330303192139,
      "learning_rate": 1.9983237110061696e-05,
      "loss": 5.8304,
      "step": 84
    },
    {
      "epoch": 0.05595786701777485,
      "grad_norm": 3.8697471618652344,
      "learning_rate": 1.9982835732835864e-05,
      "loss": 5.129,
      "step": 85
    },
    {
      "epoch": 0.05661619486504279,
      "grad_norm": 4.573492050170898,
      "learning_rate": 1.9982429611053823e-05,
      "loss": 5.1493,
      "step": 86
    },
    {
      "epoch": 0.05727452271231073,
      "grad_norm": 3.911602735519409,
      "learning_rate": 1.9982018744908592e-05,
      "loss": 5.6265,
      "step": 87
    },
    {
      "epoch": 0.05793285055957867,
      "grad_norm": 3.0715696811676025,
      "learning_rate": 1.9981603134595446e-05,
      "loss": 5.15,
      "step": 88
    },
    {
      "epoch": 0.05859117840684661,
      "grad_norm": 3.521786689758301,
      "learning_rate": 1.9981182780311913e-05,
      "loss": 4.9666,
      "step": 89
    },
    {
      "epoch": 0.05924950625411455,
      "grad_norm": 3.0700414180755615,
      "learning_rate": 1.9980757682257774e-05,
      "loss": 5.094,
      "step": 90
    },
    {
      "epoch": 0.059907834101382486,
      "grad_norm": 3.592803478240967,
      "learning_rate": 1.9980327840635066e-05,
      "loss": 5.4737,
      "step": 91
    },
    {
      "epoch": 0.06056616194865043,
      "grad_norm": 2.6883506774902344,
      "learning_rate": 1.997989325564808e-05,
      "loss": 4.8749,
      "step": 92
    },
    {
      "epoch": 0.061224489795918366,
      "grad_norm": 3.218057632446289,
      "learning_rate": 1.9979453927503366e-05,
      "loss": 4.871,
      "step": 93
    },
    {
      "epoch": 0.06188281764318631,
      "grad_norm": 3.1622769832611084,
      "learning_rate": 1.9979009856409718e-05,
      "loss": 4.8265,
      "step": 94
    },
    {
      "epoch": 0.06254114549045425,
      "grad_norm": 2.3957061767578125,
      "learning_rate": 1.9978561042578194e-05,
      "loss": 4.9538,
      "step": 95
    },
    {
      "epoch": 0.06319947333772219,
      "grad_norm": 2.545215129852295,
      "learning_rate": 1.99781074862221e-05,
      "loss": 4.9696,
      "step": 96
    },
    {
      "epoch": 0.06385780118499013,
      "grad_norm": 2.868706226348877,
      "learning_rate": 1.9977649187557e-05,
      "loss": 5.4296,
      "step": 97
    },
    {
      "epoch": 0.06451612903225806,
      "grad_norm": 3.486070394515991,
      "learning_rate": 1.9977186146800707e-05,
      "loss": 4.8327,
      "step": 98
    },
    {
      "epoch": 0.065174456879526,
      "grad_norm": 2.560046911239624,
      "learning_rate": 1.99767183641733e-05,
      "loss": 4.8531,
      "step": 99
    },
    {
      "epoch": 0.06583278472679395,
      "grad_norm": 3.0063257217407227,
      "learning_rate": 1.997624583989709e-05,
      "loss": 5.1753,
      "step": 100
    },
    {
      "epoch": 0.06649111257406189,
      "grad_norm": 2.5161449909210205,
      "learning_rate": 1.9975768574196664e-05,
      "loss": 4.7236,
      "step": 101
    },
    {
      "epoch": 0.06714944042132982,
      "grad_norm": 3.150601863861084,
      "learning_rate": 1.9975286567298853e-05,
      "loss": 4.6489,
      "step": 102
    },
    {
      "epoch": 0.06780776826859776,
      "grad_norm": 2.6160809993743896,
      "learning_rate": 1.9974799819432736e-05,
      "loss": 4.6798,
      "step": 103
    },
    {
      "epoch": 0.0684660961158657,
      "grad_norm": 1.9759314060211182,
      "learning_rate": 1.997430833082965e-05,
      "loss": 4.6802,
      "step": 104
    },
    {
      "epoch": 0.06912442396313365,
      "grad_norm": 2.766986608505249,
      "learning_rate": 1.997381210172319e-05,
      "loss": 4.5548,
      "step": 105
    },
    {
      "epoch": 0.06978275181040158,
      "grad_norm": 2.303162097930908,
      "learning_rate": 1.9973311132349194e-05,
      "loss": 4.9786,
      "step": 106
    },
    {
      "epoch": 0.07044107965766952,
      "grad_norm": 3.30633544921875,
      "learning_rate": 1.9972805422945763e-05,
      "loss": 5.103,
      "step": 107
    },
    {
      "epoch": 0.07109940750493746,
      "grad_norm": 2.6778523921966553,
      "learning_rate": 1.9972294973753246e-05,
      "loss": 4.4188,
      "step": 108
    },
    {
      "epoch": 0.07175773535220539,
      "grad_norm": 2.392355442047119,
      "learning_rate": 1.9971779785014242e-05,
      "loss": 5.079,
      "step": 109
    },
    {
      "epoch": 0.07241606319947334,
      "grad_norm": 2.2442421913146973,
      "learning_rate": 1.9971259856973605e-05,
      "loss": 4.3989,
      "step": 110
    },
    {
      "epoch": 0.07307439104674128,
      "grad_norm": 2.3310251235961914,
      "learning_rate": 1.9970735189878448e-05,
      "loss": 4.4271,
      "step": 111
    },
    {
      "epoch": 0.07373271889400922,
      "grad_norm": 2.423827886581421,
      "learning_rate": 1.9970205783978125e-05,
      "loss": 4.2553,
      "step": 112
    },
    {
      "epoch": 0.07439104674127715,
      "grad_norm": 3.9609508514404297,
      "learning_rate": 1.9969671639524248e-05,
      "loss": 4.1971,
      "step": 113
    },
    {
      "epoch": 0.07504937458854509,
      "grad_norm": 2.67319655418396,
      "learning_rate": 1.9969132756770683e-05,
      "loss": 4.4095,
      "step": 114
    },
    {
      "epoch": 0.07570770243581304,
      "grad_norm": 2.4006121158599854,
      "learning_rate": 1.996858913597354e-05,
      "loss": 4.2488,
      "step": 115
    },
    {
      "epoch": 0.07636603028308098,
      "grad_norm": 2.835381507873535,
      "learning_rate": 1.9968040777391195e-05,
      "loss": 4.9313,
      "step": 116
    },
    {
      "epoch": 0.07702435813034891,
      "grad_norm": 2.2552809715270996,
      "learning_rate": 1.9967487681284255e-05,
      "loss": 4.4377,
      "step": 117
    },
    {
      "epoch": 0.07768268597761685,
      "grad_norm": 3.183151960372925,
      "learning_rate": 1.9966929847915603e-05,
      "loss": 4.7828,
      "step": 118
    },
    {
      "epoch": 0.07834101382488479,
      "grad_norm": 2.639042854309082,
      "learning_rate": 1.9966367277550358e-05,
      "loss": 4.0599,
      "step": 119
    },
    {
      "epoch": 0.07899934167215274,
      "grad_norm": 2.4085166454315186,
      "learning_rate": 1.9965799970455892e-05,
      "loss": 4.0049,
      "step": 120
    },
    {
      "epoch": 0.07965766951942067,
      "grad_norm": 2.748117208480835,
      "learning_rate": 1.9965227926901826e-05,
      "loss": 4.0094,
      "step": 121
    },
    {
      "epoch": 0.08031599736668861,
      "grad_norm": 2.413008451461792,
      "learning_rate": 1.996465114716004e-05,
      "loss": 4.3727,
      "step": 122
    },
    {
      "epoch": 0.08097432521395655,
      "grad_norm": 2.5977752208709717,
      "learning_rate": 1.9964069631504664e-05,
      "loss": 4.0847,
      "step": 123
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 2.863373041152954,
      "learning_rate": 1.996348338021207e-05,
      "loss": 4.2942,
      "step": 124
    },
    {
      "epoch": 0.08229098090849243,
      "grad_norm": 3.318307399749756,
      "learning_rate": 1.9962892393560892e-05,
      "loss": 4.6677,
      "step": 125
    },
    {
      "epoch": 0.08294930875576037,
      "grad_norm": 2.649747610092163,
      "learning_rate": 1.9962296671832e-05,
      "loss": 4.5193,
      "step": 126
    },
    {
      "epoch": 0.0836076366030283,
      "grad_norm": 3.713796377182007,
      "learning_rate": 1.9961696215308536e-05,
      "loss": 4.1783,
      "step": 127
    },
    {
      "epoch": 0.08426596445029624,
      "grad_norm": 2.4599766731262207,
      "learning_rate": 1.996109102427587e-05,
      "loss": 4.3547,
      "step": 128
    },
    {
      "epoch": 0.08492429229756418,
      "grad_norm": 2.6411564350128174,
      "learning_rate": 1.9960481099021636e-05,
      "loss": 4.4506,
      "step": 129
    },
    {
      "epoch": 0.08558262014483213,
      "grad_norm": 2.3325717449188232,
      "learning_rate": 1.995986643983571e-05,
      "loss": 4.4655,
      "step": 130
    },
    {
      "epoch": 0.08624094799210007,
      "grad_norm": 3.5290746688842773,
      "learning_rate": 1.995924704701023e-05,
      "loss": 4.4527,
      "step": 131
    },
    {
      "epoch": 0.086899275839368,
      "grad_norm": 2.791114091873169,
      "learning_rate": 1.995862292083957e-05,
      "loss": 3.8876,
      "step": 132
    },
    {
      "epoch": 0.08755760368663594,
      "grad_norm": 2.906358480453491,
      "learning_rate": 1.9957994061620357e-05,
      "loss": 4.4304,
      "step": 133
    },
    {
      "epoch": 0.08821593153390389,
      "grad_norm": 2.5056283473968506,
      "learning_rate": 1.9957360469651475e-05,
      "loss": 3.7357,
      "step": 134
    },
    {
      "epoch": 0.08887425938117183,
      "grad_norm": 2.346588373184204,
      "learning_rate": 1.995672214523405e-05,
      "loss": 4.3005,
      "step": 135
    },
    {
      "epoch": 0.08953258722843976,
      "grad_norm": 2.137253761291504,
      "learning_rate": 1.9956079088671457e-05,
      "loss": 4.2993,
      "step": 136
    },
    {
      "epoch": 0.0901909150757077,
      "grad_norm": 2.482956647872925,
      "learning_rate": 1.9955431300269328e-05,
      "loss": 3.6519,
      "step": 137
    },
    {
      "epoch": 0.09084924292297564,
      "grad_norm": 2.209796667098999,
      "learning_rate": 1.9954778780335534e-05,
      "loss": 4.2342,
      "step": 138
    },
    {
      "epoch": 0.09150757077024359,
      "grad_norm": 2.3055856227874756,
      "learning_rate": 1.99541215291802e-05,
      "loss": 3.57,
      "step": 139
    },
    {
      "epoch": 0.09216589861751152,
      "grad_norm": 2.589059591293335,
      "learning_rate": 1.99534595471157e-05,
      "loss": 3.6406,
      "step": 140
    },
    {
      "epoch": 0.09282422646477946,
      "grad_norm": 2.154482841491699,
      "learning_rate": 1.995279283445665e-05,
      "loss": 3.5667,
      "step": 141
    },
    {
      "epoch": 0.0934825543120474,
      "grad_norm": 2.5604920387268066,
      "learning_rate": 1.995212139151993e-05,
      "loss": 3.6167,
      "step": 142
    },
    {
      "epoch": 0.09414088215931533,
      "grad_norm": 1.971689224243164,
      "learning_rate": 1.9951445218624645e-05,
      "loss": 3.5271,
      "step": 143
    },
    {
      "epoch": 0.09479921000658328,
      "grad_norm": 2.4627645015716553,
      "learning_rate": 1.9950764316092168e-05,
      "loss": 3.8268,
      "step": 144
    },
    {
      "epoch": 0.09545753785385122,
      "grad_norm": 2.5557057857513428,
      "learning_rate": 1.9950078684246114e-05,
      "loss": 4.0891,
      "step": 145
    },
    {
      "epoch": 0.09611586570111916,
      "grad_norm": 1.7670050859451294,
      "learning_rate": 1.9949388323412337e-05,
      "loss": 3.4828,
      "step": 146
    },
    {
      "epoch": 0.0967741935483871,
      "grad_norm": 2.012026786804199,
      "learning_rate": 1.994869323391895e-05,
      "loss": 3.9926,
      "step": 147
    },
    {
      "epoch": 0.09743252139565503,
      "grad_norm": 1.997113585472107,
      "learning_rate": 1.9947993416096313e-05,
      "loss": 3.4312,
      "step": 148
    },
    {
      "epoch": 0.09809084924292298,
      "grad_norm": 2.0231213569641113,
      "learning_rate": 1.9947288870277027e-05,
      "loss": 3.4822,
      "step": 149
    },
    {
      "epoch": 0.09874917709019092,
      "grad_norm": 2.801786184310913,
      "learning_rate": 1.9946579596795937e-05,
      "loss": 4.1208,
      "step": 150
    },
    {
      "epoch": 0.09940750493745885,
      "grad_norm": 2.0432980060577393,
      "learning_rate": 1.9945865595990145e-05,
      "loss": 3.4211,
      "step": 151
    },
    {
      "epoch": 0.10006583278472679,
      "grad_norm": 3.753666877746582,
      "learning_rate": 1.9945146868198995e-05,
      "loss": 4.13,
      "step": 152
    },
    {
      "epoch": 0.10072416063199473,
      "grad_norm": 2.071362257003784,
      "learning_rate": 1.994442341376408e-05,
      "loss": 3.3706,
      "step": 153
    },
    {
      "epoch": 0.10138248847926268,
      "grad_norm": 1.9010730981826782,
      "learning_rate": 1.9943695233029233e-05,
      "loss": 3.5417,
      "step": 154
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 2.741018533706665,
      "learning_rate": 1.994296232634054e-05,
      "loss": 3.9604,
      "step": 155
    },
    {
      "epoch": 0.10269914417379855,
      "grad_norm": 2.000938653945923,
      "learning_rate": 1.9942224694046324e-05,
      "loss": 3.3581,
      "step": 156
    },
    {
      "epoch": 0.10335747202106649,
      "grad_norm": 1.8520958423614502,
      "learning_rate": 1.994148233649717e-05,
      "loss": 3.3522,
      "step": 157
    },
    {
      "epoch": 0.10401579986833442,
      "grad_norm": 2.0715930461883545,
      "learning_rate": 1.99407352540459e-05,
      "loss": 3.9004,
      "step": 158
    },
    {
      "epoch": 0.10467412771560237,
      "grad_norm": 2.230563163757324,
      "learning_rate": 1.993998344704757e-05,
      "loss": 3.3455,
      "step": 159
    },
    {
      "epoch": 0.10533245556287031,
      "grad_norm": 1.8605252504348755,
      "learning_rate": 1.99392269158595e-05,
      "loss": 3.2978,
      "step": 160
    },
    {
      "epoch": 0.10599078341013825,
      "grad_norm": 3.099246025085449,
      "learning_rate": 1.9938465660841248e-05,
      "loss": 3.5922,
      "step": 161
    },
    {
      "epoch": 0.10664911125740618,
      "grad_norm": 2.520150661468506,
      "learning_rate": 1.9937699682354615e-05,
      "loss": 3.7959,
      "step": 162
    },
    {
      "epoch": 0.10730743910467412,
      "grad_norm": 1.8966418504714966,
      "learning_rate": 1.993692898076365e-05,
      "loss": 3.2556,
      "step": 163
    },
    {
      "epoch": 0.10796576695194207,
      "grad_norm": 1.7534664869308472,
      "learning_rate": 1.9936153556434637e-05,
      "loss": 3.2127,
      "step": 164
    },
    {
      "epoch": 0.10862409479921001,
      "grad_norm": 2.315462827682495,
      "learning_rate": 1.993537340973613e-05,
      "loss": 3.6879,
      "step": 165
    },
    {
      "epoch": 0.10928242264647794,
      "grad_norm": 1.8993945121765137,
      "learning_rate": 1.9934588541038894e-05,
      "loss": 3.2069,
      "step": 166
    },
    {
      "epoch": 0.10994075049374588,
      "grad_norm": 2.67368483543396,
      "learning_rate": 1.9933798950715963e-05,
      "loss": 3.831,
      "step": 167
    },
    {
      "epoch": 0.11059907834101383,
      "grad_norm": 1.7740821838378906,
      "learning_rate": 1.9933004639142606e-05,
      "loss": 3.204,
      "step": 168
    },
    {
      "epoch": 0.11125740618828177,
      "grad_norm": 3.4698879718780518,
      "learning_rate": 1.9932205606696336e-05,
      "loss": 3.8814,
      "step": 169
    },
    {
      "epoch": 0.1119157340355497,
      "grad_norm": 1.7177799940109253,
      "learning_rate": 1.9931401853756908e-05,
      "loss": 3.1588,
      "step": 170
    },
    {
      "epoch": 0.11257406188281764,
      "grad_norm": 2.51397705078125,
      "learning_rate": 1.9930593380706323e-05,
      "loss": 3.8491,
      "step": 171
    },
    {
      "epoch": 0.11323238973008558,
      "grad_norm": 2.3391685485839844,
      "learning_rate": 1.992978018792883e-05,
      "loss": 3.164,
      "step": 172
    },
    {
      "epoch": 0.11389071757735353,
      "grad_norm": 2.598336935043335,
      "learning_rate": 1.992896227581091e-05,
      "loss": 3.6823,
      "step": 173
    },
    {
      "epoch": 0.11454904542462147,
      "grad_norm": 2.037001132965088,
      "learning_rate": 1.99281396447413e-05,
      "loss": 3.7331,
      "step": 174
    },
    {
      "epoch": 0.1152073732718894,
      "grad_norm": 2.27333664894104,
      "learning_rate": 1.9927312295110966e-05,
      "loss": 3.4595,
      "step": 175
    },
    {
      "epoch": 0.11586570111915734,
      "grad_norm": 2.2041969299316406,
      "learning_rate": 1.9926480227313124e-05,
      "loss": 3.4363,
      "step": 176
    },
    {
      "epoch": 0.11652402896642527,
      "grad_norm": 2.7446296215057373,
      "learning_rate": 1.9925643441743242e-05,
      "loss": 3.6058,
      "step": 177
    },
    {
      "epoch": 0.11718235681369323,
      "grad_norm": 1.920914649963379,
      "learning_rate": 1.9924801938799008e-05,
      "loss": 3.1468,
      "step": 178
    },
    {
      "epoch": 0.11784068466096116,
      "grad_norm": 1.7597464323043823,
      "learning_rate": 1.992395571888037e-05,
      "loss": 3.327,
      "step": 179
    },
    {
      "epoch": 0.1184990125082291,
      "grad_norm": 2.9652254581451416,
      "learning_rate": 1.992310478238951e-05,
      "loss": 3.6187,
      "step": 180
    },
    {
      "epoch": 0.11915734035549704,
      "grad_norm": 1.6151669025421143,
      "learning_rate": 1.992224912973086e-05,
      "loss": 3.1067,
      "step": 181
    },
    {
      "epoch": 0.11981566820276497,
      "grad_norm": 1.983323335647583,
      "learning_rate": 1.9921388761311078e-05,
      "loss": 3.1157,
      "step": 182
    },
    {
      "epoch": 0.12047399605003292,
      "grad_norm": 2.382209539413452,
      "learning_rate": 1.9920523677539076e-05,
      "loss": 3.4194,
      "step": 183
    },
    {
      "epoch": 0.12113232389730086,
      "grad_norm": 3.938505172729492,
      "learning_rate": 1.9919653878826007e-05,
      "loss": 3.0943,
      "step": 184
    },
    {
      "epoch": 0.1217906517445688,
      "grad_norm": 1.7502325773239136,
      "learning_rate": 1.991877936558526e-05,
      "loss": 3.301,
      "step": 185
    },
    {
      "epoch": 0.12244897959183673,
      "grad_norm": 1.9340423345565796,
      "learning_rate": 1.991790013823246e-05,
      "loss": 3.2746,
      "step": 186
    },
    {
      "epoch": 0.12310730743910467,
      "grad_norm": 1.4838693141937256,
      "learning_rate": 1.991701619718549e-05,
      "loss": 3.0239,
      "step": 187
    },
    {
      "epoch": 0.12376563528637262,
      "grad_norm": 2.4557509422302246,
      "learning_rate": 1.991612754286445e-05,
      "loss": 3.3402,
      "step": 188
    },
    {
      "epoch": 0.12442396313364056,
      "grad_norm": 1.3655600547790527,
      "learning_rate": 1.99152341756917e-05,
      "loss": 3.0043,
      "step": 189
    },
    {
      "epoch": 0.1250822909809085,
      "grad_norm": 1.8512146472930908,
      "learning_rate": 1.991433609609183e-05,
      "loss": 3.6034,
      "step": 190
    },
    {
      "epoch": 0.12574061882817644,
      "grad_norm": 2.3076725006103516,
      "learning_rate": 1.9913433304491673e-05,
      "loss": 3.3447,
      "step": 191
    },
    {
      "epoch": 0.12639894667544438,
      "grad_norm": 1.3704451322555542,
      "learning_rate": 1.991252580132029e-05,
      "loss": 2.9648,
      "step": 192
    },
    {
      "epoch": 0.12705727452271232,
      "grad_norm": 1.542527437210083,
      "learning_rate": 1.9911613587009008e-05,
      "loss": 3.0018,
      "step": 193
    },
    {
      "epoch": 0.12771560236998025,
      "grad_norm": 1.8302013874053955,
      "learning_rate": 1.9910696661991364e-05,
      "loss": 3.5838,
      "step": 194
    },
    {
      "epoch": 0.1283739302172482,
      "grad_norm": 1.5709903240203857,
      "learning_rate": 1.990977502670315e-05,
      "loss": 2.9824,
      "step": 195
    },
    {
      "epoch": 0.12903225806451613,
      "grad_norm": 3.0927462577819824,
      "learning_rate": 1.990884868158239e-05,
      "loss": 3.7743,
      "step": 196
    },
    {
      "epoch": 0.12969058591178406,
      "grad_norm": 2.024407386779785,
      "learning_rate": 1.9907917627069353e-05,
      "loss": 3.03,
      "step": 197
    },
    {
      "epoch": 0.130348913759052,
      "grad_norm": 1.7120108604431152,
      "learning_rate": 1.9906981863606542e-05,
      "loss": 2.9748,
      "step": 198
    },
    {
      "epoch": 0.13100724160631994,
      "grad_norm": 1.8692477941513062,
      "learning_rate": 1.99060413916387e-05,
      "loss": 3.2427,
      "step": 199
    },
    {
      "epoch": 0.1316655694535879,
      "grad_norm": 1.542039394378662,
      "learning_rate": 1.9905096211612804e-05,
      "loss": 2.9317,
      "step": 200
    },
    {
      "epoch": 0.13232389730085584,
      "grad_norm": 1.9848405122756958,
      "learning_rate": 1.990414632397807e-05,
      "loss": 3.5591,
      "step": 201
    },
    {
      "epoch": 0.13298222514812377,
      "grad_norm": 2.160036325454712,
      "learning_rate": 1.9903191729185956e-05,
      "loss": 3.2723,
      "step": 202
    },
    {
      "epoch": 0.1336405529953917,
      "grad_norm": 2.1194567680358887,
      "learning_rate": 1.9902232427690152e-05,
      "loss": 3.2262,
      "step": 203
    },
    {
      "epoch": 0.13429888084265965,
      "grad_norm": 1.6180696487426758,
      "learning_rate": 1.9901268419946583e-05,
      "loss": 2.9709,
      "step": 204
    },
    {
      "epoch": 0.13495720868992758,
      "grad_norm": 1.3903084993362427,
      "learning_rate": 1.990029970641342e-05,
      "loss": 2.8974,
      "step": 205
    },
    {
      "epoch": 0.13561553653719552,
      "grad_norm": 1.5495710372924805,
      "learning_rate": 1.989932628755106e-05,
      "loss": 2.9421,
      "step": 206
    },
    {
      "epoch": 0.13627386438446346,
      "grad_norm": 2.454497814178467,
      "learning_rate": 1.9898348163822145e-05,
      "loss": 3.2583,
      "step": 207
    },
    {
      "epoch": 0.1369321922317314,
      "grad_norm": 2.9349637031555176,
      "learning_rate": 1.989736533569155e-05,
      "loss": 3.3928,
      "step": 208
    },
    {
      "epoch": 0.13759052007899933,
      "grad_norm": 1.7321641445159912,
      "learning_rate": 1.989637780362638e-05,
      "loss": 3.1773,
      "step": 209
    },
    {
      "epoch": 0.1382488479262673,
      "grad_norm": 1.3349387645721436,
      "learning_rate": 1.989538556809598e-05,
      "loss": 2.8905,
      "step": 210
    },
    {
      "epoch": 0.13890717577353523,
      "grad_norm": 1.364284873008728,
      "learning_rate": 1.989438862957194e-05,
      "loss": 2.8833,
      "step": 211
    },
    {
      "epoch": 0.13956550362080317,
      "grad_norm": 1.2163786888122559,
      "learning_rate": 1.989338698852807e-05,
      "loss": 2.8581,
      "step": 212
    },
    {
      "epoch": 0.1402238314680711,
      "grad_norm": 3.2900891304016113,
      "learning_rate": 1.9892380645440422e-05,
      "loss": 3.6383,
      "step": 213
    },
    {
      "epoch": 0.14088215931533904,
      "grad_norm": 1.6610667705535889,
      "learning_rate": 1.989136960078728e-05,
      "loss": 2.8692,
      "step": 214
    },
    {
      "epoch": 0.14154048716260698,
      "grad_norm": 1.2586286067962646,
      "learning_rate": 1.989035385504917e-05,
      "loss": 2.8684,
      "step": 215
    },
    {
      "epoch": 0.1421988150098749,
      "grad_norm": 2.0542991161346436,
      "learning_rate": 1.9889333408708843e-05,
      "loss": 3.14,
      "step": 216
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 2.2744741439819336,
      "learning_rate": 1.9888308262251286e-05,
      "loss": 3.4688,
      "step": 217
    },
    {
      "epoch": 0.14351547070441079,
      "grad_norm": 1.552370309829712,
      "learning_rate": 1.9887278416163728e-05,
      "loss": 3.0561,
      "step": 218
    },
    {
      "epoch": 0.14417379855167872,
      "grad_norm": 1.8462457656860352,
      "learning_rate": 1.988624387093562e-05,
      "loss": 3.1194,
      "step": 219
    },
    {
      "epoch": 0.1448321263989467,
      "grad_norm": 2.7355306148529053,
      "learning_rate": 1.9885204627058655e-05,
      "loss": 3.5628,
      "step": 220
    },
    {
      "epoch": 0.14549045424621462,
      "grad_norm": 3.0996041297912598,
      "learning_rate": 1.9884160685026755e-05,
      "loss": 3.3747,
      "step": 221
    },
    {
      "epoch": 0.14614878209348256,
      "grad_norm": 2.01707124710083,
      "learning_rate": 1.9883112045336075e-05,
      "loss": 3.4883,
      "step": 222
    },
    {
      "epoch": 0.1468071099407505,
      "grad_norm": 1.6864993572235107,
      "learning_rate": 1.9882058708485004e-05,
      "loss": 3.0757,
      "step": 223
    },
    {
      "epoch": 0.14746543778801843,
      "grad_norm": 2.2969207763671875,
      "learning_rate": 1.9881000674974164e-05,
      "loss": 3.5023,
      "step": 224
    },
    {
      "epoch": 0.14812376563528637,
      "grad_norm": 1.6781461238861084,
      "learning_rate": 1.9879937945306405e-05,
      "loss": 3.0099,
      "step": 225
    },
    {
      "epoch": 0.1487820934825543,
      "grad_norm": 1.356586217880249,
      "learning_rate": 1.9878870519986813e-05,
      "loss": 2.8274,
      "step": 226
    },
    {
      "epoch": 0.14944042132982224,
      "grad_norm": 1.9196473360061646,
      "learning_rate": 1.9877798399522706e-05,
      "loss": 3.0793,
      "step": 227
    },
    {
      "epoch": 0.15009874917709018,
      "grad_norm": 3.2683324813842773,
      "learning_rate": 1.9876721584423633e-05,
      "loss": 3.284,
      "step": 228
    },
    {
      "epoch": 0.15075707702435814,
      "grad_norm": 1.54341459274292,
      "learning_rate": 1.987564007520137e-05,
      "loss": 2.8158,
      "step": 229
    },
    {
      "epoch": 0.15141540487162608,
      "grad_norm": 3.076253890991211,
      "learning_rate": 1.987455387236993e-05,
      "loss": 3.2679,
      "step": 230
    },
    {
      "epoch": 0.15207373271889402,
      "grad_norm": 1.48078453540802,
      "learning_rate": 1.9873462976445553e-05,
      "loss": 2.8355,
      "step": 231
    },
    {
      "epoch": 0.15273206056616195,
      "grad_norm": 1.739936113357544,
      "learning_rate": 1.9872367387946717e-05,
      "loss": 2.8387,
      "step": 232
    },
    {
      "epoch": 0.1533903884134299,
      "grad_norm": 2.4108481407165527,
      "learning_rate": 1.9871267107394112e-05,
      "loss": 3.4115,
      "step": 233
    },
    {
      "epoch": 0.15404871626069783,
      "grad_norm": 2.8860809803009033,
      "learning_rate": 1.987016213531068e-05,
      "loss": 3.3147,
      "step": 234
    },
    {
      "epoch": 0.15470704410796576,
      "grad_norm": 1.4123402833938599,
      "learning_rate": 1.9869052472221574e-05,
      "loss": 2.8743,
      "step": 235
    },
    {
      "epoch": 0.1553653719552337,
      "grad_norm": 17.004268646240234,
      "learning_rate": 1.9867938118654192e-05,
      "loss": 2.8628,
      "step": 236
    },
    {
      "epoch": 0.15602369980250164,
      "grad_norm": 2.1127288341522217,
      "learning_rate": 1.986681907513815e-05,
      "loss": 2.9012,
      "step": 237
    },
    {
      "epoch": 0.15668202764976957,
      "grad_norm": 1.4673717021942139,
      "learning_rate": 1.9865695342205303e-05,
      "loss": 2.7949,
      "step": 238
    },
    {
      "epoch": 0.15734035549703754,
      "grad_norm": 2.4624133110046387,
      "learning_rate": 1.9864566920389724e-05,
      "loss": 3.1483,
      "step": 239
    },
    {
      "epoch": 0.15799868334430547,
      "grad_norm": 1.4588769674301147,
      "learning_rate": 1.986343381022772e-05,
      "loss": 2.7752,
      "step": 240
    },
    {
      "epoch": 0.1586570111915734,
      "grad_norm": 1.3642317056655884,
      "learning_rate": 1.9862296012257827e-05,
      "loss": 2.7462,
      "step": 241
    },
    {
      "epoch": 0.15931533903884135,
      "grad_norm": 1.595202922821045,
      "learning_rate": 1.986115352702081e-05,
      "loss": 2.8389,
      "step": 242
    },
    {
      "epoch": 0.15997366688610928,
      "grad_norm": 2.9767844676971436,
      "learning_rate": 1.9860006355059656e-05,
      "loss": 3.1907,
      "step": 243
    },
    {
      "epoch": 0.16063199473337722,
      "grad_norm": 2.906883716583252,
      "learning_rate": 1.985885449691958e-05,
      "loss": 3.0062,
      "step": 244
    },
    {
      "epoch": 0.16129032258064516,
      "grad_norm": 2.009366273880005,
      "learning_rate": 1.985769795314804e-05,
      "loss": 3.0194,
      "step": 245
    },
    {
      "epoch": 0.1619486504279131,
      "grad_norm": 3.2595980167388916,
      "learning_rate": 1.9856536724294696e-05,
      "loss": 3.1669,
      "step": 246
    },
    {
      "epoch": 0.16260697827518103,
      "grad_norm": 2.0266640186309814,
      "learning_rate": 1.985537081091145e-05,
      "loss": 3.3835,
      "step": 247
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 2.722224473953247,
      "learning_rate": 1.9854200213552426e-05,
      "loss": 3.1597,
      "step": 248
    },
    {
      "epoch": 0.16392363396971693,
      "grad_norm": 2.4468650817871094,
      "learning_rate": 1.985302493277398e-05,
      "loss": 3.4259,
      "step": 249
    },
    {
      "epoch": 0.16458196181698487,
      "grad_norm": 2.4355688095092773,
      "learning_rate": 1.9851844969134687e-05,
      "loss": 3.0825,
      "step": 250
    },
    {
      "epoch": 0.1652402896642528,
      "grad_norm": 3.266981601715088,
      "learning_rate": 1.985066032319535e-05,
      "loss": 3.3658,
      "step": 251
    },
    {
      "epoch": 0.16589861751152074,
      "grad_norm": 2.2083301544189453,
      "learning_rate": 1.9849470995518993e-05,
      "loss": 3.3846,
      "step": 252
    },
    {
      "epoch": 0.16655694535878868,
      "grad_norm": 1.5849153995513916,
      "learning_rate": 1.9848276986670874e-05,
      "loss": 2.9541,
      "step": 253
    },
    {
      "epoch": 0.1672152732060566,
      "grad_norm": 1.5867443084716797,
      "learning_rate": 1.9847078297218476e-05,
      "loss": 2.8341,
      "step": 254
    },
    {
      "epoch": 0.16787360105332455,
      "grad_norm": 1.7902761697769165,
      "learning_rate": 1.984587492773149e-05,
      "loss": 2.804,
      "step": 255
    },
    {
      "epoch": 0.1685319289005925,
      "grad_norm": 1.7937899827957153,
      "learning_rate": 1.9844666878781853e-05,
      "loss": 3.053,
      "step": 256
    },
    {
      "epoch": 0.16919025674786042,
      "grad_norm": 1.817107081413269,
      "learning_rate": 1.9843454150943712e-05,
      "loss": 2.9731,
      "step": 257
    },
    {
      "epoch": 0.16984858459512836,
      "grad_norm": 1.6986255645751953,
      "learning_rate": 1.984223674479344e-05,
      "loss": 2.8088,
      "step": 258
    },
    {
      "epoch": 0.17050691244239632,
      "grad_norm": 1.913499116897583,
      "learning_rate": 1.984101466090964e-05,
      "loss": 2.8594,
      "step": 259
    },
    {
      "epoch": 0.17116524028966426,
      "grad_norm": 1.9442312717437744,
      "learning_rate": 1.9839787899873133e-05,
      "loss": 2.9551,
      "step": 260
    },
    {
      "epoch": 0.1718235681369322,
      "grad_norm": 2.1121976375579834,
      "learning_rate": 1.983855646226696e-05,
      "loss": 3.2997,
      "step": 261
    },
    {
      "epoch": 0.17248189598420013,
      "grad_norm": 2.0044729709625244,
      "learning_rate": 1.983732034867639e-05,
      "loss": 3.2596,
      "step": 262
    },
    {
      "epoch": 0.17314022383146807,
      "grad_norm": 1.8241734504699707,
      "learning_rate": 1.983607955968891e-05,
      "loss": 3.2291,
      "step": 263
    },
    {
      "epoch": 0.173798551678736,
      "grad_norm": 1.645841360092163,
      "learning_rate": 1.9834834095894235e-05,
      "loss": 2.9084,
      "step": 264
    },
    {
      "epoch": 0.17445687952600394,
      "grad_norm": 1.2569036483764648,
      "learning_rate": 1.9833583957884298e-05,
      "loss": 2.7413,
      "step": 265
    },
    {
      "epoch": 0.17511520737327188,
      "grad_norm": 1.7455312013626099,
      "learning_rate": 1.983232914625325e-05,
      "loss": 3.0312,
      "step": 266
    },
    {
      "epoch": 0.17577353522053982,
      "grad_norm": 1.478868842124939,
      "learning_rate": 1.983106966159747e-05,
      "loss": 2.7247,
      "step": 267
    },
    {
      "epoch": 0.17643186306780778,
      "grad_norm": 1.5567370653152466,
      "learning_rate": 1.982980550451556e-05,
      "loss": 2.7562,
      "step": 268
    },
    {
      "epoch": 0.17709019091507572,
      "grad_norm": 3.2118542194366455,
      "learning_rate": 1.982853667560833e-05,
      "loss": 3.0352,
      "step": 269
    },
    {
      "epoch": 0.17774851876234365,
      "grad_norm": 1.5324227809906006,
      "learning_rate": 1.982726317547882e-05,
      "loss": 2.7596,
      "step": 270
    },
    {
      "epoch": 0.1784068466096116,
      "grad_norm": 2.849654197692871,
      "learning_rate": 1.9825985004732287e-05,
      "loss": 3.1214,
      "step": 271
    },
    {
      "epoch": 0.17906517445687953,
      "grad_norm": 2.6880054473876953,
      "learning_rate": 1.982470216397621e-05,
      "loss": 2.9515,
      "step": 272
    },
    {
      "epoch": 0.17972350230414746,
      "grad_norm": 2.848451614379883,
      "learning_rate": 1.982341465382029e-05,
      "loss": 3.0063,
      "step": 273
    },
    {
      "epoch": 0.1803818301514154,
      "grad_norm": 2.9858286380767822,
      "learning_rate": 1.9822122474876443e-05,
      "loss": 3.0791,
      "step": 274
    },
    {
      "epoch": 0.18104015799868334,
      "grad_norm": 2.091684579849243,
      "learning_rate": 1.98208256277588e-05,
      "loss": 2.9254,
      "step": 275
    },
    {
      "epoch": 0.18169848584595127,
      "grad_norm": 2.490894079208374,
      "learning_rate": 1.9819524113083718e-05,
      "loss": 3.2702,
      "step": 276
    },
    {
      "epoch": 0.1823568136932192,
      "grad_norm": 1.3432810306549072,
      "learning_rate": 1.9818217931469768e-05,
      "loss": 2.6898,
      "step": 277
    },
    {
      "epoch": 0.18301514154048718,
      "grad_norm": 1.773485541343689,
      "learning_rate": 1.9816907083537743e-05,
      "loss": 2.8889,
      "step": 278
    },
    {
      "epoch": 0.1836734693877551,
      "grad_norm": 1.7290115356445312,
      "learning_rate": 1.9815591569910654e-05,
      "loss": 2.8799,
      "step": 279
    },
    {
      "epoch": 0.18433179723502305,
      "grad_norm": 1.3714981079101562,
      "learning_rate": 1.9814271391213725e-05,
      "loss": 2.7437,
      "step": 280
    },
    {
      "epoch": 0.18499012508229098,
      "grad_norm": 2.7247955799102783,
      "learning_rate": 1.9812946548074394e-05,
      "loss": 3.0225,
      "step": 281
    },
    {
      "epoch": 0.18564845292955892,
      "grad_norm": 1.4606871604919434,
      "learning_rate": 1.981161704112233e-05,
      "loss": 2.8427,
      "step": 282
    },
    {
      "epoch": 0.18630678077682686,
      "grad_norm": 1.3744639158248901,
      "learning_rate": 1.98102828709894e-05,
      "loss": 2.6586,
      "step": 283
    },
    {
      "epoch": 0.1869651086240948,
      "grad_norm": 1.4403542280197144,
      "learning_rate": 1.9808944038309704e-05,
      "loss": 2.8307,
      "step": 284
    },
    {
      "epoch": 0.18762343647136273,
      "grad_norm": 2.6586012840270996,
      "learning_rate": 1.9807600543719546e-05,
      "loss": 3.0704,
      "step": 285
    },
    {
      "epoch": 0.18828176431863067,
      "grad_norm": 1.4103467464447021,
      "learning_rate": 1.9806252387857453e-05,
      "loss": 2.7994,
      "step": 286
    },
    {
      "epoch": 0.1889400921658986,
      "grad_norm": 1.224830985069275,
      "learning_rate": 1.980489957136417e-05,
      "loss": 2.6608,
      "step": 287
    },
    {
      "epoch": 0.18959842001316657,
      "grad_norm": 1.4543243646621704,
      "learning_rate": 1.9803542094882645e-05,
      "loss": 2.6665,
      "step": 288
    },
    {
      "epoch": 0.1902567478604345,
      "grad_norm": 3.31369948387146,
      "learning_rate": 1.9802179959058046e-05,
      "loss": 3.2615,
      "step": 289
    },
    {
      "epoch": 0.19091507570770244,
      "grad_norm": 1.444581389427185,
      "learning_rate": 1.9800813164537764e-05,
      "loss": 2.68,
      "step": 290
    },
    {
      "epoch": 0.19157340355497038,
      "grad_norm": 2.773939847946167,
      "learning_rate": 1.9799441711971393e-05,
      "loss": 3.2036,
      "step": 291
    },
    {
      "epoch": 0.19223173140223832,
      "grad_norm": 1.5869349241256714,
      "learning_rate": 1.979806560201075e-05,
      "loss": 2.8175,
      "step": 292
    },
    {
      "epoch": 0.19289005924950625,
      "grad_norm": 2.3035969734191895,
      "learning_rate": 1.9796684835309855e-05,
      "loss": 3.2647,
      "step": 293
    },
    {
      "epoch": 0.1935483870967742,
      "grad_norm": 2.451908588409424,
      "learning_rate": 1.9795299412524948e-05,
      "loss": 2.81,
      "step": 294
    },
    {
      "epoch": 0.19420671494404212,
      "grad_norm": 1.656747817993164,
      "learning_rate": 1.979390933431448e-05,
      "loss": 2.7756,
      "step": 295
    },
    {
      "epoch": 0.19486504279131006,
      "grad_norm": 3.2026562690734863,
      "learning_rate": 1.979251460133912e-05,
      "loss": 2.9722,
      "step": 296
    },
    {
      "epoch": 0.19552337063857803,
      "grad_norm": 3.316241502761841,
      "learning_rate": 1.979111521426174e-05,
      "loss": 3.2739,
      "step": 297
    },
    {
      "epoch": 0.19618169848584596,
      "grad_norm": 1.3424428701400757,
      "learning_rate": 1.9789711173747435e-05,
      "loss": 2.6614,
      "step": 298
    },
    {
      "epoch": 0.1968400263331139,
      "grad_norm": 1.2824918031692505,
      "learning_rate": 1.97883024804635e-05,
      "loss": 2.6318,
      "step": 299
    },
    {
      "epoch": 0.19749835418038184,
      "grad_norm": 2.2038705348968506,
      "learning_rate": 1.9786889135079443e-05,
      "loss": 3.176,
      "step": 300
    },
    {
      "epoch": 0.19815668202764977,
      "grad_norm": 2.976475954055786,
      "learning_rate": 1.978547113826699e-05,
      "loss": 2.8907,
      "step": 301
    },
    {
      "epoch": 0.1988150098749177,
      "grad_norm": 1.929645299911499,
      "learning_rate": 1.978404849070008e-05,
      "loss": 2.7926,
      "step": 302
    },
    {
      "epoch": 0.19947333772218565,
      "grad_norm": 3.117734670639038,
      "learning_rate": 1.978262119305485e-05,
      "loss": 3.2086,
      "step": 303
    },
    {
      "epoch": 0.20013166556945358,
      "grad_norm": 2.996832847595215,
      "learning_rate": 1.9781189246009657e-05,
      "loss": 3.1544,
      "step": 304
    },
    {
      "epoch": 0.20078999341672152,
      "grad_norm": 1.7162845134735107,
      "learning_rate": 1.9779752650245064e-05,
      "loss": 2.6846,
      "step": 305
    },
    {
      "epoch": 0.20144832126398945,
      "grad_norm": 1.5888218879699707,
      "learning_rate": 1.977831140644384e-05,
      "loss": 2.6721,
      "step": 306
    },
    {
      "epoch": 0.20210664911125742,
      "grad_norm": 1.9520171880722046,
      "learning_rate": 1.9776865515290973e-05,
      "loss": 2.7952,
      "step": 307
    },
    {
      "epoch": 0.20276497695852536,
      "grad_norm": 1.9875901937484741,
      "learning_rate": 1.9775414977473648e-05,
      "loss": 2.6696,
      "step": 308
    },
    {
      "epoch": 0.2034233048057933,
      "grad_norm": 2.7956669330596924,
      "learning_rate": 1.977395979368127e-05,
      "loss": 3.2136,
      "step": 309
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 2.05853271484375,
      "learning_rate": 1.977249996460544e-05,
      "loss": 2.6837,
      "step": 310
    },
    {
      "epoch": 0.20473996050032917,
      "grad_norm": 1.8104143142700195,
      "learning_rate": 1.977103549093998e-05,
      "loss": 2.831,
      "step": 311
    },
    {
      "epoch": 0.2053982883475971,
      "grad_norm": 1.4895117282867432,
      "learning_rate": 1.9769566373380907e-05,
      "loss": 2.5998,
      "step": 312
    },
    {
      "epoch": 0.20605661619486504,
      "grad_norm": 1.4531326293945312,
      "learning_rate": 1.976809261262645e-05,
      "loss": 2.7268,
      "step": 313
    },
    {
      "epoch": 0.20671494404213298,
      "grad_norm": 1.4459314346313477,
      "learning_rate": 1.9766614209377047e-05,
      "loss": 2.6258,
      "step": 314
    },
    {
      "epoch": 0.2073732718894009,
      "grad_norm": 1.5482181310653687,
      "learning_rate": 1.9765131164335345e-05,
      "loss": 2.756,
      "step": 315
    },
    {
      "epoch": 0.20803159973666885,
      "grad_norm": 2.1149773597717285,
      "learning_rate": 1.976364347820619e-05,
      "loss": 2.7616,
      "step": 316
    },
    {
      "epoch": 0.2086899275839368,
      "grad_norm": 1.8698488473892212,
      "learning_rate": 1.9762151151696633e-05,
      "loss": 2.7245,
      "step": 317
    },
    {
      "epoch": 0.20934825543120475,
      "grad_norm": 1.8071258068084717,
      "learning_rate": 1.9760654185515933e-05,
      "loss": 2.7552,
      "step": 318
    },
    {
      "epoch": 0.21000658327847269,
      "grad_norm": 3.092121124267578,
      "learning_rate": 1.9759152580375565e-05,
      "loss": 3.1062,
      "step": 319
    },
    {
      "epoch": 0.21066491112574062,
      "grad_norm": 3.405305862426758,
      "learning_rate": 1.9757646336989186e-05,
      "loss": 2.9366,
      "step": 320
    },
    {
      "epoch": 0.21132323897300856,
      "grad_norm": 3.3745017051696777,
      "learning_rate": 1.9756135456072684e-05,
      "loss": 3.1693,
      "step": 321
    },
    {
      "epoch": 0.2119815668202765,
      "grad_norm": 1.5363982915878296,
      "learning_rate": 1.975461993834413e-05,
      "loss": 2.6217,
      "step": 322
    },
    {
      "epoch": 0.21263989466754443,
      "grad_norm": 1.8189693689346313,
      "learning_rate": 1.9753099784523803e-05,
      "loss": 2.6107,
      "step": 323
    },
    {
      "epoch": 0.21329822251481237,
      "grad_norm": 1.6527682542800903,
      "learning_rate": 1.9751574995334198e-05,
      "loss": 2.7176,
      "step": 324
    },
    {
      "epoch": 0.2139565503620803,
      "grad_norm": 3.5018796920776367,
      "learning_rate": 1.9750045571499993e-05,
      "loss": 3.2613,
      "step": 325
    },
    {
      "epoch": 0.21461487820934824,
      "grad_norm": 1.3125276565551758,
      "learning_rate": 1.974851151374809e-05,
      "loss": 2.6974,
      "step": 326
    },
    {
      "epoch": 0.2152732060566162,
      "grad_norm": 1.7678117752075195,
      "learning_rate": 1.9746972822807574e-05,
      "loss": 2.7123,
      "step": 327
    },
    {
      "epoch": 0.21593153390388414,
      "grad_norm": 2.8678712844848633,
      "learning_rate": 1.9745429499409747e-05,
      "loss": 3.0928,
      "step": 328
    },
    {
      "epoch": 0.21658986175115208,
      "grad_norm": 1.6343375444412231,
      "learning_rate": 1.97438815442881e-05,
      "loss": 2.59,
      "step": 329
    },
    {
      "epoch": 0.21724818959842002,
      "grad_norm": 3.2938685417175293,
      "learning_rate": 1.9742328958178336e-05,
      "loss": 3.1847,
      "step": 330
    },
    {
      "epoch": 0.21790651744568795,
      "grad_norm": 3.6565184593200684,
      "learning_rate": 1.9740771741818356e-05,
      "loss": 2.8936,
      "step": 331
    },
    {
      "epoch": 0.2185648452929559,
      "grad_norm": 3.935868978500366,
      "learning_rate": 1.973920989594826e-05,
      "loss": 2.9117,
      "step": 332
    },
    {
      "epoch": 0.21922317314022383,
      "grad_norm": 4.003252983093262,
      "learning_rate": 1.9737643421310346e-05,
      "loss": 3.22,
      "step": 333
    },
    {
      "epoch": 0.21988150098749176,
      "grad_norm": 3.419938564300537,
      "learning_rate": 1.9736072318649115e-05,
      "loss": 3.0822,
      "step": 334
    },
    {
      "epoch": 0.2205398288347597,
      "grad_norm": 1.8106396198272705,
      "learning_rate": 1.9734496588711268e-05,
      "loss": 2.6299,
      "step": 335
    },
    {
      "epoch": 0.22119815668202766,
      "grad_norm": 5.059349060058594,
      "learning_rate": 1.9732916232245703e-05,
      "loss": 2.7717,
      "step": 336
    },
    {
      "epoch": 0.2218564845292956,
      "grad_norm": 2.3793206214904785,
      "learning_rate": 1.973133125000352e-05,
      "loss": 2.7224,
      "step": 337
    },
    {
      "epoch": 0.22251481237656354,
      "grad_norm": 1.401849389076233,
      "learning_rate": 1.9729741642738017e-05,
      "loss": 2.6587,
      "step": 338
    },
    {
      "epoch": 0.22317314022383147,
      "grad_norm": 9.446893692016602,
      "learning_rate": 1.9728147411204687e-05,
      "loss": 3.0096,
      "step": 339
    },
    {
      "epoch": 0.2238314680710994,
      "grad_norm": 1.9510033130645752,
      "learning_rate": 1.972654855616122e-05,
      "loss": 2.6108,
      "step": 340
    },
    {
      "epoch": 0.22448979591836735,
      "grad_norm": 2.379551410675049,
      "learning_rate": 1.9724945078367513e-05,
      "loss": 2.6671,
      "step": 341
    },
    {
      "epoch": 0.22514812376563528,
      "grad_norm": 1.413527488708496,
      "learning_rate": 1.9723336978585648e-05,
      "loss": 2.6507,
      "step": 342
    },
    {
      "epoch": 0.22580645161290322,
      "grad_norm": 3.9692492485046387,
      "learning_rate": 1.9721724257579907e-05,
      "loss": 3.017,
      "step": 343
    },
    {
      "epoch": 0.22646477946017116,
      "grad_norm": 3.6127676963806152,
      "learning_rate": 1.9720106916116776e-05,
      "loss": 3.0447,
      "step": 344
    },
    {
      "epoch": 0.2271231073074391,
      "grad_norm": 3.3369762897491455,
      "learning_rate": 1.9718484954964925e-05,
      "loss": 3.2041,
      "step": 345
    },
    {
      "epoch": 0.22778143515470706,
      "grad_norm": 3.9670374393463135,
      "learning_rate": 1.971685837489523e-05,
      "loss": 2.7362,
      "step": 346
    },
    {
      "epoch": 0.228439763001975,
      "grad_norm": 1.6107233762741089,
      "learning_rate": 1.971522717668076e-05,
      "loss": 2.7382,
      "step": 347
    },
    {
      "epoch": 0.22909809084924293,
      "grad_norm": 1.7240175008773804,
      "learning_rate": 1.9713591361096772e-05,
      "loss": 2.5906,
      "step": 348
    },
    {
      "epoch": 0.22975641869651087,
      "grad_norm": 1.8823719024658203,
      "learning_rate": 1.971195092892072e-05,
      "loss": 2.695,
      "step": 349
    },
    {
      "epoch": 0.2304147465437788,
      "grad_norm": 3.5420749187469482,
      "learning_rate": 1.971030588093226e-05,
      "loss": 2.8447,
      "step": 350
    },
    {
      "epoch": 0.23107307439104674,
      "grad_norm": 2.4758970737457275,
      "learning_rate": 1.970865621791324e-05,
      "loss": 2.6581,
      "step": 351
    },
    {
      "epoch": 0.23173140223831468,
      "grad_norm": 5.697980880737305,
      "learning_rate": 1.9707001940647688e-05,
      "loss": 2.7337,
      "step": 352
    },
    {
      "epoch": 0.2323897300855826,
      "grad_norm": 4.297050952911377,
      "learning_rate": 1.970534304992184e-05,
      "loss": 3.027,
      "step": 353
    },
    {
      "epoch": 0.23304805793285055,
      "grad_norm": 3.735689878463745,
      "learning_rate": 1.9703679546524114e-05,
      "loss": 3.1477,
      "step": 354
    },
    {
      "epoch": 0.2337063857801185,
      "grad_norm": 4.207089900970459,
      "learning_rate": 1.9702011431245136e-05,
      "loss": 2.9294,
      "step": 355
    },
    {
      "epoch": 0.23436471362738645,
      "grad_norm": 3.4478044509887695,
      "learning_rate": 1.9700338704877704e-05,
      "loss": 2.825,
      "step": 356
    },
    {
      "epoch": 0.2350230414746544,
      "grad_norm": 3.4185431003570557,
      "learning_rate": 1.9698661368216818e-05,
      "loss": 2.6731,
      "step": 357
    },
    {
      "epoch": 0.23568136932192232,
      "grad_norm": 1.8587210178375244,
      "learning_rate": 1.9696979422059676e-05,
      "loss": 2.5922,
      "step": 358
    },
    {
      "epoch": 0.23633969716919026,
      "grad_norm": 1.7071951627731323,
      "learning_rate": 1.969529286720565e-05,
      "loss": 2.5644,
      "step": 359
    },
    {
      "epoch": 0.2369980250164582,
      "grad_norm": 2.2252697944641113,
      "learning_rate": 1.9693601704456316e-05,
      "loss": 2.7906,
      "step": 360
    },
    {
      "epoch": 0.23765635286372613,
      "grad_norm": 1.6598243713378906,
      "learning_rate": 1.969190593461543e-05,
      "loss": 2.6652,
      "step": 361
    },
    {
      "epoch": 0.23831468071099407,
      "grad_norm": 3.6497035026550293,
      "learning_rate": 1.969020555848895e-05,
      "loss": 2.9899,
      "step": 362
    },
    {
      "epoch": 0.238973008558262,
      "grad_norm": 1.7276849746704102,
      "learning_rate": 1.9688500576885012e-05,
      "loss": 2.681,
      "step": 363
    },
    {
      "epoch": 0.23963133640552994,
      "grad_norm": 1.4279242753982544,
      "learning_rate": 1.9686790990613948e-05,
      "loss": 2.5458,
      "step": 364
    },
    {
      "epoch": 0.24028966425279788,
      "grad_norm": 1.9591439962387085,
      "learning_rate": 1.968507680048827e-05,
      "loss": 2.7349,
      "step": 365
    },
    {
      "epoch": 0.24094799210006584,
      "grad_norm": 2.6819982528686523,
      "learning_rate": 1.968335800732269e-05,
      "loss": 2.6805,
      "step": 366
    },
    {
      "epoch": 0.24160631994733378,
      "grad_norm": 4.210615634918213,
      "learning_rate": 1.9681634611934095e-05,
      "loss": 2.8854,
      "step": 367
    },
    {
      "epoch": 0.24226464779460172,
      "grad_norm": 1.6788020133972168,
      "learning_rate": 1.9679906615141568e-05,
      "loss": 2.5238,
      "step": 368
    },
    {
      "epoch": 0.24292297564186965,
      "grad_norm": 3.368138551712036,
      "learning_rate": 1.967817401776638e-05,
      "loss": 2.9707,
      "step": 369
    },
    {
      "epoch": 0.2435813034891376,
      "grad_norm": 2.3193230628967285,
      "learning_rate": 1.967643682063198e-05,
      "loss": 2.611,
      "step": 370
    },
    {
      "epoch": 0.24423963133640553,
      "grad_norm": 2.8985636234283447,
      "learning_rate": 1.967469502456401e-05,
      "loss": 2.6256,
      "step": 371
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 2.377729654312134,
      "learning_rate": 1.9672948630390296e-05,
      "loss": 2.6391,
      "step": 372
    },
    {
      "epoch": 0.2455562870309414,
      "grad_norm": 1.6995549201965332,
      "learning_rate": 1.967119763894085e-05,
      "loss": 2.6645,
      "step": 373
    },
    {
      "epoch": 0.24621461487820934,
      "grad_norm": 3.8403522968292236,
      "learning_rate": 1.9669442051047864e-05,
      "loss": 2.7429,
      "step": 374
    },
    {
      "epoch": 0.2468729427254773,
      "grad_norm": 2.759704113006592,
      "learning_rate": 1.9667681867545723e-05,
      "loss": 2.7787,
      "step": 375
    },
    {
      "epoch": 0.24753127057274524,
      "grad_norm": 1.864598274230957,
      "learning_rate": 1.9665917089270995e-05,
      "loss": 2.5275,
      "step": 376
    },
    {
      "epoch": 0.24818959842001317,
      "grad_norm": 1.652747392654419,
      "learning_rate": 1.9664147717062422e-05,
      "loss": 2.5114,
      "step": 377
    },
    {
      "epoch": 0.2488479262672811,
      "grad_norm": 5.6726555824279785,
      "learning_rate": 1.9662373751760934e-05,
      "loss": 3.4232,
      "step": 378
    },
    {
      "epoch": 0.24950625411454905,
      "grad_norm": 3.386847496032715,
      "learning_rate": 1.9660595194209655e-05,
      "loss": 2.9916,
      "step": 379
    },
    {
      "epoch": 0.250164581961817,
      "grad_norm": 4.1844964027404785,
      "learning_rate": 1.965881204525388e-05,
      "loss": 2.7144,
      "step": 380
    },
    {
      "epoch": 0.25082290980908495,
      "grad_norm": 4.686529636383057,
      "learning_rate": 1.965702430574108e-05,
      "loss": 3.0215,
      "step": 381
    },
    {
      "epoch": 0.2514812376563529,
      "grad_norm": 3.8674845695495605,
      "learning_rate": 1.9655231976520926e-05,
      "loss": 2.8284,
      "step": 382
    },
    {
      "epoch": 0.2521395655036208,
      "grad_norm": 3.9643964767456055,
      "learning_rate": 1.965343505844526e-05,
      "loss": 2.9299,
      "step": 383
    },
    {
      "epoch": 0.25279789335088876,
      "grad_norm": 4.547634601593018,
      "learning_rate": 1.9651633552368098e-05,
      "loss": 3.0529,
      "step": 384
    },
    {
      "epoch": 0.2534562211981567,
      "grad_norm": 1.9808528423309326,
      "learning_rate": 1.9649827459145654e-05,
      "loss": 2.5411,
      "step": 385
    },
    {
      "epoch": 0.25411454904542463,
      "grad_norm": 2.2210099697113037,
      "learning_rate": 1.9648016779636307e-05,
      "loss": 2.5016,
      "step": 386
    },
    {
      "epoch": 0.25477287689269257,
      "grad_norm": 3.3054301738739014,
      "learning_rate": 1.9646201514700622e-05,
      "loss": 2.7668,
      "step": 387
    },
    {
      "epoch": 0.2554312047399605,
      "grad_norm": 4.91308069229126,
      "learning_rate": 1.9644381665201342e-05,
      "loss": 2.9534,
      "step": 388
    },
    {
      "epoch": 0.25608953258722844,
      "grad_norm": 1.8647180795669556,
      "learning_rate": 1.9642557232003395e-05,
      "loss": 2.6474,
      "step": 389
    },
    {
      "epoch": 0.2567478604344964,
      "grad_norm": 4.141273021697998,
      "learning_rate": 1.9640728215973872e-05,
      "loss": 2.6367,
      "step": 390
    },
    {
      "epoch": 0.2574061882817643,
      "grad_norm": 4.0680341720581055,
      "learning_rate": 1.9638894617982058e-05,
      "loss": 2.8623,
      "step": 391
    },
    {
      "epoch": 0.25806451612903225,
      "grad_norm": 1.7959867715835571,
      "learning_rate": 1.963705643889941e-05,
      "loss": 2.4892,
      "step": 392
    },
    {
      "epoch": 0.2587228439763002,
      "grad_norm": 3.2926599979400635,
      "learning_rate": 1.9635213679599562e-05,
      "loss": 2.9126,
      "step": 393
    },
    {
      "epoch": 0.2593811718235681,
      "grad_norm": 3.3991963863372803,
      "learning_rate": 1.9633366340958318e-05,
      "loss": 2.7973,
      "step": 394
    },
    {
      "epoch": 0.26003949967083606,
      "grad_norm": 5.236316204071045,
      "learning_rate": 1.9631514423853676e-05,
      "loss": 3.1557,
      "step": 395
    },
    {
      "epoch": 0.260697827518104,
      "grad_norm": 7.271364212036133,
      "learning_rate": 1.9629657929165794e-05,
      "loss": 2.7448,
      "step": 396
    },
    {
      "epoch": 0.26135615536537193,
      "grad_norm": 2.3482284545898438,
      "learning_rate": 1.962779685777701e-05,
      "loss": 2.7191,
      "step": 397
    },
    {
      "epoch": 0.26201448321263987,
      "grad_norm": 2.0241215229034424,
      "learning_rate": 1.9625931210571834e-05,
      "loss": 2.5,
      "step": 398
    },
    {
      "epoch": 0.2626728110599078,
      "grad_norm": 1.999038815498352,
      "learning_rate": 1.9624060988436968e-05,
      "loss": 2.6301,
      "step": 399
    },
    {
      "epoch": 0.2633311389071758,
      "grad_norm": 1.4373767375946045,
      "learning_rate": 1.9622186192261258e-05,
      "loss": 2.4641,
      "step": 400
    },
    {
      "epoch": 0.26398946675444374,
      "grad_norm": 4.878281593322754,
      "learning_rate": 1.9620306822935757e-05,
      "loss": 2.8252,
      "step": 401
    },
    {
      "epoch": 0.2646477946017117,
      "grad_norm": 4.351806640625,
      "learning_rate": 1.9618422881353664e-05,
      "loss": 3.1315,
      "step": 402
    },
    {
      "epoch": 0.2653061224489796,
      "grad_norm": 2.9609768390655518,
      "learning_rate": 1.9616534368410364e-05,
      "loss": 2.7576,
      "step": 403
    },
    {
      "epoch": 0.26596445029624755,
      "grad_norm": 4.378626823425293,
      "learning_rate": 1.961464128500342e-05,
      "loss": 2.6997,
      "step": 404
    },
    {
      "epoch": 0.2666227781435155,
      "grad_norm": 4.206915855407715,
      "learning_rate": 1.961274363203255e-05,
      "loss": 2.8139,
      "step": 405
    },
    {
      "epoch": 0.2672811059907834,
      "grad_norm": 2.3368613719940186,
      "learning_rate": 1.9610841410399668e-05,
      "loss": 2.511,
      "step": 406
    },
    {
      "epoch": 0.26793943383805136,
      "grad_norm": 6.67822265625,
      "learning_rate": 1.9608934621008833e-05,
      "loss": 2.9362,
      "step": 407
    },
    {
      "epoch": 0.2685977616853193,
      "grad_norm": 3.66219162940979,
      "learning_rate": 1.9607023264766294e-05,
      "loss": 2.7071,
      "step": 408
    },
    {
      "epoch": 0.26925608953258723,
      "grad_norm": 2.7651000022888184,
      "learning_rate": 1.9605107342580462e-05,
      "loss": 2.5135,
      "step": 409
    },
    {
      "epoch": 0.26991441737985516,
      "grad_norm": 2.649740695953369,
      "learning_rate": 1.960318685536192e-05,
      "loss": 2.706,
      "step": 410
    },
    {
      "epoch": 0.2705727452271231,
      "grad_norm": 1.3944814205169678,
      "learning_rate": 1.960126180402342e-05,
      "loss": 2.4347,
      "step": 411
    },
    {
      "epoch": 0.27123107307439104,
      "grad_norm": 5.30812931060791,
      "learning_rate": 1.9599332189479886e-05,
      "loss": 2.6592,
      "step": 412
    },
    {
      "epoch": 0.271889400921659,
      "grad_norm": 3.615025281906128,
      "learning_rate": 1.959739801264841e-05,
      "loss": 2.6932,
      "step": 413
    },
    {
      "epoch": 0.2725477287689269,
      "grad_norm": 4.890690326690674,
      "learning_rate": 1.959545927444824e-05,
      "loss": 2.6545,
      "step": 414
    },
    {
      "epoch": 0.27320605661619485,
      "grad_norm": 4.678014278411865,
      "learning_rate": 1.9593515975800813e-05,
      "loss": 2.6366,
      "step": 415
    },
    {
      "epoch": 0.2738643844634628,
      "grad_norm": 3.4501750469207764,
      "learning_rate": 1.959156811762973e-05,
      "loss": 2.8616,
      "step": 416
    },
    {
      "epoch": 0.2745227123107307,
      "grad_norm": 5.473855972290039,
      "learning_rate": 1.958961570086073e-05,
      "loss": 2.7225,
      "step": 417
    },
    {
      "epoch": 0.27518104015799866,
      "grad_norm": 3.00425386428833,
      "learning_rate": 1.9587658726421765e-05,
      "loss": 2.8162,
      "step": 418
    },
    {
      "epoch": 0.27583936800526665,
      "grad_norm": 3.2126882076263428,
      "learning_rate": 1.9585697195242913e-05,
      "loss": 2.5896,
      "step": 419
    },
    {
      "epoch": 0.2764976958525346,
      "grad_norm": 3.036923408508301,
      "learning_rate": 1.958373110825644e-05,
      "loss": 2.7677,
      "step": 420
    },
    {
      "epoch": 0.2771560236998025,
      "grad_norm": 3.88696551322937,
      "learning_rate": 1.9581760466396768e-05,
      "loss": 2.9239,
      "step": 421
    },
    {
      "epoch": 0.27781435154707046,
      "grad_norm": 2.5312209129333496,
      "learning_rate": 1.957978527060049e-05,
      "loss": 2.4844,
      "step": 422
    },
    {
      "epoch": 0.2784726793943384,
      "grad_norm": 3.7664237022399902,
      "learning_rate": 1.9577805521806358e-05,
      "loss": 2.7364,
      "step": 423
    },
    {
      "epoch": 0.27913100724160633,
      "grad_norm": 1.2214285135269165,
      "learning_rate": 1.9575821220955294e-05,
      "loss": 2.395,
      "step": 424
    },
    {
      "epoch": 0.27978933508887427,
      "grad_norm": 1.7204995155334473,
      "learning_rate": 1.957383236899037e-05,
      "loss": 2.435,
      "step": 425
    },
    {
      "epoch": 0.2804476629361422,
      "grad_norm": 3.8269152641296387,
      "learning_rate": 1.957183896685684e-05,
      "loss": 2.9428,
      "step": 426
    },
    {
      "epoch": 0.28110599078341014,
      "grad_norm": 4.7457356452941895,
      "learning_rate": 1.9569841015502105e-05,
      "loss": 3.0352,
      "step": 427
    },
    {
      "epoch": 0.2817643186306781,
      "grad_norm": 4.291551113128662,
      "learning_rate": 1.9567838515875736e-05,
      "loss": 2.9061,
      "step": 428
    },
    {
      "epoch": 0.282422646477946,
      "grad_norm": 1.8207719326019287,
      "learning_rate": 1.9565831468929467e-05,
      "loss": 2.4116,
      "step": 429
    },
    {
      "epoch": 0.28308097432521395,
      "grad_norm": 5.056065559387207,
      "learning_rate": 1.9563819875617188e-05,
      "loss": 2.7669,
      "step": 430
    },
    {
      "epoch": 0.2837393021724819,
      "grad_norm": 2.2634541988372803,
      "learning_rate": 1.9561803736894945e-05,
      "loss": 2.7587,
      "step": 431
    },
    {
      "epoch": 0.2843976300197498,
      "grad_norm": 1.348231315612793,
      "learning_rate": 1.9559783053720963e-05,
      "loss": 2.3785,
      "step": 432
    },
    {
      "epoch": 0.28505595786701776,
      "grad_norm": 1.7288810014724731,
      "learning_rate": 1.9557757827055604e-05,
      "loss": 2.4361,
      "step": 433
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 4.4860310554504395,
      "learning_rate": 1.955572805786141e-05,
      "loss": 2.9527,
      "step": 434
    },
    {
      "epoch": 0.28637261356155364,
      "grad_norm": 4.800720691680908,
      "learning_rate": 1.9553693747103063e-05,
      "loss": 2.9028,
      "step": 435
    },
    {
      "epoch": 0.28703094140882157,
      "grad_norm": 7.1803789138793945,
      "learning_rate": 1.9551654895747417e-05,
      "loss": 3.3798,
      "step": 436
    },
    {
      "epoch": 0.2876892692560895,
      "grad_norm": 2.4980838298797607,
      "learning_rate": 1.9549611504763484e-05,
      "loss": 2.6863,
      "step": 437
    },
    {
      "epoch": 0.28834759710335744,
      "grad_norm": 2.4048666954040527,
      "learning_rate": 1.9547563575122423e-05,
      "loss": 2.5399,
      "step": 438
    },
    {
      "epoch": 0.28900592495062544,
      "grad_norm": 3.0057644844055176,
      "learning_rate": 1.9545511107797558e-05,
      "loss": 2.5421,
      "step": 439
    },
    {
      "epoch": 0.2896642527978934,
      "grad_norm": 4.894601821899414,
      "learning_rate": 1.954345410376437e-05,
      "loss": 2.7315,
      "step": 440
    },
    {
      "epoch": 0.2903225806451613,
      "grad_norm": 2.210334300994873,
      "learning_rate": 1.954139256400049e-05,
      "loss": 2.6684,
      "step": 441
    },
    {
      "epoch": 0.29098090849242925,
      "grad_norm": 2.783161163330078,
      "learning_rate": 1.9539326489485713e-05,
      "loss": 2.7059,
      "step": 442
    },
    {
      "epoch": 0.2916392363396972,
      "grad_norm": 2.06597638130188,
      "learning_rate": 1.9537255881201986e-05,
      "loss": 2.6396,
      "step": 443
    },
    {
      "epoch": 0.2922975641869651,
      "grad_norm": 7.055760860443115,
      "learning_rate": 1.9535180740133402e-05,
      "loss": 3.0471,
      "step": 444
    },
    {
      "epoch": 0.29295589203423306,
      "grad_norm": 2.159224510192871,
      "learning_rate": 1.9533101067266224e-05,
      "loss": 2.4057,
      "step": 445
    },
    {
      "epoch": 0.293614219881501,
      "grad_norm": 3.463954210281372,
      "learning_rate": 1.9531016863588857e-05,
      "loss": 2.7656,
      "step": 446
    },
    {
      "epoch": 0.29427254772876893,
      "grad_norm": 3.0934760570526123,
      "learning_rate": 1.9528928130091865e-05,
      "loss": 2.734,
      "step": 447
    },
    {
      "epoch": 0.29493087557603687,
      "grad_norm": 2.7830913066864014,
      "learning_rate": 1.952683486776797e-05,
      "loss": 2.7606,
      "step": 448
    },
    {
      "epoch": 0.2955892034233048,
      "grad_norm": 3.171853542327881,
      "learning_rate": 1.9524737077612027e-05,
      "loss": 2.7401,
      "step": 449
    },
    {
      "epoch": 0.29624753127057274,
      "grad_norm": 5.899575710296631,
      "learning_rate": 1.9522634760621063e-05,
      "loss": 2.9465,
      "step": 450
    },
    {
      "epoch": 0.2969058591178407,
      "grad_norm": 1.9818476438522339,
      "learning_rate": 1.9520527917794247e-05,
      "loss": 2.4014,
      "step": 451
    },
    {
      "epoch": 0.2975641869651086,
      "grad_norm": 5.611133575439453,
      "learning_rate": 1.95184165501329e-05,
      "loss": 3.0433,
      "step": 452
    },
    {
      "epoch": 0.29822251481237655,
      "grad_norm": 3.737417697906494,
      "learning_rate": 1.95163006586405e-05,
      "loss": 2.725,
      "step": 453
    },
    {
      "epoch": 0.2988808426596445,
      "grad_norm": 1.7786228656768799,
      "learning_rate": 1.9514180244322665e-05,
      "loss": 2.4304,
      "step": 454
    },
    {
      "epoch": 0.2995391705069124,
      "grad_norm": 4.2436676025390625,
      "learning_rate": 1.9512055308187167e-05,
      "loss": 2.5089,
      "step": 455
    },
    {
      "epoch": 0.30019749835418036,
      "grad_norm": 3.864055633544922,
      "learning_rate": 1.950992585124393e-05,
      "loss": 2.6011,
      "step": 456
    },
    {
      "epoch": 0.3008558262014483,
      "grad_norm": 4.270410060882568,
      "learning_rate": 1.9507791874505023e-05,
      "loss": 2.7625,
      "step": 457
    },
    {
      "epoch": 0.3015141540487163,
      "grad_norm": 3.369089365005493,
      "learning_rate": 1.9505653378984665e-05,
      "loss": 2.6325,
      "step": 458
    },
    {
      "epoch": 0.3021724818959842,
      "grad_norm": 2.092414140701294,
      "learning_rate": 1.950351036569922e-05,
      "loss": 2.4247,
      "step": 459
    },
    {
      "epoch": 0.30283080974325216,
      "grad_norm": 2.1840667724609375,
      "learning_rate": 1.9501362835667197e-05,
      "loss": 2.4048,
      "step": 460
    },
    {
      "epoch": 0.3034891375905201,
      "grad_norm": 3.900118589401245,
      "learning_rate": 1.9499210789909264e-05,
      "loss": 2.5976,
      "step": 461
    },
    {
      "epoch": 0.30414746543778803,
      "grad_norm": 3.389120101928711,
      "learning_rate": 1.9497054229448223e-05,
      "loss": 2.7373,
      "step": 462
    },
    {
      "epoch": 0.30480579328505597,
      "grad_norm": 1.926769495010376,
      "learning_rate": 1.9494893155309023e-05,
      "loss": 2.3834,
      "step": 463
    },
    {
      "epoch": 0.3054641211323239,
      "grad_norm": 1.90253746509552,
      "learning_rate": 1.9492727568518764e-05,
      "loss": 2.3793,
      "step": 464
    },
    {
      "epoch": 0.30612244897959184,
      "grad_norm": 3.8679847717285156,
      "learning_rate": 1.949055747010669e-05,
      "loss": 2.6245,
      "step": 465
    },
    {
      "epoch": 0.3067807768268598,
      "grad_norm": 5.369344711303711,
      "learning_rate": 1.948838286110418e-05,
      "loss": 2.7638,
      "step": 466
    },
    {
      "epoch": 0.3074391046741277,
      "grad_norm": 5.120275974273682,
      "learning_rate": 1.9486203742544763e-05,
      "loss": 2.8025,
      "step": 467
    },
    {
      "epoch": 0.30809743252139565,
      "grad_norm": 2.803297996520996,
      "learning_rate": 1.9484020115464123e-05,
      "loss": 2.6462,
      "step": 468
    },
    {
      "epoch": 0.3087557603686636,
      "grad_norm": 5.920924663543701,
      "learning_rate": 1.9481831980900062e-05,
      "loss": 3.0627,
      "step": 469
    },
    {
      "epoch": 0.3094140882159315,
      "grad_norm": 3.139413595199585,
      "learning_rate": 1.9479639339892545e-05,
      "loss": 2.673,
      "step": 470
    },
    {
      "epoch": 0.31007241606319946,
      "grad_norm": 1.2189431190490723,
      "learning_rate": 1.947744219348367e-05,
      "loss": 2.3398,
      "step": 471
    },
    {
      "epoch": 0.3107307439104674,
      "grad_norm": 4.805859088897705,
      "learning_rate": 1.9475240542717678e-05,
      "loss": 3.0414,
      "step": 472
    },
    {
      "epoch": 0.31138907175773534,
      "grad_norm": 2.653714656829834,
      "learning_rate": 1.9473034388640954e-05,
      "loss": 2.4989,
      "step": 473
    },
    {
      "epoch": 0.3120473996050033,
      "grad_norm": 2.697383165359497,
      "learning_rate": 1.9470823732302012e-05,
      "loss": 2.6624,
      "step": 474
    },
    {
      "epoch": 0.3127057274522712,
      "grad_norm": 1.8958102464675903,
      "learning_rate": 1.9468608574751517e-05,
      "loss": 2.403,
      "step": 475
    },
    {
      "epoch": 0.31336405529953915,
      "grad_norm": 1.6052722930908203,
      "learning_rate": 1.9466388917042275e-05,
      "loss": 2.3541,
      "step": 476
    },
    {
      "epoch": 0.3140223831468071,
      "grad_norm": 4.813432216644287,
      "learning_rate": 1.9464164760229218e-05,
      "loss": 2.8798,
      "step": 477
    },
    {
      "epoch": 0.3146807109940751,
      "grad_norm": 2.028597593307495,
      "learning_rate": 1.946193610536943e-05,
      "loss": 2.6187,
      "step": 478
    },
    {
      "epoch": 0.315339038841343,
      "grad_norm": 1.713054895401001,
      "learning_rate": 1.9459702953522127e-05,
      "loss": 2.4242,
      "step": 479
    },
    {
      "epoch": 0.31599736668861095,
      "grad_norm": 4.9309515953063965,
      "learning_rate": 1.9457465305748657e-05,
      "loss": 2.8511,
      "step": 480
    },
    {
      "epoch": 0.3166556945358789,
      "grad_norm": 1.7015801668167114,
      "learning_rate": 1.9455223163112514e-05,
      "loss": 2.3613,
      "step": 481
    },
    {
      "epoch": 0.3173140223831468,
      "grad_norm": 1.4767192602157593,
      "learning_rate": 1.9452976526679323e-05,
      "loss": 2.3599,
      "step": 482
    },
    {
      "epoch": 0.31797235023041476,
      "grad_norm": 1.6658892631530762,
      "learning_rate": 1.945072539751685e-05,
      "loss": 2.3881,
      "step": 483
    },
    {
      "epoch": 0.3186306780776827,
      "grad_norm": 2.959716320037842,
      "learning_rate": 1.9448469776694986e-05,
      "loss": 2.765,
      "step": 484
    },
    {
      "epoch": 0.31928900592495063,
      "grad_norm": 2.627398729324341,
      "learning_rate": 1.9446209665285764e-05,
      "loss": 2.681,
      "step": 485
    },
    {
      "epoch": 0.31994733377221857,
      "grad_norm": 4.99540901184082,
      "learning_rate": 1.9443945064363352e-05,
      "loss": 2.7416,
      "step": 486
    },
    {
      "epoch": 0.3206056616194865,
      "grad_norm": 2.2486798763275146,
      "learning_rate": 1.944167597500405e-05,
      "loss": 2.4401,
      "step": 487
    },
    {
      "epoch": 0.32126398946675444,
      "grad_norm": 2.194077968597412,
      "learning_rate": 1.9439402398286294e-05,
      "loss": 2.6116,
      "step": 488
    },
    {
      "epoch": 0.3219223173140224,
      "grad_norm": 1.4760123491287231,
      "learning_rate": 1.9437124335290646e-05,
      "loss": 2.3526,
      "step": 489
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 3.0264739990234375,
      "learning_rate": 1.9434841787099804e-05,
      "loss": 2.6488,
      "step": 490
    },
    {
      "epoch": 0.32323897300855825,
      "grad_norm": 1.5772356986999512,
      "learning_rate": 1.94325547547986e-05,
      "loss": 2.326,
      "step": 491
    },
    {
      "epoch": 0.3238973008558262,
      "grad_norm": 4.8926472663879395,
      "learning_rate": 1.9430263239473993e-05,
      "loss": 2.8248,
      "step": 492
    },
    {
      "epoch": 0.3245556287030941,
      "grad_norm": 2.448193311691284,
      "learning_rate": 1.942796724221508e-05,
      "loss": 2.4282,
      "step": 493
    },
    {
      "epoch": 0.32521395655036206,
      "grad_norm": 2.041302442550659,
      "learning_rate": 1.9425666764113073e-05,
      "loss": 2.3888,
      "step": 494
    },
    {
      "epoch": 0.32587228439763,
      "grad_norm": 6.246432781219482,
      "learning_rate": 1.9423361806261333e-05,
      "loss": 2.8583,
      "step": 495
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 1.3287420272827148,
      "learning_rate": 1.9421052369755335e-05,
      "loss": 2.3169,
      "step": 496
    },
    {
      "epoch": 0.3271889400921659,
      "grad_norm": 5.969987869262695,
      "learning_rate": 1.9418738455692692e-05,
      "loss": 3.0998,
      "step": 497
    },
    {
      "epoch": 0.32784726793943386,
      "grad_norm": 2.642185688018799,
      "learning_rate": 1.9416420065173135e-05,
      "loss": 2.6616,
      "step": 498
    },
    {
      "epoch": 0.3285055957867018,
      "grad_norm": 2.389864921569824,
      "learning_rate": 1.9414097199298537e-05,
      "loss": 2.6313,
      "step": 499
    },
    {
      "epoch": 0.32916392363396973,
      "grad_norm": 6.055987358093262,
      "learning_rate": 1.9411769859172882e-05,
      "loss": 3.0812,
      "step": 500
    },
    {
      "epoch": 0.32982225148123767,
      "grad_norm": 3.8160393238067627,
      "learning_rate": 1.9409438045902292e-05,
      "loss": 2.5244,
      "step": 501
    },
    {
      "epoch": 0.3304805793285056,
      "grad_norm": 1.2141650915145874,
      "learning_rate": 1.9407101760595012e-05,
      "loss": 2.3157,
      "step": 502
    },
    {
      "epoch": 0.33113890717577354,
      "grad_norm": 1.4891281127929688,
      "learning_rate": 1.9404761004361407e-05,
      "loss": 2.2982,
      "step": 503
    },
    {
      "epoch": 0.3317972350230415,
      "grad_norm": 1.2639280557632446,
      "learning_rate": 1.9402415778313978e-05,
      "loss": 2.3049,
      "step": 504
    },
    {
      "epoch": 0.3324555628703094,
      "grad_norm": 5.841401100158691,
      "learning_rate": 1.940006608356734e-05,
      "loss": 3.1346,
      "step": 505
    },
    {
      "epoch": 0.33311389071757735,
      "grad_norm": 2.916005849838257,
      "learning_rate": 1.9397711921238233e-05,
      "loss": 2.6561,
      "step": 506
    },
    {
      "epoch": 0.3337722185648453,
      "grad_norm": 2.9546985626220703,
      "learning_rate": 1.939535329244553e-05,
      "loss": 2.6304,
      "step": 507
    },
    {
      "epoch": 0.3344305464121132,
      "grad_norm": 1.2026060819625854,
      "learning_rate": 1.9392990198310215e-05,
      "loss": 2.3055,
      "step": 508
    },
    {
      "epoch": 0.33508887425938116,
      "grad_norm": 2.98868465423584,
      "learning_rate": 1.9390622639955398e-05,
      "loss": 2.4815,
      "step": 509
    },
    {
      "epoch": 0.3357472021066491,
      "grad_norm": 2.889523983001709,
      "learning_rate": 1.9388250618506316e-05,
      "loss": 2.6385,
      "step": 510
    },
    {
      "epoch": 0.33640552995391704,
      "grad_norm": 1.2371212244033813,
      "learning_rate": 1.9385874135090316e-05,
      "loss": 2.2888,
      "step": 511
    },
    {
      "epoch": 0.337063857801185,
      "grad_norm": 2.6011552810668945,
      "learning_rate": 1.938349319083688e-05,
      "loss": 2.3894,
      "step": 512
    },
    {
      "epoch": 0.3377221856484529,
      "grad_norm": 1.29328191280365,
      "learning_rate": 1.93811077868776e-05,
      "loss": 2.267,
      "step": 513
    },
    {
      "epoch": 0.33838051349572085,
      "grad_norm": 1.3121635913848877,
      "learning_rate": 1.9378717924346192e-05,
      "loss": 2.2934,
      "step": 514
    },
    {
      "epoch": 0.3390388413429888,
      "grad_norm": 2.9126436710357666,
      "learning_rate": 1.9376323604378488e-05,
      "loss": 2.6326,
      "step": 515
    },
    {
      "epoch": 0.3396971691902567,
      "grad_norm": 1.8599148988723755,
      "learning_rate": 1.9373924828112437e-05,
      "loss": 2.3142,
      "step": 516
    },
    {
      "epoch": 0.3403554970375247,
      "grad_norm": 1.191074252128601,
      "learning_rate": 1.937152159668811e-05,
      "loss": 2.2779,
      "step": 517
    },
    {
      "epoch": 0.34101382488479265,
      "grad_norm": 1.2660760879516602,
      "learning_rate": 1.93691139112477e-05,
      "loss": 2.2772,
      "step": 518
    },
    {
      "epoch": 0.3416721527320606,
      "grad_norm": 6.2863383293151855,
      "learning_rate": 1.9366701772935503e-05,
      "loss": 3.0122,
      "step": 519
    },
    {
      "epoch": 0.3423304805793285,
      "grad_norm": 7.804314136505127,
      "learning_rate": 1.936428518289794e-05,
      "loss": 3.1662,
      "step": 520
    },
    {
      "epoch": 0.34298880842659646,
      "grad_norm": 1.401176929473877,
      "learning_rate": 1.936186414228355e-05,
      "loss": 2.2967,
      "step": 521
    },
    {
      "epoch": 0.3436471362738644,
      "grad_norm": 5.61639404296875,
      "learning_rate": 1.9359438652242986e-05,
      "loss": 2.9558,
      "step": 522
    },
    {
      "epoch": 0.34430546412113233,
      "grad_norm": 3.976329803466797,
      "learning_rate": 1.935700871392901e-05,
      "loss": 2.7468,
      "step": 523
    },
    {
      "epoch": 0.34496379196840027,
      "grad_norm": 5.049617767333984,
      "learning_rate": 1.9354574328496506e-05,
      "loss": 3.0376,
      "step": 524
    },
    {
      "epoch": 0.3456221198156682,
      "grad_norm": 6.9117841720581055,
      "learning_rate": 1.9352135497102464e-05,
      "loss": 2.8199,
      "step": 525
    },
    {
      "epoch": 0.34628044766293614,
      "grad_norm": 1.0275481939315796,
      "learning_rate": 1.9349692220905993e-05,
      "loss": 2.25,
      "step": 526
    },
    {
      "epoch": 0.3469387755102041,
      "grad_norm": 1.8720109462738037,
      "learning_rate": 1.934724450106831e-05,
      "loss": 2.31,
      "step": 527
    },
    {
      "epoch": 0.347597103357472,
      "grad_norm": 1.1937016248703003,
      "learning_rate": 1.9344792338752755e-05,
      "loss": 2.2414,
      "step": 528
    },
    {
      "epoch": 0.34825543120473995,
      "grad_norm": 2.864475727081299,
      "learning_rate": 1.9342335735124755e-05,
      "loss": 2.5986,
      "step": 529
    },
    {
      "epoch": 0.3489137590520079,
      "grad_norm": 5.858397006988525,
      "learning_rate": 1.9339874691351876e-05,
      "loss": 2.9273,
      "step": 530
    },
    {
      "epoch": 0.3495720868992758,
      "grad_norm": 6.272004127502441,
      "learning_rate": 1.9337409208603778e-05,
      "loss": 2.8,
      "step": 531
    },
    {
      "epoch": 0.35023041474654376,
      "grad_norm": 4.489917755126953,
      "learning_rate": 1.9334939288052236e-05,
      "loss": 2.7035,
      "step": 532
    },
    {
      "epoch": 0.3508887425938117,
      "grad_norm": 8.360403060913086,
      "learning_rate": 1.9332464930871126e-05,
      "loss": 2.9648,
      "step": 533
    },
    {
      "epoch": 0.35154707044107963,
      "grad_norm": 3.5226333141326904,
      "learning_rate": 1.9329986138236446e-05,
      "loss": 2.706,
      "step": 534
    },
    {
      "epoch": 0.35220539828834757,
      "grad_norm": 5.809921741485596,
      "learning_rate": 1.932750291132629e-05,
      "loss": 2.6104,
      "step": 535
    },
    {
      "epoch": 0.35286372613561556,
      "grad_norm": 2.3786463737487793,
      "learning_rate": 1.9325015251320872e-05,
      "loss": 2.3446,
      "step": 536
    },
    {
      "epoch": 0.3535220539828835,
      "grad_norm": 1.8284940719604492,
      "learning_rate": 1.9322523159402503e-05,
      "loss": 2.2775,
      "step": 537
    },
    {
      "epoch": 0.35418038183015144,
      "grad_norm": 2.9902918338775635,
      "learning_rate": 1.9320026636755598e-05,
      "loss": 2.5677,
      "step": 538
    },
    {
      "epoch": 0.3548387096774194,
      "grad_norm": 6.0271806716918945,
      "learning_rate": 1.9317525684566686e-05,
      "loss": 2.9725,
      "step": 539
    },
    {
      "epoch": 0.3554970375246873,
      "grad_norm": 2.6642298698425293,
      "learning_rate": 1.93150203040244e-05,
      "loss": 2.5876,
      "step": 540
    },
    {
      "epoch": 0.35615536537195525,
      "grad_norm": 3.704655647277832,
      "learning_rate": 1.9312510496319473e-05,
      "loss": 2.6698,
      "step": 541
    },
    {
      "epoch": 0.3568136932192232,
      "grad_norm": 3.248015880584717,
      "learning_rate": 1.9309996262644746e-05,
      "loss": 2.6664,
      "step": 542
    },
    {
      "epoch": 0.3574720210664911,
      "grad_norm": 5.642409801483154,
      "learning_rate": 1.9307477604195162e-05,
      "loss": 2.7618,
      "step": 543
    },
    {
      "epoch": 0.35813034891375906,
      "grad_norm": 1.5749586820602417,
      "learning_rate": 1.9304954522167768e-05,
      "loss": 2.2539,
      "step": 544
    },
    {
      "epoch": 0.358788676761027,
      "grad_norm": 2.073608160018921,
      "learning_rate": 1.930242701776171e-05,
      "loss": 2.2658,
      "step": 545
    },
    {
      "epoch": 0.35944700460829493,
      "grad_norm": 1.9486699104309082,
      "learning_rate": 1.929989509217824e-05,
      "loss": 2.279,
      "step": 546
    },
    {
      "epoch": 0.36010533245556287,
      "grad_norm": 4.1719465255737305,
      "learning_rate": 1.9297358746620713e-05,
      "loss": 2.5451,
      "step": 547
    },
    {
      "epoch": 0.3607636603028308,
      "grad_norm": 4.972867965698242,
      "learning_rate": 1.9294817982294578e-05,
      "loss": 2.7135,
      "step": 548
    },
    {
      "epoch": 0.36142198815009874,
      "grad_norm": 5.467557430267334,
      "learning_rate": 1.9292272800407387e-05,
      "loss": 2.7655,
      "step": 549
    },
    {
      "epoch": 0.3620803159973667,
      "grad_norm": 3.9501898288726807,
      "learning_rate": 1.9289723202168796e-05,
      "loss": 2.6203,
      "step": 550
    },
    {
      "epoch": 0.3627386438446346,
      "grad_norm": 4.0764265060424805,
      "learning_rate": 1.9287169188790555e-05,
      "loss": 2.7899,
      "step": 551
    },
    {
      "epoch": 0.36339697169190255,
      "grad_norm": 3.3346476554870605,
      "learning_rate": 1.9284610761486505e-05,
      "loss": 2.4287,
      "step": 552
    },
    {
      "epoch": 0.3640552995391705,
      "grad_norm": 1.9505503177642822,
      "learning_rate": 1.928204792147261e-05,
      "loss": 2.2958,
      "step": 553
    },
    {
      "epoch": 0.3647136273864384,
      "grad_norm": 3.130131483078003,
      "learning_rate": 1.92794806699669e-05,
      "loss": 2.5998,
      "step": 554
    },
    {
      "epoch": 0.36537195523370636,
      "grad_norm": 1.5125364065170288,
      "learning_rate": 1.927690900818952e-05,
      "loss": 2.2426,
      "step": 555
    },
    {
      "epoch": 0.36603028308097435,
      "grad_norm": 2.277676820755005,
      "learning_rate": 1.9274332937362706e-05,
      "loss": 2.3203,
      "step": 556
    },
    {
      "epoch": 0.3666886109282423,
      "grad_norm": 1.5303877592086792,
      "learning_rate": 1.92717524587108e-05,
      "loss": 2.2629,
      "step": 557
    },
    {
      "epoch": 0.3673469387755102,
      "grad_norm": 2.633280038833618,
      "learning_rate": 1.926916757346022e-05,
      "loss": 2.5761,
      "step": 558
    },
    {
      "epoch": 0.36800526662277816,
      "grad_norm": 2.7266924381256104,
      "learning_rate": 1.926657828283949e-05,
      "loss": 2.5752,
      "step": 559
    },
    {
      "epoch": 0.3686635944700461,
      "grad_norm": 2.5036203861236572,
      "learning_rate": 1.9263984588079227e-05,
      "loss": 2.5797,
      "step": 560
    },
    {
      "epoch": 0.36932192231731403,
      "grad_norm": 2.517575263977051,
      "learning_rate": 1.926138649041214e-05,
      "loss": 2.6196,
      "step": 561
    },
    {
      "epoch": 0.36998025016458197,
      "grad_norm": 1.4767903089523315,
      "learning_rate": 1.925878399107303e-05,
      "loss": 2.2331,
      "step": 562
    },
    {
      "epoch": 0.3706385780118499,
      "grad_norm": 1.477097511291504,
      "learning_rate": 1.925617709129879e-05,
      "loss": 2.2511,
      "step": 563
    },
    {
      "epoch": 0.37129690585911784,
      "grad_norm": 2.5476365089416504,
      "learning_rate": 1.9253565792328404e-05,
      "loss": 2.5478,
      "step": 564
    },
    {
      "epoch": 0.3719552337063858,
      "grad_norm": 5.175194263458252,
      "learning_rate": 1.9250950095402953e-05,
      "loss": 2.642,
      "step": 565
    },
    {
      "epoch": 0.3726135615536537,
      "grad_norm": 1.4435786008834839,
      "learning_rate": 1.9248330001765595e-05,
      "loss": 2.2431,
      "step": 566
    },
    {
      "epoch": 0.37327188940092165,
      "grad_norm": 1.096521258354187,
      "learning_rate": 1.9245705512661594e-05,
      "loss": 2.234,
      "step": 567
    },
    {
      "epoch": 0.3739302172481896,
      "grad_norm": 6.644186496734619,
      "learning_rate": 1.9243076629338283e-05,
      "loss": 3.0632,
      "step": 568
    },
    {
      "epoch": 0.3745885450954575,
      "grad_norm": 1.5913218259811401,
      "learning_rate": 1.924044335304511e-05,
      "loss": 2.2548,
      "step": 569
    },
    {
      "epoch": 0.37524687294272546,
      "grad_norm": 7.957432270050049,
      "learning_rate": 1.923780568503358e-05,
      "loss": 2.8794,
      "step": 570
    },
    {
      "epoch": 0.3759052007899934,
      "grad_norm": 3.1715500354766846,
      "learning_rate": 1.9235163626557316e-05,
      "loss": 2.344,
      "step": 571
    },
    {
      "epoch": 0.37656352863726134,
      "grad_norm": 7.614261150360107,
      "learning_rate": 1.9232517178872002e-05,
      "loss": 2.8877,
      "step": 572
    },
    {
      "epoch": 0.37722185648452927,
      "grad_norm": 6.211179733276367,
      "learning_rate": 1.9229866343235427e-05,
      "loss": 2.6677,
      "step": 573
    },
    {
      "epoch": 0.3778801843317972,
      "grad_norm": 6.320873260498047,
      "learning_rate": 1.922721112090745e-05,
      "loss": 2.7114,
      "step": 574
    },
    {
      "epoch": 0.3785385121790652,
      "grad_norm": 2.2141544818878174,
      "learning_rate": 1.9224551513150023e-05,
      "loss": 2.3244,
      "step": 575
    },
    {
      "epoch": 0.37919684002633314,
      "grad_norm": 4.999366283416748,
      "learning_rate": 1.9221887521227187e-05,
      "loss": 2.705,
      "step": 576
    },
    {
      "epoch": 0.3798551678736011,
      "grad_norm": 2.278338670730591,
      "learning_rate": 1.921921914640506e-05,
      "loss": 2.4835,
      "step": 577
    },
    {
      "epoch": 0.380513495720869,
      "grad_norm": 1.4016642570495605,
      "learning_rate": 1.9216546389951838e-05,
      "loss": 2.2227,
      "step": 578
    },
    {
      "epoch": 0.38117182356813695,
      "grad_norm": 1.0710160732269287,
      "learning_rate": 1.921386925313781e-05,
      "loss": 2.1944,
      "step": 579
    },
    {
      "epoch": 0.3818301514154049,
      "grad_norm": 8.698652267456055,
      "learning_rate": 1.9211187737235347e-05,
      "loss": 3.2589,
      "step": 580
    },
    {
      "epoch": 0.3824884792626728,
      "grad_norm": 4.373236656188965,
      "learning_rate": 1.920850184351889e-05,
      "loss": 2.7574,
      "step": 581
    },
    {
      "epoch": 0.38314680710994076,
      "grad_norm": 1.6603223085403442,
      "learning_rate": 1.920581157326497e-05,
      "loss": 2.222,
      "step": 582
    },
    {
      "epoch": 0.3838051349572087,
      "grad_norm": 6.803229808807373,
      "learning_rate": 1.920311692775219e-05,
      "loss": 2.6102,
      "step": 583
    },
    {
      "epoch": 0.38446346280447663,
      "grad_norm": 5.618320465087891,
      "learning_rate": 1.9200417908261245e-05,
      "loss": 2.7108,
      "step": 584
    },
    {
      "epoch": 0.38512179065174457,
      "grad_norm": 1.258826732635498,
      "learning_rate": 1.9197714516074904e-05,
      "loss": 2.226,
      "step": 585
    },
    {
      "epoch": 0.3857801184990125,
      "grad_norm": 5.859325408935547,
      "learning_rate": 1.9195006752478004e-05,
      "loss": 2.7471,
      "step": 586
    },
    {
      "epoch": 0.38643844634628044,
      "grad_norm": 10.549585342407227,
      "learning_rate": 1.9192294618757473e-05,
      "loss": 3.7301,
      "step": 587
    },
    {
      "epoch": 0.3870967741935484,
      "grad_norm": 3.521552801132202,
      "learning_rate": 1.918957811620231e-05,
      "loss": 2.5352,
      "step": 588
    },
    {
      "epoch": 0.3877551020408163,
      "grad_norm": 8.334102630615234,
      "learning_rate": 1.9186857246103586e-05,
      "loss": 2.9682,
      "step": 589
    },
    {
      "epoch": 0.38841342988808425,
      "grad_norm": 2.4196090698242188,
      "learning_rate": 1.9184132009754458e-05,
      "loss": 2.2531,
      "step": 590
    },
    {
      "epoch": 0.3890717577353522,
      "grad_norm": 4.929253578186035,
      "learning_rate": 1.9181402408450154e-05,
      "loss": 2.5692,
      "step": 591
    },
    {
      "epoch": 0.3897300855826201,
      "grad_norm": 2.827125072479248,
      "learning_rate": 1.9178668443487968e-05,
      "loss": 2.5342,
      "step": 592
    },
    {
      "epoch": 0.39038841342988806,
      "grad_norm": 7.566347599029541,
      "learning_rate": 1.917593011616728e-05,
      "loss": 2.7192,
      "step": 593
    },
    {
      "epoch": 0.39104674127715605,
      "grad_norm": 6.609847545623779,
      "learning_rate": 1.9173187427789547e-05,
      "loss": 2.5422,
      "step": 594
    },
    {
      "epoch": 0.391705069124424,
      "grad_norm": 6.349096775054932,
      "learning_rate": 1.9170440379658275e-05,
      "loss": 2.8034,
      "step": 595
    },
    {
      "epoch": 0.3923633969716919,
      "grad_norm": 6.130876541137695,
      "learning_rate": 1.9167688973079066e-05,
      "loss": 2.722,
      "step": 596
    },
    {
      "epoch": 0.39302172481895986,
      "grad_norm": 7.123457908630371,
      "learning_rate": 1.9164933209359586e-05,
      "loss": 2.7347,
      "step": 597
    },
    {
      "epoch": 0.3936800526662278,
      "grad_norm": 4.843188285827637,
      "learning_rate": 1.9162173089809562e-05,
      "loss": 2.4989,
      "step": 598
    },
    {
      "epoch": 0.39433838051349573,
      "grad_norm": 2.1110498905181885,
      "learning_rate": 1.915940861574081e-05,
      "loss": 2.2423,
      "step": 599
    },
    {
      "epoch": 0.39499670836076367,
      "grad_norm": 9.594202041625977,
      "learning_rate": 1.9156639788467198e-05,
      "loss": 3.4089,
      "step": 600
    },
    {
      "epoch": 0.3956550362080316,
      "grad_norm": 1.9963423013687134,
      "learning_rate": 1.9153866609304677e-05,
      "loss": 2.2045,
      "step": 601
    },
    {
      "epoch": 0.39631336405529954,
      "grad_norm": 4.3596696853637695,
      "learning_rate": 1.915108907957125e-05,
      "loss": 2.5104,
      "step": 602
    },
    {
      "epoch": 0.3969716919025675,
      "grad_norm": 4.565990924835205,
      "learning_rate": 1.914830720058701e-05,
      "loss": 2.4927,
      "step": 603
    },
    {
      "epoch": 0.3976300197498354,
      "grad_norm": 5.868993282318115,
      "learning_rate": 1.91455209736741e-05,
      "loss": 2.4507,
      "step": 604
    },
    {
      "epoch": 0.39828834759710335,
      "grad_norm": 3.863186836242676,
      "learning_rate": 1.914273040015673e-05,
      "loss": 2.4329,
      "step": 605
    },
    {
      "epoch": 0.3989466754443713,
      "grad_norm": 4.006386756896973,
      "learning_rate": 1.913993548136118e-05,
      "loss": 2.5225,
      "step": 606
    },
    {
      "epoch": 0.3996050032916392,
      "grad_norm": 4.718785762786865,
      "learning_rate": 1.9137136218615797e-05,
      "loss": 2.4883,
      "step": 607
    },
    {
      "epoch": 0.40026333113890716,
      "grad_norm": 4.034761428833008,
      "learning_rate": 1.913433261325099e-05,
      "loss": 2.3146,
      "step": 608
    },
    {
      "epoch": 0.4009216589861751,
      "grad_norm": 8.187286376953125,
      "learning_rate": 1.9131524666599233e-05,
      "loss": 2.8466,
      "step": 609
    },
    {
      "epoch": 0.40157998683344304,
      "grad_norm": 3.0848159790039062,
      "learning_rate": 1.9128712379995062e-05,
      "loss": 2.5568,
      "step": 610
    },
    {
      "epoch": 0.402238314680711,
      "grad_norm": 3.4988808631896973,
      "learning_rate": 1.9125895754775076e-05,
      "loss": 2.3422,
      "step": 611
    },
    {
      "epoch": 0.4028966425279789,
      "grad_norm": 4.927262783050537,
      "learning_rate": 1.912307479227794e-05,
      "loss": 2.5193,
      "step": 612
    },
    {
      "epoch": 0.40355497037524685,
      "grad_norm": 1.84965181350708,
      "learning_rate": 1.9120249493844373e-05,
      "loss": 2.2214,
      "step": 613
    },
    {
      "epoch": 0.40421329822251484,
      "grad_norm": 10.399998664855957,
      "learning_rate": 1.9117419860817158e-05,
      "loss": 3.2659,
      "step": 614
    },
    {
      "epoch": 0.4048716260697828,
      "grad_norm": 5.304758548736572,
      "learning_rate": 1.9114585894541142e-05,
      "loss": 2.575,
      "step": 615
    },
    {
      "epoch": 0.4055299539170507,
      "grad_norm": 5.333985328674316,
      "learning_rate": 1.9111747596363227e-05,
      "loss": 2.581,
      "step": 616
    },
    {
      "epoch": 0.40618828176431865,
      "grad_norm": 4.0051751136779785,
      "learning_rate": 1.910890496763237e-05,
      "loss": 2.6759,
      "step": 617
    },
    {
      "epoch": 0.4068466096115866,
      "grad_norm": 5.118816375732422,
      "learning_rate": 1.9106058009699596e-05,
      "loss": 2.421,
      "step": 618
    },
    {
      "epoch": 0.4075049374588545,
      "grad_norm": 2.70750093460083,
      "learning_rate": 1.9103206723917986e-05,
      "loss": 2.2433,
      "step": 619
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 7.446139812469482,
      "learning_rate": 1.9100351111642666e-05,
      "loss": 2.5552,
      "step": 620
    },
    {
      "epoch": 0.4088215931533904,
      "grad_norm": 5.066293239593506,
      "learning_rate": 1.9097491174230835e-05,
      "loss": 2.4202,
      "step": 621
    },
    {
      "epoch": 0.40947992100065833,
      "grad_norm": 6.489253997802734,
      "learning_rate": 1.9094626913041733e-05,
      "loss": 2.446,
      "step": 622
    },
    {
      "epoch": 0.41013824884792627,
      "grad_norm": 9.018980026245117,
      "learning_rate": 1.9091758329436664e-05,
      "loss": 3.1972,
      "step": 623
    },
    {
      "epoch": 0.4107965766951942,
      "grad_norm": 2.0421440601348877,
      "learning_rate": 1.9088885424778985e-05,
      "loss": 2.2397,
      "step": 624
    },
    {
      "epoch": 0.41145490454246214,
      "grad_norm": 2.389516830444336,
      "learning_rate": 1.9086008200434103e-05,
      "loss": 2.2327,
      "step": 625
    },
    {
      "epoch": 0.4121132323897301,
      "grad_norm": 1.8935073614120483,
      "learning_rate": 1.908312665776948e-05,
      "loss": 2.2311,
      "step": 626
    },
    {
      "epoch": 0.412771560236998,
      "grad_norm": 3.055163621902466,
      "learning_rate": 1.9080240798154637e-05,
      "loss": 2.5588,
      "step": 627
    },
    {
      "epoch": 0.41342988808426595,
      "grad_norm": 2.4955122470855713,
      "learning_rate": 1.9077350622961133e-05,
      "loss": 2.3247,
      "step": 628
    },
    {
      "epoch": 0.4140882159315339,
      "grad_norm": 2.867558479309082,
      "learning_rate": 1.907445613356259e-05,
      "loss": 2.3331,
      "step": 629
    },
    {
      "epoch": 0.4147465437788018,
      "grad_norm": 3.8508338928222656,
      "learning_rate": 1.907155733133467e-05,
      "loss": 2.6411,
      "step": 630
    },
    {
      "epoch": 0.41540487162606976,
      "grad_norm": 5.139443397521973,
      "learning_rate": 1.9068654217655097e-05,
      "loss": 2.5104,
      "step": 631
    },
    {
      "epoch": 0.4160631994733377,
      "grad_norm": 3.965968608856201,
      "learning_rate": 1.9065746793903638e-05,
      "loss": 2.6331,
      "step": 632
    },
    {
      "epoch": 0.4167215273206057,
      "grad_norm": 6.2897210121154785,
      "learning_rate": 1.9062835061462105e-05,
      "loss": 2.8302,
      "step": 633
    },
    {
      "epoch": 0.4173798551678736,
      "grad_norm": 3.9840621948242188,
      "learning_rate": 1.9059919021714363e-05,
      "loss": 2.4657,
      "step": 634
    },
    {
      "epoch": 0.41803818301514156,
      "grad_norm": 6.0380964279174805,
      "learning_rate": 1.905699867604632e-05,
      "loss": 2.535,
      "step": 635
    },
    {
      "epoch": 0.4186965108624095,
      "grad_norm": 7.124414920806885,
      "learning_rate": 1.9054074025845936e-05,
      "loss": 2.7475,
      "step": 636
    },
    {
      "epoch": 0.41935483870967744,
      "grad_norm": 4.083868980407715,
      "learning_rate": 1.9051145072503216e-05,
      "loss": 2.3756,
      "step": 637
    },
    {
      "epoch": 0.42001316655694537,
      "grad_norm": 1.832573652267456,
      "learning_rate": 1.9048211817410198e-05,
      "loss": 2.1992,
      "step": 638
    },
    {
      "epoch": 0.4206714944042133,
      "grad_norm": 1.0213074684143066,
      "learning_rate": 1.904527426196098e-05,
      "loss": 2.1652,
      "step": 639
    },
    {
      "epoch": 0.42132982225148125,
      "grad_norm": 7.384653568267822,
      "learning_rate": 1.90423324075517e-05,
      "loss": 2.7547,
      "step": 640
    },
    {
      "epoch": 0.4219881500987492,
      "grad_norm": 1.7479236125946045,
      "learning_rate": 1.9039386255580538e-05,
      "loss": 2.1286,
      "step": 641
    },
    {
      "epoch": 0.4226464779460171,
      "grad_norm": 3.285409450531006,
      "learning_rate": 1.9036435807447708e-05,
      "loss": 2.2742,
      "step": 642
    },
    {
      "epoch": 0.42330480579328505,
      "grad_norm": 1.1523613929748535,
      "learning_rate": 1.903348106455548e-05,
      "loss": 2.1604,
      "step": 643
    },
    {
      "epoch": 0.423963133640553,
      "grad_norm": 6.838241100311279,
      "learning_rate": 1.903052202830816e-05,
      "loss": 2.8378,
      "step": 644
    },
    {
      "epoch": 0.42462146148782093,
      "grad_norm": 2.294785737991333,
      "learning_rate": 1.9027558700112086e-05,
      "loss": 2.2583,
      "step": 645
    },
    {
      "epoch": 0.42527978933508886,
      "grad_norm": 0.9292038083076477,
      "learning_rate": 1.902459108137565e-05,
      "loss": 2.1361,
      "step": 646
    },
    {
      "epoch": 0.4259381171823568,
      "grad_norm": 1.249712586402893,
      "learning_rate": 1.9021619173509274e-05,
      "loss": 2.1508,
      "step": 647
    },
    {
      "epoch": 0.42659644502962474,
      "grad_norm": 0.9707431197166443,
      "learning_rate": 1.9018642977925422e-05,
      "loss": 2.1328,
      "step": 648
    },
    {
      "epoch": 0.4272547728768927,
      "grad_norm": 0.8796018958091736,
      "learning_rate": 1.9015662496038595e-05,
      "loss": 2.1164,
      "step": 649
    },
    {
      "epoch": 0.4279131007241606,
      "grad_norm": 8.908533096313477,
      "learning_rate": 1.9012677729265324e-05,
      "loss": 3.27,
      "step": 650
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 9.638325691223145,
      "learning_rate": 1.900968867902419e-05,
      "loss": 2.7405,
      "step": 651
    },
    {
      "epoch": 0.4292297564186965,
      "grad_norm": 9.063857078552246,
      "learning_rate": 1.9006695346735805e-05,
      "loss": 2.8106,
      "step": 652
    },
    {
      "epoch": 0.4298880842659645,
      "grad_norm": 0.9857887625694275,
      "learning_rate": 1.900369773382281e-05,
      "loss": 2.1124,
      "step": 653
    },
    {
      "epoch": 0.4305464121132324,
      "grad_norm": 5.996527671813965,
      "learning_rate": 1.9000695841709883e-05,
      "loss": 2.844,
      "step": 654
    },
    {
      "epoch": 0.43120473996050035,
      "grad_norm": 3.1727352142333984,
      "learning_rate": 1.899768967182374e-05,
      "loss": 2.2526,
      "step": 655
    },
    {
      "epoch": 0.4318630678077683,
      "grad_norm": 9.927995681762695,
      "learning_rate": 1.8994679225593126e-05,
      "loss": 3.0336,
      "step": 656
    },
    {
      "epoch": 0.4325213956550362,
      "grad_norm": 5.8433027267456055,
      "learning_rate": 1.8991664504448826e-05,
      "loss": 2.7785,
      "step": 657
    },
    {
      "epoch": 0.43317972350230416,
      "grad_norm": 9.787739753723145,
      "learning_rate": 1.8988645509823642e-05,
      "loss": 2.6151,
      "step": 658
    },
    {
      "epoch": 0.4338380513495721,
      "grad_norm": 1.5333478450775146,
      "learning_rate": 1.8985622243152422e-05,
      "loss": 2.1279,
      "step": 659
    },
    {
      "epoch": 0.43449637919684003,
      "grad_norm": 2.440659523010254,
      "learning_rate": 1.8982594705872034e-05,
      "loss": 2.1402,
      "step": 660
    },
    {
      "epoch": 0.43515470704410797,
      "grad_norm": 7.026559352874756,
      "learning_rate": 1.8979562899421383e-05,
      "loss": 2.9809,
      "step": 661
    },
    {
      "epoch": 0.4358130348913759,
      "grad_norm": 6.748578071594238,
      "learning_rate": 1.8976526825241396e-05,
      "loss": 2.6171,
      "step": 662
    },
    {
      "epoch": 0.43647136273864384,
      "grad_norm": 6.799321174621582,
      "learning_rate": 1.8973486484775037e-05,
      "loss": 2.7173,
      "step": 663
    },
    {
      "epoch": 0.4371296905859118,
      "grad_norm": 4.298779010772705,
      "learning_rate": 1.897044187946729e-05,
      "loss": 2.2804,
      "step": 664
    },
    {
      "epoch": 0.4377880184331797,
      "grad_norm": 1.8325477838516235,
      "learning_rate": 1.896739301076517e-05,
      "loss": 2.1503,
      "step": 665
    },
    {
      "epoch": 0.43844634628044765,
      "grad_norm": 6.960696697235107,
      "learning_rate": 1.8964339880117716e-05,
      "loss": 2.9454,
      "step": 666
    },
    {
      "epoch": 0.4391046741277156,
      "grad_norm": 8.539948463439941,
      "learning_rate": 1.8961282488975995e-05,
      "loss": 2.6244,
      "step": 667
    },
    {
      "epoch": 0.4397630019749835,
      "grad_norm": 1.0657052993774414,
      "learning_rate": 1.8958220838793098e-05,
      "loss": 2.0967,
      "step": 668
    },
    {
      "epoch": 0.44042132982225146,
      "grad_norm": 1.947129249572754,
      "learning_rate": 1.8955154931024133e-05,
      "loss": 2.1853,
      "step": 669
    },
    {
      "epoch": 0.4410796576695194,
      "grad_norm": 9.454431533813477,
      "learning_rate": 1.8952084767126248e-05,
      "loss": 2.8411,
      "step": 670
    },
    {
      "epoch": 0.44173798551678733,
      "grad_norm": 1.5164861679077148,
      "learning_rate": 1.89490103485586e-05,
      "loss": 2.111,
      "step": 671
    },
    {
      "epoch": 0.4423963133640553,
      "grad_norm": 10.436172485351562,
      "learning_rate": 1.8945931676782373e-05,
      "loss": 2.8985,
      "step": 672
    },
    {
      "epoch": 0.44305464121132326,
      "grad_norm": 5.324672222137451,
      "learning_rate": 1.8942848753260764e-05,
      "loss": 2.7218,
      "step": 673
    },
    {
      "epoch": 0.4437129690585912,
      "grad_norm": 5.171081066131592,
      "learning_rate": 1.893976157945901e-05,
      "loss": 2.6746,
      "step": 674
    },
    {
      "epoch": 0.44437129690585914,
      "grad_norm": 9.433597564697266,
      "learning_rate": 1.893667015684435e-05,
      "loss": 2.8922,
      "step": 675
    },
    {
      "epoch": 0.4450296247531271,
      "grad_norm": 5.550064563751221,
      "learning_rate": 1.893357448688605e-05,
      "loss": 2.5806,
      "step": 676
    },
    {
      "epoch": 0.445687952600395,
      "grad_norm": 1.5962846279144287,
      "learning_rate": 1.8930474571055396e-05,
      "loss": 2.1212,
      "step": 677
    },
    {
      "epoch": 0.44634628044766295,
      "grad_norm": 1.2745842933654785,
      "learning_rate": 1.8927370410825686e-05,
      "loss": 2.1312,
      "step": 678
    },
    {
      "epoch": 0.4470046082949309,
      "grad_norm": 8.251864433288574,
      "learning_rate": 1.8924262007672235e-05,
      "loss": 2.8994,
      "step": 679
    },
    {
      "epoch": 0.4476629361421988,
      "grad_norm": 1.228054165840149,
      "learning_rate": 1.8921149363072384e-05,
      "loss": 2.1093,
      "step": 680
    },
    {
      "epoch": 0.44832126398946676,
      "grad_norm": 1.3843134641647339,
      "learning_rate": 1.8918032478505485e-05,
      "loss": 2.111,
      "step": 681
    },
    {
      "epoch": 0.4489795918367347,
      "grad_norm": 1.7924468517303467,
      "learning_rate": 1.8914911355452895e-05,
      "loss": 2.1147,
      "step": 682
    },
    {
      "epoch": 0.44963791968400263,
      "grad_norm": 7.084935665130615,
      "learning_rate": 1.8911785995398008e-05,
      "loss": 2.6789,
      "step": 683
    },
    {
      "epoch": 0.45029624753127057,
      "grad_norm": 5.497318267822266,
      "learning_rate": 1.8908656399826206e-05,
      "loss": 2.6839,
      "step": 684
    },
    {
      "epoch": 0.4509545753785385,
      "grad_norm": 5.196445941925049,
      "learning_rate": 1.8905522570224905e-05,
      "loss": 2.6165,
      "step": 685
    },
    {
      "epoch": 0.45161290322580644,
      "grad_norm": 5.217240810394287,
      "learning_rate": 1.8902384508083518e-05,
      "loss": 2.6414,
      "step": 686
    },
    {
      "epoch": 0.4522712310730744,
      "grad_norm": 3.9121315479278564,
      "learning_rate": 1.8899242214893477e-05,
      "loss": 2.6081,
      "step": 687
    },
    {
      "epoch": 0.4529295589203423,
      "grad_norm": 4.132244110107422,
      "learning_rate": 1.889609569214823e-05,
      "loss": 2.3322,
      "step": 688
    },
    {
      "epoch": 0.45358788676761025,
      "grad_norm": 5.357375144958496,
      "learning_rate": 1.8892944941343226e-05,
      "loss": 2.6753,
      "step": 689
    },
    {
      "epoch": 0.4542462146148782,
      "grad_norm": 1.9864457845687866,
      "learning_rate": 1.888978996397592e-05,
      "loss": 2.0971,
      "step": 690
    },
    {
      "epoch": 0.4549045424621461,
      "grad_norm": 2.0266854763031006,
      "learning_rate": 1.8886630761545793e-05,
      "loss": 2.1087,
      "step": 691
    },
    {
      "epoch": 0.4555628703094141,
      "grad_norm": 5.014176368713379,
      "learning_rate": 1.8883467335554317e-05,
      "loss": 2.6312,
      "step": 692
    },
    {
      "epoch": 0.45622119815668205,
      "grad_norm": 4.819976806640625,
      "learning_rate": 1.888029968750498e-05,
      "loss": 2.3931,
      "step": 693
    },
    {
      "epoch": 0.45687952600395,
      "grad_norm": 1.3596620559692383,
      "learning_rate": 1.8877127818903274e-05,
      "loss": 2.1161,
      "step": 694
    },
    {
      "epoch": 0.4575378538512179,
      "grad_norm": 5.2243523597717285,
      "learning_rate": 1.88739517312567e-05,
      "loss": 2.4241,
      "step": 695
    },
    {
      "epoch": 0.45819618169848586,
      "grad_norm": 2.9084665775299072,
      "learning_rate": 1.8870771426074757e-05,
      "loss": 2.1904,
      "step": 696
    },
    {
      "epoch": 0.4588545095457538,
      "grad_norm": 3.445279598236084,
      "learning_rate": 1.8867586904868954e-05,
      "loss": 2.5214,
      "step": 697
    },
    {
      "epoch": 0.45951283739302173,
      "grad_norm": 1.904663324356079,
      "learning_rate": 1.8864398169152808e-05,
      "loss": 2.1359,
      "step": 698
    },
    {
      "epoch": 0.46017116524028967,
      "grad_norm": 1.3755303621292114,
      "learning_rate": 1.8861205220441827e-05,
      "loss": 2.1148,
      "step": 699
    },
    {
      "epoch": 0.4608294930875576,
      "grad_norm": 2.749868392944336,
      "learning_rate": 1.8858008060253533e-05,
      "loss": 2.1432,
      "step": 700
    },
    {
      "epoch": 0.46148782093482554,
      "grad_norm": 3.5039565563201904,
      "learning_rate": 1.885480669010744e-05,
      "loss": 2.1909,
      "step": 701
    },
    {
      "epoch": 0.4621461487820935,
      "grad_norm": 8.947022438049316,
      "learning_rate": 1.8851601111525074e-05,
      "loss": 3.1823,
      "step": 702
    },
    {
      "epoch": 0.4628044766293614,
      "grad_norm": 9.09266471862793,
      "learning_rate": 1.884839132602995e-05,
      "loss": 3.1682,
      "step": 703
    },
    {
      "epoch": 0.46346280447662935,
      "grad_norm": 2.8307769298553467,
      "learning_rate": 1.8845177335147587e-05,
      "loss": 2.2144,
      "step": 704
    },
    {
      "epoch": 0.4641211323238973,
      "grad_norm": 3.9545376300811768,
      "learning_rate": 1.8841959140405504e-05,
      "loss": 2.4922,
      "step": 705
    },
    {
      "epoch": 0.4647794601711652,
      "grad_norm": 1.1039429903030396,
      "learning_rate": 1.883873674333322e-05,
      "loss": 2.0855,
      "step": 706
    },
    {
      "epoch": 0.46543778801843316,
      "grad_norm": 9.459474563598633,
      "learning_rate": 1.8835510145462242e-05,
      "loss": 3.1257,
      "step": 707
    },
    {
      "epoch": 0.4660961158657011,
      "grad_norm": 7.571737289428711,
      "learning_rate": 1.8832279348326086e-05,
      "loss": 2.6138,
      "step": 708
    },
    {
      "epoch": 0.46675444371296904,
      "grad_norm": 7.5309882164001465,
      "learning_rate": 1.8829044353460255e-05,
      "loss": 2.696,
      "step": 709
    },
    {
      "epoch": 0.467412771560237,
      "grad_norm": 5.82879114151001,
      "learning_rate": 1.882580516240225e-05,
      "loss": 2.7147,
      "step": 710
    },
    {
      "epoch": 0.46807109940750496,
      "grad_norm": 1.1284600496292114,
      "learning_rate": 1.882256177669156e-05,
      "loss": 2.0978,
      "step": 711
    },
    {
      "epoch": 0.4687294272547729,
      "grad_norm": 1.6315642595291138,
      "learning_rate": 1.881931419786968e-05,
      "loss": 2.1004,
      "step": 712
    },
    {
      "epoch": 0.46938775510204084,
      "grad_norm": 6.871844291687012,
      "learning_rate": 1.881606242748009e-05,
      "loss": 2.689,
      "step": 713
    },
    {
      "epoch": 0.4700460829493088,
      "grad_norm": 1.351233720779419,
      "learning_rate": 1.881280646706827e-05,
      "loss": 2.1044,
      "step": 714
    },
    {
      "epoch": 0.4707044107965767,
      "grad_norm": 4.443942546844482,
      "learning_rate": 1.8809546318181668e-05,
      "loss": 2.5305,
      "step": 715
    },
    {
      "epoch": 0.47136273864384465,
      "grad_norm": 2.784508228302002,
      "learning_rate": 1.8806281982369752e-05,
      "loss": 2.2301,
      "step": 716
    },
    {
      "epoch": 0.4720210664911126,
      "grad_norm": 7.724234580993652,
      "learning_rate": 1.8803013461183966e-05,
      "loss": 2.6346,
      "step": 717
    },
    {
      "epoch": 0.4726793943383805,
      "grad_norm": 1.1010345220565796,
      "learning_rate": 1.8799740756177742e-05,
      "loss": 2.0723,
      "step": 718
    },
    {
      "epoch": 0.47333772218564846,
      "grad_norm": 1.394056797027588,
      "learning_rate": 1.8796463868906503e-05,
      "loss": 2.0997,
      "step": 719
    },
    {
      "epoch": 0.4739960500329164,
      "grad_norm": 2.2595508098602295,
      "learning_rate": 1.8793182800927666e-05,
      "loss": 2.097,
      "step": 720
    },
    {
      "epoch": 0.47465437788018433,
      "grad_norm": 8.064784049987793,
      "learning_rate": 1.8789897553800623e-05,
      "loss": 2.6234,
      "step": 721
    },
    {
      "epoch": 0.47531270572745227,
      "grad_norm": 1.6769598722457886,
      "learning_rate": 1.8786608129086756e-05,
      "loss": 2.109,
      "step": 722
    },
    {
      "epoch": 0.4759710335747202,
      "grad_norm": 3.896010398864746,
      "learning_rate": 1.878331452834944e-05,
      "loss": 2.4003,
      "step": 723
    },
    {
      "epoch": 0.47662936142198814,
      "grad_norm": 6.900142192840576,
      "learning_rate": 1.8780016753154026e-05,
      "loss": 2.7496,
      "step": 724
    },
    {
      "epoch": 0.4772876892692561,
      "grad_norm": 2.318535327911377,
      "learning_rate": 1.877671480506785e-05,
      "loss": 2.0794,
      "step": 725
    },
    {
      "epoch": 0.477946017116524,
      "grad_norm": 5.105133056640625,
      "learning_rate": 1.877340868566024e-05,
      "loss": 2.6302,
      "step": 726
    },
    {
      "epoch": 0.47860434496379195,
      "grad_norm": 1.5766164064407349,
      "learning_rate": 1.8770098396502492e-05,
      "loss": 2.0778,
      "step": 727
    },
    {
      "epoch": 0.4792626728110599,
      "grad_norm": 10.432999610900879,
      "learning_rate": 1.87667839391679e-05,
      "loss": 2.9834,
      "step": 728
    },
    {
      "epoch": 0.4799210006583278,
      "grad_norm": 1.2382371425628662,
      "learning_rate": 1.8763465315231726e-05,
      "loss": 2.0664,
      "step": 729
    },
    {
      "epoch": 0.48057932850559576,
      "grad_norm": 7.245316982269287,
      "learning_rate": 1.8760142526271218e-05,
      "loss": 2.7137,
      "step": 730
    },
    {
      "epoch": 0.48123765635286375,
      "grad_norm": 1.1946696043014526,
      "learning_rate": 1.87568155738656e-05,
      "loss": 2.0681,
      "step": 731
    },
    {
      "epoch": 0.4818959842001317,
      "grad_norm": 2.3306522369384766,
      "learning_rate": 1.8753484459596078e-05,
      "loss": 2.1365,
      "step": 732
    },
    {
      "epoch": 0.4825543120473996,
      "grad_norm": 2.1481239795684814,
      "learning_rate": 1.875014918504584e-05,
      "loss": 2.1359,
      "step": 733
    },
    {
      "epoch": 0.48321263989466756,
      "grad_norm": 6.927103519439697,
      "learning_rate": 1.874680975180004e-05,
      "loss": 2.6113,
      "step": 734
    },
    {
      "epoch": 0.4838709677419355,
      "grad_norm": 7.0045552253723145,
      "learning_rate": 1.8743466161445823e-05,
      "loss": 2.5424,
      "step": 735
    },
    {
      "epoch": 0.48452929558920343,
      "grad_norm": 5.466546058654785,
      "learning_rate": 1.8740118415572294e-05,
      "loss": 2.6109,
      "step": 736
    },
    {
      "epoch": 0.48518762343647137,
      "grad_norm": 10.003315925598145,
      "learning_rate": 1.8736766515770546e-05,
      "loss": 2.6911,
      "step": 737
    },
    {
      "epoch": 0.4858459512837393,
      "grad_norm": 4.99899959564209,
      "learning_rate": 1.8733410463633638e-05,
      "loss": 2.6039,
      "step": 738
    },
    {
      "epoch": 0.48650427913100724,
      "grad_norm": 9.727168083190918,
      "learning_rate": 1.8730050260756608e-05,
      "loss": 3.3741,
      "step": 739
    },
    {
      "epoch": 0.4871626069782752,
      "grad_norm": 1.300735592842102,
      "learning_rate": 1.8726685908736462e-05,
      "loss": 2.0614,
      "step": 740
    },
    {
      "epoch": 0.4878209348255431,
      "grad_norm": 7.078756332397461,
      "learning_rate": 1.8723317409172178e-05,
      "loss": 2.7173,
      "step": 741
    },
    {
      "epoch": 0.48847926267281105,
      "grad_norm": 6.5929083824157715,
      "learning_rate": 1.8719944763664716e-05,
      "loss": 2.6223,
      "step": 742
    },
    {
      "epoch": 0.489137590520079,
      "grad_norm": 2.990337371826172,
      "learning_rate": 1.871656797381699e-05,
      "loss": 2.1919,
      "step": 743
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 1.1492395401000977,
      "learning_rate": 1.8713187041233896e-05,
      "loss": 2.0666,
      "step": 744
    },
    {
      "epoch": 0.49045424621461486,
      "grad_norm": 7.129739284515381,
      "learning_rate": 1.870980196752229e-05,
      "loss": 2.6309,
      "step": 745
    },
    {
      "epoch": 0.4911125740618828,
      "grad_norm": 4.6081461906433105,
      "learning_rate": 1.8706412754291005e-05,
      "loss": 2.3638,
      "step": 746
    },
    {
      "epoch": 0.49177090190915074,
      "grad_norm": 3.429516315460205,
      "learning_rate": 1.8703019403150836e-05,
      "loss": 2.2524,
      "step": 747
    },
    {
      "epoch": 0.4924292297564187,
      "grad_norm": 11.94173526763916,
      "learning_rate": 1.8699621915714546e-05,
      "loss": 3.7179,
      "step": 748
    },
    {
      "epoch": 0.4930875576036866,
      "grad_norm": 1.463671088218689,
      "learning_rate": 1.869622029359686e-05,
      "loss": 2.0715,
      "step": 749
    },
    {
      "epoch": 0.4937458854509546,
      "grad_norm": 5.647487163543701,
      "learning_rate": 1.8692814538414477e-05,
      "loss": 2.4162,
      "step": 750
    },
    {
      "epoch": 0.49440421329822254,
      "grad_norm": 1.4851677417755127,
      "learning_rate": 1.868940465178605e-05,
      "loss": 2.0618,
      "step": 751
    },
    {
      "epoch": 0.4950625411454905,
      "grad_norm": 1.2585245370864868,
      "learning_rate": 1.8685990635332207e-05,
      "loss": 2.0722,
      "step": 752
    },
    {
      "epoch": 0.4957208689927584,
      "grad_norm": 3.8736624717712402,
      "learning_rate": 1.8682572490675524e-05,
      "loss": 2.4969,
      "step": 753
    },
    {
      "epoch": 0.49637919684002635,
      "grad_norm": 4.616135597229004,
      "learning_rate": 1.8679150219440557e-05,
      "loss": 2.5832,
      "step": 754
    },
    {
      "epoch": 0.4970375246872943,
      "grad_norm": 8.65936279296875,
      "learning_rate": 1.8675723823253803e-05,
      "loss": 2.6832,
      "step": 755
    },
    {
      "epoch": 0.4976958525345622,
      "grad_norm": 6.559420585632324,
      "learning_rate": 1.867229330374374e-05,
      "loss": 2.4424,
      "step": 756
    },
    {
      "epoch": 0.49835418038183016,
      "grad_norm": 3.6060781478881836,
      "learning_rate": 1.8668858662540784e-05,
      "loss": 2.4794,
      "step": 757
    },
    {
      "epoch": 0.4990125082290981,
      "grad_norm": 8.179116249084473,
      "learning_rate": 1.8665419901277335e-05,
      "loss": 2.8399,
      "step": 758
    },
    {
      "epoch": 0.49967083607636603,
      "grad_norm": 5.195646286010742,
      "learning_rate": 1.866197702158773e-05,
      "loss": 2.6054,
      "step": 759
    },
    {
      "epoch": 0.500329163923634,
      "grad_norm": 4.249410152435303,
      "learning_rate": 1.8658530025108272e-05,
      "loss": 2.2587,
      "step": 760
    },
    {
      "epoch": 0.500987491770902,
      "grad_norm": 2.9051365852355957,
      "learning_rate": 1.8655078913477224e-05,
      "loss": 2.1443,
      "step": 761
    },
    {
      "epoch": 0.5016458196181699,
      "grad_norm": 4.313390254974365,
      "learning_rate": 1.8651623688334798e-05,
      "loss": 2.5221,
      "step": 762
    },
    {
      "epoch": 0.5023041474654378,
      "grad_norm": 2.343940258026123,
      "learning_rate": 1.8648164351323158e-05,
      "loss": 2.0935,
      "step": 763
    },
    {
      "epoch": 0.5029624753127058,
      "grad_norm": 6.165236473083496,
      "learning_rate": 1.8644700904086435e-05,
      "loss": 2.5911,
      "step": 764
    },
    {
      "epoch": 0.5036208031599737,
      "grad_norm": 7.7772064208984375,
      "learning_rate": 1.8641233348270706e-05,
      "loss": 2.8301,
      "step": 765
    },
    {
      "epoch": 0.5042791310072416,
      "grad_norm": 2.1288182735443115,
      "learning_rate": 1.8637761685523998e-05,
      "loss": 2.06,
      "step": 766
    },
    {
      "epoch": 0.5049374588545096,
      "grad_norm": 6.719149589538574,
      "learning_rate": 1.8634285917496294e-05,
      "loss": 2.608,
      "step": 767
    },
    {
      "epoch": 0.5055957867017775,
      "grad_norm": 1.6993417739868164,
      "learning_rate": 1.8630806045839527e-05,
      "loss": 2.0571,
      "step": 768
    },
    {
      "epoch": 0.5062541145490455,
      "grad_norm": 6.528232097625732,
      "learning_rate": 1.862732207220758e-05,
      "loss": 2.4026,
      "step": 769
    },
    {
      "epoch": 0.5069124423963134,
      "grad_norm": 2.5469107627868652,
      "learning_rate": 1.862383399825629e-05,
      "loss": 2.0976,
      "step": 770
    },
    {
      "epoch": 0.5075707702435813,
      "grad_norm": 1.6186901330947876,
      "learning_rate": 1.8620341825643435e-05,
      "loss": 2.0693,
      "step": 771
    },
    {
      "epoch": 0.5082290980908493,
      "grad_norm": 4.201664447784424,
      "learning_rate": 1.8616845556028745e-05,
      "loss": 2.4706,
      "step": 772
    },
    {
      "epoch": 0.5088874259381172,
      "grad_norm": 7.508376121520996,
      "learning_rate": 1.8613345191073895e-05,
      "loss": 2.4383,
      "step": 773
    },
    {
      "epoch": 0.5095457537853851,
      "grad_norm": 12.67233657836914,
      "learning_rate": 1.8609840732442514e-05,
      "loss": 2.6765,
      "step": 774
    },
    {
      "epoch": 0.5102040816326531,
      "grad_norm": 7.159793853759766,
      "learning_rate": 1.8606332181800165e-05,
      "loss": 2.6627,
      "step": 775
    },
    {
      "epoch": 0.510862409479921,
      "grad_norm": 1.626363754272461,
      "learning_rate": 1.860281954081436e-05,
      "loss": 2.041,
      "step": 776
    },
    {
      "epoch": 0.511520737327189,
      "grad_norm": 8.732198715209961,
      "learning_rate": 1.8599302811154572e-05,
      "loss": 2.5261,
      "step": 777
    },
    {
      "epoch": 0.5121790651744569,
      "grad_norm": 10.00016975402832,
      "learning_rate": 1.8595781994492185e-05,
      "loss": 2.7123,
      "step": 778
    },
    {
      "epoch": 0.5128373930217248,
      "grad_norm": 10.073387145996094,
      "learning_rate": 1.859225709250055e-05,
      "loss": 2.4189,
      "step": 779
    },
    {
      "epoch": 0.5134957208689928,
      "grad_norm": 8.301525115966797,
      "learning_rate": 1.8588728106854948e-05,
      "loss": 2.5363,
      "step": 780
    },
    {
      "epoch": 0.5141540487162607,
      "grad_norm": 5.423044204711914,
      "learning_rate": 1.8585195039232607e-05,
      "loss": 2.409,
      "step": 781
    },
    {
      "epoch": 0.5148123765635286,
      "grad_norm": 4.318052768707275,
      "learning_rate": 1.8581657891312693e-05,
      "loss": 2.4758,
      "step": 782
    },
    {
      "epoch": 0.5154707044107966,
      "grad_norm": 10.299749374389648,
      "learning_rate": 1.8578116664776314e-05,
      "loss": 2.6256,
      "step": 783
    },
    {
      "epoch": 0.5161290322580645,
      "grad_norm": 10.837515830993652,
      "learning_rate": 1.857457136130651e-05,
      "loss": 2.7646,
      "step": 784
    },
    {
      "epoch": 0.5167873601053324,
      "grad_norm": 3.475219249725342,
      "learning_rate": 1.8571021982588268e-05,
      "loss": 2.122,
      "step": 785
    },
    {
      "epoch": 0.5174456879526004,
      "grad_norm": 3.61643648147583,
      "learning_rate": 1.8567468530308494e-05,
      "loss": 2.1813,
      "step": 786
    },
    {
      "epoch": 0.5181040157998683,
      "grad_norm": 6.06953763961792,
      "learning_rate": 1.8563911006156056e-05,
      "loss": 2.5259,
      "step": 787
    },
    {
      "epoch": 0.5187623436471362,
      "grad_norm": 7.4275665283203125,
      "learning_rate": 1.8560349411821734e-05,
      "loss": 2.5598,
      "step": 788
    },
    {
      "epoch": 0.5194206714944042,
      "grad_norm": 4.868776321411133,
      "learning_rate": 1.8556783748998257e-05,
      "loss": 2.425,
      "step": 789
    },
    {
      "epoch": 0.5200789993416721,
      "grad_norm": 7.3087544441223145,
      "learning_rate": 1.855321401938028e-05,
      "loss": 2.6762,
      "step": 790
    },
    {
      "epoch": 0.5207373271889401,
      "grad_norm": 3.9044792652130127,
      "learning_rate": 1.8549640224664392e-05,
      "loss": 2.4835,
      "step": 791
    },
    {
      "epoch": 0.521395655036208,
      "grad_norm": 6.1233062744140625,
      "learning_rate": 1.854606236654912e-05,
      "loss": 2.6822,
      "step": 792
    },
    {
      "epoch": 0.5220539828834759,
      "grad_norm": 5.827073574066162,
      "learning_rate": 1.8542480446734917e-05,
      "loss": 2.466,
      "step": 793
    },
    {
      "epoch": 0.5227123107307439,
      "grad_norm": 4.382372856140137,
      "learning_rate": 1.853889446692416e-05,
      "loss": 2.3808,
      "step": 794
    },
    {
      "epoch": 0.5233706385780118,
      "grad_norm": 3.4185447692871094,
      "learning_rate": 1.8535304428821173e-05,
      "loss": 2.1576,
      "step": 795
    },
    {
      "epoch": 0.5240289664252797,
      "grad_norm": 3.267268180847168,
      "learning_rate": 1.8531710334132192e-05,
      "loss": 2.2116,
      "step": 796
    },
    {
      "epoch": 0.5246872942725477,
      "grad_norm": 3.8491482734680176,
      "learning_rate": 1.852811218456538e-05,
      "loss": 2.2078,
      "step": 797
    },
    {
      "epoch": 0.5253456221198156,
      "grad_norm": 4.591673374176025,
      "learning_rate": 1.8524509981830855e-05,
      "loss": 2.331,
      "step": 798
    },
    {
      "epoch": 0.5260039499670837,
      "grad_norm": 3.279446840286255,
      "learning_rate": 1.8520903727640623e-05,
      "loss": 2.5113,
      "step": 799
    },
    {
      "epoch": 0.5266622778143516,
      "grad_norm": 5.849329471588135,
      "learning_rate": 1.851729342370864e-05,
      "loss": 2.4541,
      "step": 800
    },
    {
      "epoch": 0.5273206056616195,
      "grad_norm": 2.580895185470581,
      "learning_rate": 1.851367907175078e-05,
      "loss": 2.4209,
      "step": 801
    },
    {
      "epoch": 0.5279789335088875,
      "grad_norm": 2.963371515274048,
      "learning_rate": 1.8510060673484842e-05,
      "loss": 2.1417,
      "step": 802
    },
    {
      "epoch": 0.5286372613561554,
      "grad_norm": 10.293188095092773,
      "learning_rate": 1.850643823063054e-05,
      "loss": 2.8311,
      "step": 803
    },
    {
      "epoch": 0.5292955892034233,
      "grad_norm": 3.8232686519622803,
      "learning_rate": 1.850281174490953e-05,
      "loss": 2.4461,
      "step": 804
    },
    {
      "epoch": 0.5299539170506913,
      "grad_norm": 6.331925392150879,
      "learning_rate": 1.8499181218045368e-05,
      "loss": 2.5037,
      "step": 805
    },
    {
      "epoch": 0.5306122448979592,
      "grad_norm": 4.174190521240234,
      "learning_rate": 1.849554665176354e-05,
      "loss": 2.3645,
      "step": 806
    },
    {
      "epoch": 0.5312705727452272,
      "grad_norm": 2.2722017765045166,
      "learning_rate": 1.8491908047791452e-05,
      "loss": 2.0811,
      "step": 807
    },
    {
      "epoch": 0.5319289005924951,
      "grad_norm": 6.03823184967041,
      "learning_rate": 1.848826540785843e-05,
      "loss": 2.476,
      "step": 808
    },
    {
      "epoch": 0.532587228439763,
      "grad_norm": 3.2506165504455566,
      "learning_rate": 1.848461873369572e-05,
      "loss": 2.0958,
      "step": 809
    },
    {
      "epoch": 0.533245556287031,
      "grad_norm": 2.7868456840515137,
      "learning_rate": 1.8480968027036476e-05,
      "loss": 2.384,
      "step": 810
    },
    {
      "epoch": 0.5339038841342989,
      "grad_norm": 2.107084035873413,
      "learning_rate": 1.8477313289615776e-05,
      "loss": 2.1146,
      "step": 811
    },
    {
      "epoch": 0.5345622119815668,
      "grad_norm": 5.45908260345459,
      "learning_rate": 1.847365452317061e-05,
      "loss": 2.4459,
      "step": 812
    },
    {
      "epoch": 0.5352205398288348,
      "grad_norm": 1.9440175294876099,
      "learning_rate": 1.8469991729439888e-05,
      "loss": 2.0566,
      "step": 813
    },
    {
      "epoch": 0.5358788676761027,
      "grad_norm": 4.953359127044678,
      "learning_rate": 1.8466324910164432e-05,
      "loss": 2.5598,
      "step": 814
    },
    {
      "epoch": 0.5365371955233706,
      "grad_norm": 8.993978500366211,
      "learning_rate": 1.8462654067086977e-05,
      "loss": 2.7227,
      "step": 815
    },
    {
      "epoch": 0.5371955233706386,
      "grad_norm": 2.587611198425293,
      "learning_rate": 1.8458979201952163e-05,
      "loss": 2.1152,
      "step": 816
    },
    {
      "epoch": 0.5378538512179065,
      "grad_norm": 6.698744773864746,
      "learning_rate": 1.8455300316506555e-05,
      "loss": 2.5594,
      "step": 817
    },
    {
      "epoch": 0.5385121790651745,
      "grad_norm": 4.243170738220215,
      "learning_rate": 1.8451617412498616e-05,
      "loss": 2.5051,
      "step": 818
    },
    {
      "epoch": 0.5391705069124424,
      "grad_norm": 2.2343528270721436,
      "learning_rate": 1.8447930491678735e-05,
      "loss": 2.0654,
      "step": 819
    },
    {
      "epoch": 0.5398288347597103,
      "grad_norm": 3.3338472843170166,
      "learning_rate": 1.8444239555799187e-05,
      "loss": 2.238,
      "step": 820
    },
    {
      "epoch": 0.5404871626069783,
      "grad_norm": 2.561542510986328,
      "learning_rate": 1.844054460661418e-05,
      "loss": 2.0597,
      "step": 821
    },
    {
      "epoch": 0.5411454904542462,
      "grad_norm": 4.044174671173096,
      "learning_rate": 1.843684564587981e-05,
      "loss": 2.3905,
      "step": 822
    },
    {
      "epoch": 0.5418038183015141,
      "grad_norm": 6.9761223793029785,
      "learning_rate": 1.843314267535409e-05,
      "loss": 2.5101,
      "step": 823
    },
    {
      "epoch": 0.5424621461487821,
      "grad_norm": 7.028048992156982,
      "learning_rate": 1.8429435696796935e-05,
      "loss": 2.5031,
      "step": 824
    },
    {
      "epoch": 0.54312047399605,
      "grad_norm": 3.7467381954193115,
      "learning_rate": 1.8425724711970167e-05,
      "loss": 2.3953,
      "step": 825
    },
    {
      "epoch": 0.543778801843318,
      "grad_norm": 2.937837600708008,
      "learning_rate": 1.842200972263751e-05,
      "loss": 2.133,
      "step": 826
    },
    {
      "epoch": 0.5444371296905859,
      "grad_norm": 6.6591949462890625,
      "learning_rate": 1.8418290730564594e-05,
      "loss": 2.5626,
      "step": 827
    },
    {
      "epoch": 0.5450954575378538,
      "grad_norm": 1.782350778579712,
      "learning_rate": 1.8414567737518947e-05,
      "loss": 2.0426,
      "step": 828
    },
    {
      "epoch": 0.5457537853851218,
      "grad_norm": 4.1012043952941895,
      "learning_rate": 1.8410840745270002e-05,
      "loss": 2.3565,
      "step": 829
    },
    {
      "epoch": 0.5464121132323897,
      "grad_norm": 1.7325059175491333,
      "learning_rate": 1.840710975558909e-05,
      "loss": 2.0177,
      "step": 830
    },
    {
      "epoch": 0.5470704410796576,
      "grad_norm": 1.9887192249298096,
      "learning_rate": 1.840337477024945e-05,
      "loss": 2.0604,
      "step": 831
    },
    {
      "epoch": 0.5477287689269256,
      "grad_norm": 3.332765579223633,
      "learning_rate": 1.8399635791026205e-05,
      "loss": 2.3272,
      "step": 832
    },
    {
      "epoch": 0.5483870967741935,
      "grad_norm": 5.7197370529174805,
      "learning_rate": 1.839589281969639e-05,
      "loss": 2.473,
      "step": 833
    },
    {
      "epoch": 0.5490454246214614,
      "grad_norm": 1.765673041343689,
      "learning_rate": 1.8392145858038934e-05,
      "loss": 2.0428,
      "step": 834
    },
    {
      "epoch": 0.5497037524687294,
      "grad_norm": 10.97443962097168,
      "learning_rate": 1.838839490783465e-05,
      "loss": 2.9866,
      "step": 835
    },
    {
      "epoch": 0.5503620803159973,
      "grad_norm": 3.344568967819214,
      "learning_rate": 1.8384639970866278e-05,
      "loss": 2.1708,
      "step": 836
    },
    {
      "epoch": 0.5510204081632653,
      "grad_norm": 9.330440521240234,
      "learning_rate": 1.8380881048918406e-05,
      "loss": 2.5843,
      "step": 837
    },
    {
      "epoch": 0.5516787360105333,
      "grad_norm": 1.0687068700790405,
      "learning_rate": 1.8377118143777564e-05,
      "loss": 1.9902,
      "step": 838
    },
    {
      "epoch": 0.5523370638578012,
      "grad_norm": 8.37528133392334,
      "learning_rate": 1.837335125723214e-05,
      "loss": 2.5644,
      "step": 839
    },
    {
      "epoch": 0.5529953917050692,
      "grad_norm": 6.932075500488281,
      "learning_rate": 1.8369580391072435e-05,
      "loss": 2.5241,
      "step": 840
    },
    {
      "epoch": 0.5536537195523371,
      "grad_norm": 3.5181641578674316,
      "learning_rate": 1.836580554709063e-05,
      "loss": 2.0922,
      "step": 841
    },
    {
      "epoch": 0.554312047399605,
      "grad_norm": 1.4771008491516113,
      "learning_rate": 1.8362026727080802e-05,
      "loss": 2.0089,
      "step": 842
    },
    {
      "epoch": 0.554970375246873,
      "grad_norm": 5.345823287963867,
      "learning_rate": 1.8358243932838914e-05,
      "loss": 2.5139,
      "step": 843
    },
    {
      "epoch": 0.5556287030941409,
      "grad_norm": 2.4686081409454346,
      "learning_rate": 1.835445716616282e-05,
      "loss": 2.0249,
      "step": 844
    },
    {
      "epoch": 0.5562870309414089,
      "grad_norm": 108.38745880126953,
      "learning_rate": 1.8350666428852263e-05,
      "loss": 3.2602,
      "step": 845
    },
    {
      "epoch": 0.5569453587886768,
      "grad_norm": 1.743747591972351,
      "learning_rate": 1.8346871722708873e-05,
      "loss": 2.0095,
      "step": 846
    },
    {
      "epoch": 0.5576036866359447,
      "grad_norm": 1.2054840326309204,
      "learning_rate": 1.834307304953616e-05,
      "loss": 1.9821,
      "step": 847
    },
    {
      "epoch": 0.5582620144832127,
      "grad_norm": 1.5055407285690308,
      "learning_rate": 1.8339270411139534e-05,
      "loss": 1.9879,
      "step": 848
    },
    {
      "epoch": 0.5589203423304806,
      "grad_norm": 13.096724510192871,
      "learning_rate": 1.8335463809326276e-05,
      "loss": 3.0337,
      "step": 849
    },
    {
      "epoch": 0.5595786701777485,
      "grad_norm": 2.0042622089385986,
      "learning_rate": 1.8331653245905548e-05,
      "loss": 2.0262,
      "step": 850
    },
    {
      "epoch": 0.5602369980250165,
      "grad_norm": 7.783090114593506,
      "learning_rate": 1.832783872268841e-05,
      "loss": 2.2772,
      "step": 851
    },
    {
      "epoch": 0.5608953258722844,
      "grad_norm": 2.201930046081543,
      "learning_rate": 1.8324020241487796e-05,
      "loss": 1.9922,
      "step": 852
    },
    {
      "epoch": 0.5615536537195523,
      "grad_norm": 5.828827857971191,
      "learning_rate": 1.8320197804118517e-05,
      "loss": 2.5895,
      "step": 853
    },
    {
      "epoch": 0.5622119815668203,
      "grad_norm": 1.4269193410873413,
      "learning_rate": 1.831637141239727e-05,
      "loss": 2.007,
      "step": 854
    },
    {
      "epoch": 0.5628703094140882,
      "grad_norm": 7.439321517944336,
      "learning_rate": 1.8312541068142625e-05,
      "loss": 2.5215,
      "step": 855
    },
    {
      "epoch": 0.5635286372613562,
      "grad_norm": 9.046836853027344,
      "learning_rate": 1.830870677317504e-05,
      "loss": 2.7769,
      "step": 856
    },
    {
      "epoch": 0.5641869651086241,
      "grad_norm": 7.0011186599731445,
      "learning_rate": 1.830486852931685e-05,
      "loss": 2.4348,
      "step": 857
    },
    {
      "epoch": 0.564845292955892,
      "grad_norm": 1.1380858421325684,
      "learning_rate": 1.8301026338392255e-05,
      "loss": 1.9958,
      "step": 858
    },
    {
      "epoch": 0.56550362080316,
      "grad_norm": 12.11836051940918,
      "learning_rate": 1.8297180202227338e-05,
      "loss": 2.8489,
      "step": 859
    },
    {
      "epoch": 0.5661619486504279,
      "grad_norm": 7.937870025634766,
      "learning_rate": 1.829333012265006e-05,
      "loss": 2.5733,
      "step": 860
    },
    {
      "epoch": 0.5668202764976958,
      "grad_norm": 1.3128833770751953,
      "learning_rate": 1.8289476101490257e-05,
      "loss": 1.9909,
      "step": 861
    },
    {
      "epoch": 0.5674786043449638,
      "grad_norm": 1.2912636995315552,
      "learning_rate": 1.8285618140579628e-05,
      "loss": 1.998,
      "step": 862
    },
    {
      "epoch": 0.5681369321922317,
      "grad_norm": 3.235626220703125,
      "learning_rate": 1.8281756241751756e-05,
      "loss": 2.1935,
      "step": 863
    },
    {
      "epoch": 0.5687952600394997,
      "grad_norm": 4.573970794677734,
      "learning_rate": 1.827789040684209e-05,
      "loss": 2.3587,
      "step": 864
    },
    {
      "epoch": 0.5694535878867676,
      "grad_norm": 6.755995750427246,
      "learning_rate": 1.8274020637687954e-05,
      "loss": 2.4181,
      "step": 865
    },
    {
      "epoch": 0.5701119157340355,
      "grad_norm": 2.020007371902466,
      "learning_rate": 1.8270146936128534e-05,
      "loss": 2.0418,
      "step": 866
    },
    {
      "epoch": 0.5707702435813035,
      "grad_norm": 13.411078453063965,
      "learning_rate": 1.826626930400489e-05,
      "loss": 3.5438,
      "step": 867
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 8.36430549621582,
      "learning_rate": 1.826238774315995e-05,
      "loss": 2.6578,
      "step": 868
    },
    {
      "epoch": 0.5720868992758393,
      "grad_norm": 5.624197483062744,
      "learning_rate": 1.825850225543851e-05,
      "loss": 2.1769,
      "step": 869
    },
    {
      "epoch": 0.5727452271231073,
      "grad_norm": 2.4883038997650146,
      "learning_rate": 1.825461284268723e-05,
      "loss": 1.9712,
      "step": 870
    },
    {
      "epoch": 0.5734035549703752,
      "grad_norm": 7.974734306335449,
      "learning_rate": 1.825071950675464e-05,
      "loss": 2.781,
      "step": 871
    },
    {
      "epoch": 0.5740618828176431,
      "grad_norm": 5.421822547912598,
      "learning_rate": 1.8246822249491125e-05,
      "loss": 2.3611,
      "step": 872
    },
    {
      "epoch": 0.5747202106649111,
      "grad_norm": 1.3905093669891357,
      "learning_rate": 1.8242921072748948e-05,
      "loss": 1.9835,
      "step": 873
    },
    {
      "epoch": 0.575378538512179,
      "grad_norm": 6.04810905456543,
      "learning_rate": 1.8239015978382215e-05,
      "loss": 2.2031,
      "step": 874
    },
    {
      "epoch": 0.576036866359447,
      "grad_norm": 4.051810264587402,
      "learning_rate": 1.8235106968246917e-05,
      "loss": 2.0012,
      "step": 875
    },
    {
      "epoch": 0.5766951942067149,
      "grad_norm": 7.026024341583252,
      "learning_rate": 1.823119404420089e-05,
      "loss": 2.7662,
      "step": 876
    },
    {
      "epoch": 0.5773535220539829,
      "grad_norm": 8.77591323852539,
      "learning_rate": 1.8227277208103833e-05,
      "loss": 2.7823,
      "step": 877
    },
    {
      "epoch": 0.5780118499012509,
      "grad_norm": 9.38334846496582,
      "learning_rate": 1.822335646181731e-05,
      "loss": 2.481,
      "step": 878
    },
    {
      "epoch": 0.5786701777485188,
      "grad_norm": 1.4119741916656494,
      "learning_rate": 1.821943180720474e-05,
      "loss": 1.9629,
      "step": 879
    },
    {
      "epoch": 0.5793285055957867,
      "grad_norm": 11.891871452331543,
      "learning_rate": 1.821550324613139e-05,
      "loss": 3.4134,
      "step": 880
    },
    {
      "epoch": 0.5799868334430547,
      "grad_norm": 4.44343376159668,
      "learning_rate": 1.8211570780464406e-05,
      "loss": 2.4078,
      "step": 881
    },
    {
      "epoch": 0.5806451612903226,
      "grad_norm": 1.2465893030166626,
      "learning_rate": 1.8207634412072765e-05,
      "loss": 1.9744,
      "step": 882
    },
    {
      "epoch": 0.5813034891375906,
      "grad_norm": 7.372378349304199,
      "learning_rate": 1.8203694142827316e-05,
      "loss": 2.4892,
      "step": 883
    },
    {
      "epoch": 0.5819618169848585,
      "grad_norm": 2.047065496444702,
      "learning_rate": 1.8199749974600756e-05,
      "loss": 1.9927,
      "step": 884
    },
    {
      "epoch": 0.5826201448321264,
      "grad_norm": 1.2199575901031494,
      "learning_rate": 1.8195801909267635e-05,
      "loss": 1.9582,
      "step": 885
    },
    {
      "epoch": 0.5832784726793944,
      "grad_norm": 5.75964879989624,
      "learning_rate": 1.819184994870436e-05,
      "loss": 2.233,
      "step": 886
    },
    {
      "epoch": 0.5839368005266623,
      "grad_norm": 5.163981914520264,
      "learning_rate": 1.8187894094789173e-05,
      "loss": 2.1641,
      "step": 887
    },
    {
      "epoch": 0.5845951283739302,
      "grad_norm": 4.269145488739014,
      "learning_rate": 1.818393434940219e-05,
      "loss": 2.1058,
      "step": 888
    },
    {
      "epoch": 0.5852534562211982,
      "grad_norm": 1.6170226335525513,
      "learning_rate": 1.8179970714425355e-05,
      "loss": 1.9738,
      "step": 889
    },
    {
      "epoch": 0.5859117840684661,
      "grad_norm": 9.061895370483398,
      "learning_rate": 1.8176003191742477e-05,
      "loss": 2.5644,
      "step": 890
    },
    {
      "epoch": 0.586570111915734,
      "grad_norm": 1.4921884536743164,
      "learning_rate": 1.8172031783239203e-05,
      "loss": 1.959,
      "step": 891
    },
    {
      "epoch": 0.587228439763002,
      "grad_norm": 7.958132743835449,
      "learning_rate": 1.8168056490803036e-05,
      "loss": 2.6923,
      "step": 892
    },
    {
      "epoch": 0.5878867676102699,
      "grad_norm": 1.3514111042022705,
      "learning_rate": 1.816407731632331e-05,
      "loss": 1.9486,
      "step": 893
    },
    {
      "epoch": 0.5885450954575379,
      "grad_norm": 2.276005744934082,
      "learning_rate": 1.8160094261691218e-05,
      "loss": 1.9788,
      "step": 894
    },
    {
      "epoch": 0.5892034233048058,
      "grad_norm": 1.5994439125061035,
      "learning_rate": 1.815610732879979e-05,
      "loss": 1.9541,
      "step": 895
    },
    {
      "epoch": 0.5898617511520737,
      "grad_norm": 7.152420520782471,
      "learning_rate": 1.81521165195439e-05,
      "loss": 2.5656,
      "step": 896
    },
    {
      "epoch": 0.5905200789993417,
      "grad_norm": 12.19804573059082,
      "learning_rate": 1.8148121835820268e-05,
      "loss": 2.9087,
      "step": 897
    },
    {
      "epoch": 0.5911784068466096,
      "grad_norm": 5.432537078857422,
      "learning_rate": 1.814412327952745e-05,
      "loss": 2.3948,
      "step": 898
    },
    {
      "epoch": 0.5918367346938775,
      "grad_norm": 6.259526252746582,
      "learning_rate": 1.814012085256585e-05,
      "loss": 2.3977,
      "step": 899
    },
    {
      "epoch": 0.5924950625411455,
      "grad_norm": 9.68929386138916,
      "learning_rate": 1.8136114556837703e-05,
      "loss": 2.6837,
      "step": 900
    },
    {
      "epoch": 0.5931533903884134,
      "grad_norm": 8.493772506713867,
      "learning_rate": 1.813210439424709e-05,
      "loss": 2.8294,
      "step": 901
    },
    {
      "epoch": 0.5938117182356814,
      "grad_norm": 1.9808562994003296,
      "learning_rate": 1.8128090366699922e-05,
      "loss": 1.9475,
      "step": 902
    },
    {
      "epoch": 0.5944700460829493,
      "grad_norm": 6.570996284484863,
      "learning_rate": 1.8124072476103957e-05,
      "loss": 2.6403,
      "step": 903
    },
    {
      "epoch": 0.5951283739302172,
      "grad_norm": 9.632343292236328,
      "learning_rate": 1.8120050724368776e-05,
      "loss": 2.8139,
      "step": 904
    },
    {
      "epoch": 0.5957867017774852,
      "grad_norm": 3.0703182220458984,
      "learning_rate": 1.811602511340581e-05,
      "loss": 2.4288,
      "step": 905
    },
    {
      "epoch": 0.5964450296247531,
      "grad_norm": 9.256965637207031,
      "learning_rate": 1.8111995645128314e-05,
      "loss": 2.4575,
      "step": 906
    },
    {
      "epoch": 0.597103357472021,
      "grad_norm": 2.058959722518921,
      "learning_rate": 1.810796232145138e-05,
      "loss": 1.9813,
      "step": 907
    },
    {
      "epoch": 0.597761685319289,
      "grad_norm": 3.3685810565948486,
      "learning_rate": 1.810392514429193e-05,
      "loss": 2.1534,
      "step": 908
    },
    {
      "epoch": 0.5984200131665569,
      "grad_norm": 4.831595420837402,
      "learning_rate": 1.8099884115568717e-05,
      "loss": 2.4256,
      "step": 909
    },
    {
      "epoch": 0.5990783410138248,
      "grad_norm": 5.077879905700684,
      "learning_rate": 1.8095839237202328e-05,
      "loss": 2.4146,
      "step": 910
    },
    {
      "epoch": 0.5997366688610928,
      "grad_norm": 5.772167682647705,
      "learning_rate": 1.809179051111518e-05,
      "loss": 2.5243,
      "step": 911
    },
    {
      "epoch": 0.6003949967083607,
      "grad_norm": 5.578439712524414,
      "learning_rate": 1.8087737939231512e-05,
      "loss": 2.4471,
      "step": 912
    },
    {
      "epoch": 0.6010533245556287,
      "grad_norm": 3.252227783203125,
      "learning_rate": 1.8083681523477402e-05,
      "loss": 2.3136,
      "step": 913
    },
    {
      "epoch": 0.6017116524028966,
      "grad_norm": 6.033515453338623,
      "learning_rate": 1.807962126578074e-05,
      "loss": 2.5154,
      "step": 914
    },
    {
      "epoch": 0.6023699802501645,
      "grad_norm": 2.4712014198303223,
      "learning_rate": 1.807555716807126e-05,
      "loss": 2.0198,
      "step": 915
    },
    {
      "epoch": 0.6030283080974326,
      "grad_norm": 4.44794225692749,
      "learning_rate": 1.8071489232280505e-05,
      "loss": 2.2543,
      "step": 916
    },
    {
      "epoch": 0.6036866359447005,
      "grad_norm": 4.409666061401367,
      "learning_rate": 1.806741746034185e-05,
      "loss": 2.3475,
      "step": 917
    },
    {
      "epoch": 0.6043449637919684,
      "grad_norm": 1.9782195091247559,
      "learning_rate": 1.8063341854190487e-05,
      "loss": 1.964,
      "step": 918
    },
    {
      "epoch": 0.6050032916392364,
      "grad_norm": 2.8742666244506836,
      "learning_rate": 1.8059262415763443e-05,
      "loss": 2.3061,
      "step": 919
    },
    {
      "epoch": 0.6056616194865043,
      "grad_norm": 4.291202068328857,
      "learning_rate": 1.8055179146999558e-05,
      "loss": 2.3406,
      "step": 920
    },
    {
      "epoch": 0.6063199473337723,
      "grad_norm": 5.0442304611206055,
      "learning_rate": 1.8051092049839485e-05,
      "loss": 2.2882,
      "step": 921
    },
    {
      "epoch": 0.6069782751810402,
      "grad_norm": 3.480703115463257,
      "learning_rate": 1.8047001126225715e-05,
      "loss": 2.329,
      "step": 922
    },
    {
      "epoch": 0.6076366030283081,
      "grad_norm": 1.7403920888900757,
      "learning_rate": 1.8042906378102534e-05,
      "loss": 1.9894,
      "step": 923
    },
    {
      "epoch": 0.6082949308755761,
      "grad_norm": 3.6421518325805664,
      "learning_rate": 1.803880780741607e-05,
      "loss": 2.2445,
      "step": 924
    },
    {
      "epoch": 0.608953258722844,
      "grad_norm": 2.310438871383667,
      "learning_rate": 1.803470541611425e-05,
      "loss": 1.999,
      "step": 925
    },
    {
      "epoch": 0.6096115865701119,
      "grad_norm": 3.0616416931152344,
      "learning_rate": 1.8030599206146827e-05,
      "loss": 2.0907,
      "step": 926
    },
    {
      "epoch": 0.6102699144173799,
      "grad_norm": 2.509437084197998,
      "learning_rate": 1.802648917946536e-05,
      "loss": 2.0468,
      "step": 927
    },
    {
      "epoch": 0.6109282422646478,
      "grad_norm": 6.096243381500244,
      "learning_rate": 1.8022375338023233e-05,
      "loss": 2.452,
      "step": 928
    },
    {
      "epoch": 0.6115865701119158,
      "grad_norm": 12.171248435974121,
      "learning_rate": 1.8018257683775636e-05,
      "loss": 3.0621,
      "step": 929
    },
    {
      "epoch": 0.6122448979591837,
      "grad_norm": 7.77541446685791,
      "learning_rate": 1.8014136218679566e-05,
      "loss": 2.4756,
      "step": 930
    },
    {
      "epoch": 0.6129032258064516,
      "grad_norm": 3.231736183166504,
      "learning_rate": 1.8010010944693846e-05,
      "loss": 2.1912,
      "step": 931
    },
    {
      "epoch": 0.6135615536537196,
      "grad_norm": 3.1047868728637695,
      "learning_rate": 1.80058818637791e-05,
      "loss": 2.0109,
      "step": 932
    },
    {
      "epoch": 0.6142198815009875,
      "grad_norm": 3.0037360191345215,
      "learning_rate": 1.8001748977897754e-05,
      "loss": 1.9916,
      "step": 933
    },
    {
      "epoch": 0.6148782093482554,
      "grad_norm": 2.67506742477417,
      "learning_rate": 1.7997612289014057e-05,
      "loss": 2.0256,
      "step": 934
    },
    {
      "epoch": 0.6155365371955234,
      "grad_norm": 2.2085509300231934,
      "learning_rate": 1.7993471799094058e-05,
      "loss": 1.9654,
      "step": 935
    },
    {
      "epoch": 0.6161948650427913,
      "grad_norm": 6.760376930236816,
      "learning_rate": 1.798932751010561e-05,
      "loss": 2.3541,
      "step": 936
    },
    {
      "epoch": 0.6168531928900592,
      "grad_norm": 2.8576152324676514,
      "learning_rate": 1.7985179424018382e-05,
      "loss": 2.0236,
      "step": 937
    },
    {
      "epoch": 0.6175115207373272,
      "grad_norm": 6.777705669403076,
      "learning_rate": 1.7981027542803832e-05,
      "loss": 2.4176,
      "step": 938
    },
    {
      "epoch": 0.6181698485845951,
      "grad_norm": 5.598668098449707,
      "learning_rate": 1.797687186843524e-05,
      "loss": 2.226,
      "step": 939
    },
    {
      "epoch": 0.618828176431863,
      "grad_norm": 5.585526466369629,
      "learning_rate": 1.7972712402887668e-05,
      "loss": 2.3244,
      "step": 940
    },
    {
      "epoch": 0.619486504279131,
      "grad_norm": 3.6968424320220947,
      "learning_rate": 1.7968549148138002e-05,
      "loss": 2.2173,
      "step": 941
    },
    {
      "epoch": 0.6201448321263989,
      "grad_norm": 9.282044410705566,
      "learning_rate": 1.796438210616491e-05,
      "loss": 2.4974,
      "step": 942
    },
    {
      "epoch": 0.6208031599736669,
      "grad_norm": 2.2061707973480225,
      "learning_rate": 1.796021127894887e-05,
      "loss": 1.9549,
      "step": 943
    },
    {
      "epoch": 0.6214614878209348,
      "grad_norm": 6.956143856048584,
      "learning_rate": 1.7956036668472155e-05,
      "loss": 2.1753,
      "step": 944
    },
    {
      "epoch": 0.6221198156682027,
      "grad_norm": 4.896655082702637,
      "learning_rate": 1.7951858276718845e-05,
      "loss": 2.3138,
      "step": 945
    },
    {
      "epoch": 0.6227781435154707,
      "grad_norm": 9.644468307495117,
      "learning_rate": 1.7947676105674803e-05,
      "loss": 2.8292,
      "step": 946
    },
    {
      "epoch": 0.6234364713627386,
      "grad_norm": 1.757773756980896,
      "learning_rate": 1.7943490157327698e-05,
      "loss": 1.9386,
      "step": 947
    },
    {
      "epoch": 0.6240947992100065,
      "grad_norm": 5.395407199859619,
      "learning_rate": 1.7939300433666994e-05,
      "loss": 2.1423,
      "step": 948
    },
    {
      "epoch": 0.6247531270572745,
      "grad_norm": 10.507806777954102,
      "learning_rate": 1.7935106936683938e-05,
      "loss": 2.6108,
      "step": 949
    },
    {
      "epoch": 0.6254114549045424,
      "grad_norm": 4.808689594268799,
      "learning_rate": 1.7930909668371587e-05,
      "loss": 2.3135,
      "step": 950
    },
    {
      "epoch": 0.6260697827518104,
      "grad_norm": 2.008699417114258,
      "learning_rate": 1.7926708630724782e-05,
      "loss": 1.9533,
      "step": 951
    },
    {
      "epoch": 0.6267281105990783,
      "grad_norm": 1.542204737663269,
      "learning_rate": 1.7922503825740153e-05,
      "loss": 1.9749,
      "step": 952
    },
    {
      "epoch": 0.6273864384463462,
      "grad_norm": 10.351819038391113,
      "learning_rate": 1.7918295255416125e-05,
      "loss": 2.5979,
      "step": 953
    },
    {
      "epoch": 0.6280447662936142,
      "grad_norm": 7.342312812805176,
      "learning_rate": 1.7914082921752914e-05,
      "loss": 2.6185,
      "step": 954
    },
    {
      "epoch": 0.6287030941408822,
      "grad_norm": 13.37805461883545,
      "learning_rate": 1.7909866826752515e-05,
      "loss": 2.7596,
      "step": 955
    },
    {
      "epoch": 0.6293614219881501,
      "grad_norm": 4.7408881187438965,
      "learning_rate": 1.7905646972418726e-05,
      "loss": 2.3335,
      "step": 956
    },
    {
      "epoch": 0.6300197498354181,
      "grad_norm": 14.76578140258789,
      "learning_rate": 1.7901423360757113e-05,
      "loss": 3.4297,
      "step": 957
    },
    {
      "epoch": 0.630678077682686,
      "grad_norm": 1.7333685159683228,
      "learning_rate": 1.7897195993775044e-05,
      "loss": 1.9426,
      "step": 958
    },
    {
      "epoch": 0.631336405529954,
      "grad_norm": 2.338345766067505,
      "learning_rate": 1.7892964873481667e-05,
      "loss": 1.9792,
      "step": 959
    },
    {
      "epoch": 0.6319947333772219,
      "grad_norm": 7.6304521560668945,
      "learning_rate": 1.7888730001887907e-05,
      "loss": 2.5184,
      "step": 960
    },
    {
      "epoch": 0.6326530612244898,
      "grad_norm": 6.4355387687683105,
      "learning_rate": 1.788449138100648e-05,
      "loss": 2.3459,
      "step": 961
    },
    {
      "epoch": 0.6333113890717578,
      "grad_norm": 1.5967422723770142,
      "learning_rate": 1.788024901285188e-05,
      "loss": 1.9313,
      "step": 962
    },
    {
      "epoch": 0.6339697169190257,
      "grad_norm": 4.0794243812561035,
      "learning_rate": 1.787600289944039e-05,
      "loss": 2.3169,
      "step": 963
    },
    {
      "epoch": 0.6346280447662936,
      "grad_norm": 1.8496912717819214,
      "learning_rate": 1.7871753042790057e-05,
      "loss": 1.9607,
      "step": 964
    },
    {
      "epoch": 0.6352863726135616,
      "grad_norm": 1.7467225790023804,
      "learning_rate": 1.786749944492072e-05,
      "loss": 1.9309,
      "step": 965
    },
    {
      "epoch": 0.6359447004608295,
      "grad_norm": 4.522286891937256,
      "learning_rate": 1.7863242107853996e-05,
      "loss": 2.2212,
      "step": 966
    },
    {
      "epoch": 0.6366030283080975,
      "grad_norm": 10.68502140045166,
      "learning_rate": 1.7858981033613267e-05,
      "loss": 2.65,
      "step": 967
    },
    {
      "epoch": 0.6372613561553654,
      "grad_norm": 12.80484676361084,
      "learning_rate": 1.7854716224223706e-05,
      "loss": 2.8378,
      "step": 968
    },
    {
      "epoch": 0.6379196840026333,
      "grad_norm": 3.982398748397827,
      "learning_rate": 1.7850447681712255e-05,
      "loss": 2.2932,
      "step": 969
    },
    {
      "epoch": 0.6385780118499013,
      "grad_norm": 11.560300827026367,
      "learning_rate": 1.784617540810763e-05,
      "loss": 2.8146,
      "step": 970
    },
    {
      "epoch": 0.6392363396971692,
      "grad_norm": 2.0090155601501465,
      "learning_rate": 1.7841899405440316e-05,
      "loss": 1.945,
      "step": 971
    },
    {
      "epoch": 0.6398946675444371,
      "grad_norm": 1.2468451261520386,
      "learning_rate": 1.783761967574258e-05,
      "loss": 1.912,
      "step": 972
    },
    {
      "epoch": 0.6405529953917051,
      "grad_norm": 3.0348031520843506,
      "learning_rate": 1.7833336221048453e-05,
      "loss": 1.9934,
      "step": 973
    },
    {
      "epoch": 0.641211323238973,
      "grad_norm": 4.28928804397583,
      "learning_rate": 1.7829049043393737e-05,
      "loss": 2.1046,
      "step": 974
    },
    {
      "epoch": 0.6418696510862409,
      "grad_norm": 4.849205493927002,
      "learning_rate": 1.7824758144816004e-05,
      "loss": 2.1895,
      "step": 975
    },
    {
      "epoch": 0.6425279789335089,
      "grad_norm": 1.3882566690444946,
      "learning_rate": 1.7820463527354604e-05,
      "loss": 1.9009,
      "step": 976
    },
    {
      "epoch": 0.6431863067807768,
      "grad_norm": 1.2461247444152832,
      "learning_rate": 1.781616519305063e-05,
      "loss": 1.9082,
      "step": 977
    },
    {
      "epoch": 0.6438446346280448,
      "grad_norm": 12.470179557800293,
      "learning_rate": 1.7811863143946975e-05,
      "loss": 2.4546,
      "step": 978
    },
    {
      "epoch": 0.6445029624753127,
      "grad_norm": 8.273674964904785,
      "learning_rate": 1.780755738208827e-05,
      "loss": 2.4628,
      "step": 979
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 10.699238777160645,
      "learning_rate": 1.780324790952092e-05,
      "loss": 2.5697,
      "step": 980
    },
    {
      "epoch": 0.6458196181698486,
      "grad_norm": 2.032536506652832,
      "learning_rate": 1.7798934728293097e-05,
      "loss": 1.9189,
      "step": 981
    },
    {
      "epoch": 0.6464779460171165,
      "grad_norm": 6.840184688568115,
      "learning_rate": 1.779461784045473e-05,
      "loss": 2.272,
      "step": 982
    },
    {
      "epoch": 0.6471362738643844,
      "grad_norm": 5.420124530792236,
      "learning_rate": 1.7790297248057515e-05,
      "loss": 2.2214,
      "step": 983
    },
    {
      "epoch": 0.6477946017116524,
      "grad_norm": 12.216971397399902,
      "learning_rate": 1.7785972953154905e-05,
      "loss": 2.723,
      "step": 984
    },
    {
      "epoch": 0.6484529295589203,
      "grad_norm": 9.515377044677734,
      "learning_rate": 1.7781644957802112e-05,
      "loss": 2.4655,
      "step": 985
    },
    {
      "epoch": 0.6491112574061882,
      "grad_norm": 3.8667566776275635,
      "learning_rate": 1.7777313264056113e-05,
      "loss": 2.0097,
      "step": 986
    },
    {
      "epoch": 0.6497695852534562,
      "grad_norm": 10.497723579406738,
      "learning_rate": 1.7772977873975633e-05,
      "loss": 2.507,
      "step": 987
    },
    {
      "epoch": 0.6504279131007241,
      "grad_norm": 1.440851092338562,
      "learning_rate": 1.776863878962116e-05,
      "loss": 1.8785,
      "step": 988
    },
    {
      "epoch": 0.6510862409479921,
      "grad_norm": 8.966391563415527,
      "learning_rate": 1.7764296013054937e-05,
      "loss": 2.2897,
      "step": 989
    },
    {
      "epoch": 0.65174456879526,
      "grad_norm": 2.0068647861480713,
      "learning_rate": 1.7759949546340967e-05,
      "loss": 1.9136,
      "step": 990
    },
    {
      "epoch": 0.6524028966425279,
      "grad_norm": 15.264918327331543,
      "learning_rate": 1.7755599391544992e-05,
      "loss": 2.698,
      "step": 991
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 7.728435516357422,
      "learning_rate": 1.775124555073452e-05,
      "loss": 2.503,
      "step": 992
    },
    {
      "epoch": 0.6537195523370638,
      "grad_norm": 1.3027788400650024,
      "learning_rate": 1.7746888025978807e-05,
      "loss": 1.8776,
      "step": 993
    },
    {
      "epoch": 0.6543778801843319,
      "grad_norm": 12.582983016967773,
      "learning_rate": 1.774252681934886e-05,
      "loss": 2.7251,
      "step": 994
    },
    {
      "epoch": 0.6550362080315998,
      "grad_norm": 1.2210098505020142,
      "learning_rate": 1.773816193291744e-05,
      "loss": 1.8933,
      "step": 995
    },
    {
      "epoch": 0.6556945358788677,
      "grad_norm": 17.348506927490234,
      "learning_rate": 1.7733793368759043e-05,
      "loss": 3.1283,
      "step": 996
    },
    {
      "epoch": 0.6563528637261357,
      "grad_norm": 17.267229080200195,
      "learning_rate": 1.7729421128949932e-05,
      "loss": 3.0701,
      "step": 997
    },
    {
      "epoch": 0.6570111915734036,
      "grad_norm": 1.400286316871643,
      "learning_rate": 1.7725045215568103e-05,
      "loss": 1.8826,
      "step": 998
    },
    {
      "epoch": 0.6576695194206715,
      "grad_norm": 8.920939445495605,
      "learning_rate": 1.77206656306933e-05,
      "loss": 2.2576,
      "step": 999
    },
    {
      "epoch": 0.6583278472679395,
      "grad_norm": 2.2805418968200684,
      "learning_rate": 1.771628237640702e-05,
      "loss": 1.9313,
      "step": 1000
    },
    {
      "epoch": 0.6589861751152074,
      "grad_norm": 9.716161727905273,
      "learning_rate": 1.7711895454792496e-05,
      "loss": 2.3769,
      "step": 1001
    },
    {
      "epoch": 0.6596445029624753,
      "grad_norm": 7.120400428771973,
      "learning_rate": 1.7707504867934705e-05,
      "loss": 2.0305,
      "step": 1002
    },
    {
      "epoch": 0.6603028308097433,
      "grad_norm": 1.9347506761550903,
      "learning_rate": 1.7703110617920368e-05,
      "loss": 1.916,
      "step": 1003
    },
    {
      "epoch": 0.6609611586570112,
      "grad_norm": 8.707727432250977,
      "learning_rate": 1.7698712706837946e-05,
      "loss": 2.3622,
      "step": 1004
    },
    {
      "epoch": 0.6616194865042792,
      "grad_norm": 14.360508918762207,
      "learning_rate": 1.769431113677764e-05,
      "loss": 2.7347,
      "step": 1005
    },
    {
      "epoch": 0.6622778143515471,
      "grad_norm": 5.407968997955322,
      "learning_rate": 1.7689905909831396e-05,
      "loss": 2.0816,
      "step": 1006
    },
    {
      "epoch": 0.662936142198815,
      "grad_norm": 1.698626160621643,
      "learning_rate": 1.7685497028092885e-05,
      "loss": 1.9072,
      "step": 1007
    },
    {
      "epoch": 0.663594470046083,
      "grad_norm": 2.956859827041626,
      "learning_rate": 1.7681084493657526e-05,
      "loss": 1.8944,
      "step": 1008
    },
    {
      "epoch": 0.6642527978933509,
      "grad_norm": 10.902826309204102,
      "learning_rate": 1.7676668308622467e-05,
      "loss": 2.5806,
      "step": 1009
    },
    {
      "epoch": 0.6649111257406188,
      "grad_norm": 9.658365249633789,
      "learning_rate": 1.76722484750866e-05,
      "loss": 2.5775,
      "step": 1010
    },
    {
      "epoch": 0.6655694535878868,
      "grad_norm": 1.924260139465332,
      "learning_rate": 1.766782499515054e-05,
      "loss": 1.9102,
      "step": 1011
    },
    {
      "epoch": 0.6662277814351547,
      "grad_norm": 7.772851467132568,
      "learning_rate": 1.766339787091664e-05,
      "loss": 2.2276,
      "step": 1012
    },
    {
      "epoch": 0.6668861092824226,
      "grad_norm": 13.36499309539795,
      "learning_rate": 1.765896710448899e-05,
      "loss": 2.9443,
      "step": 1013
    },
    {
      "epoch": 0.6675444371296906,
      "grad_norm": 5.853291988372803,
      "learning_rate": 1.7654532697973406e-05,
      "loss": 2.3517,
      "step": 1014
    },
    {
      "epoch": 0.6682027649769585,
      "grad_norm": 2.156719446182251,
      "learning_rate": 1.765009465347743e-05,
      "loss": 1.8847,
      "step": 1015
    },
    {
      "epoch": 0.6688610928242265,
      "grad_norm": 8.23852252960205,
      "learning_rate": 1.7645652973110347e-05,
      "loss": 2.3882,
      "step": 1016
    },
    {
      "epoch": 0.6695194206714944,
      "grad_norm": 8.346841812133789,
      "learning_rate": 1.7641207658983148e-05,
      "loss": 2.2867,
      "step": 1017
    },
    {
      "epoch": 0.6701777485187623,
      "grad_norm": 2.9178361892700195,
      "learning_rate": 1.7636758713208574e-05,
      "loss": 1.9474,
      "step": 1018
    },
    {
      "epoch": 0.6708360763660303,
      "grad_norm": 1.8403109312057495,
      "learning_rate": 1.7632306137901074e-05,
      "loss": 1.9046,
      "step": 1019
    },
    {
      "epoch": 0.6714944042132982,
      "grad_norm": 7.436522006988525,
      "learning_rate": 1.7627849935176837e-05,
      "loss": 2.5965,
      "step": 1020
    },
    {
      "epoch": 0.6721527320605661,
      "grad_norm": 16.099834442138672,
      "learning_rate": 1.762339010715376e-05,
      "loss": 2.8294,
      "step": 1021
    },
    {
      "epoch": 0.6728110599078341,
      "grad_norm": 11.08159351348877,
      "learning_rate": 1.7618926655951482e-05,
      "loss": 2.6386,
      "step": 1022
    },
    {
      "epoch": 0.673469387755102,
      "grad_norm": 2.4480905532836914,
      "learning_rate": 1.7614459583691346e-05,
      "loss": 1.9283,
      "step": 1023
    },
    {
      "epoch": 0.67412771560237,
      "grad_norm": 15.329627990722656,
      "learning_rate": 1.7609988892496427e-05,
      "loss": 2.9431,
      "step": 1024
    },
    {
      "epoch": 0.6747860434496379,
      "grad_norm": 7.851742744445801,
      "learning_rate": 1.7605514584491512e-05,
      "loss": 2.5078,
      "step": 1025
    },
    {
      "epoch": 0.6754443712969058,
      "grad_norm": 8.98287296295166,
      "learning_rate": 1.760103666180312e-05,
      "loss": 2.1916,
      "step": 1026
    },
    {
      "epoch": 0.6761026991441738,
      "grad_norm": 8.529905319213867,
      "learning_rate": 1.7596555126559472e-05,
      "loss": 2.3318,
      "step": 1027
    },
    {
      "epoch": 0.6767610269914417,
      "grad_norm": 10.138269424438477,
      "learning_rate": 1.7592069980890515e-05,
      "loss": 2.3121,
      "step": 1028
    },
    {
      "epoch": 0.6774193548387096,
      "grad_norm": 1.338371992111206,
      "learning_rate": 1.758758122692791e-05,
      "loss": 1.8691,
      "step": 1029
    },
    {
      "epoch": 0.6780776826859776,
      "grad_norm": 1.7142457962036133,
      "learning_rate": 1.758308886680504e-05,
      "loss": 1.8952,
      "step": 1030
    },
    {
      "epoch": 0.6787360105332455,
      "grad_norm": 2.0566468238830566,
      "learning_rate": 1.7578592902656987e-05,
      "loss": 1.9144,
      "step": 1031
    },
    {
      "epoch": 0.6793943383805134,
      "grad_norm": 6.768328666687012,
      "learning_rate": 1.7574093336620555e-05,
      "loss": 2.431,
      "step": 1032
    },
    {
      "epoch": 0.6800526662277815,
      "grad_norm": 1.3486272096633911,
      "learning_rate": 1.7569590170834265e-05,
      "loss": 1.8697,
      "step": 1033
    },
    {
      "epoch": 0.6807109940750494,
      "grad_norm": 2.040402412414551,
      "learning_rate": 1.7565083407438334e-05,
      "loss": 1.8809,
      "step": 1034
    },
    {
      "epoch": 0.6813693219223174,
      "grad_norm": 2.294369697570801,
      "learning_rate": 1.7560573048574706e-05,
      "loss": 1.9105,
      "step": 1035
    },
    {
      "epoch": 0.6820276497695853,
      "grad_norm": 1.5625032186508179,
      "learning_rate": 1.7556059096387024e-05,
      "loss": 1.8631,
      "step": 1036
    },
    {
      "epoch": 0.6826859776168532,
      "grad_norm": 2.060364007949829,
      "learning_rate": 1.755154155302064e-05,
      "loss": 1.897,
      "step": 1037
    },
    {
      "epoch": 0.6833443054641212,
      "grad_norm": 1.2617413997650146,
      "learning_rate": 1.7547020420622604e-05,
      "loss": 1.8435,
      "step": 1038
    },
    {
      "epoch": 0.6840026333113891,
      "grad_norm": 11.608118057250977,
      "learning_rate": 1.7542495701341695e-05,
      "loss": 2.9063,
      "step": 1039
    },
    {
      "epoch": 0.684660961158657,
      "grad_norm": 7.729325771331787,
      "learning_rate": 1.7537967397328373e-05,
      "loss": 2.2923,
      "step": 1040
    },
    {
      "epoch": 0.685319289005925,
      "grad_norm": 10.337297439575195,
      "learning_rate": 1.753343551073482e-05,
      "loss": 2.5932,
      "step": 1041
    },
    {
      "epoch": 0.6859776168531929,
      "grad_norm": 14.050496101379395,
      "learning_rate": 1.75289000437149e-05,
      "loss": 2.7731,
      "step": 1042
    },
    {
      "epoch": 0.6866359447004609,
      "grad_norm": 13.106147766113281,
      "learning_rate": 1.7524360998424198e-05,
      "loss": 2.5101,
      "step": 1043
    },
    {
      "epoch": 0.6872942725477288,
      "grad_norm": 12.644553184509277,
      "learning_rate": 1.7519818377019993e-05,
      "loss": 2.8786,
      "step": 1044
    },
    {
      "epoch": 0.6879526003949967,
      "grad_norm": 6.997643947601318,
      "learning_rate": 1.7515272181661264e-05,
      "loss": 2.1873,
      "step": 1045
    },
    {
      "epoch": 0.6886109282422647,
      "grad_norm": 7.2416768074035645,
      "learning_rate": 1.751072241450868e-05,
      "loss": 2.3109,
      "step": 1046
    },
    {
      "epoch": 0.6892692560895326,
      "grad_norm": 8.151946067810059,
      "learning_rate": 1.7506169077724628e-05,
      "loss": 2.5405,
      "step": 1047
    },
    {
      "epoch": 0.6899275839368005,
      "grad_norm": 11.393950462341309,
      "learning_rate": 1.7501612173473162e-05,
      "loss": 2.6714,
      "step": 1048
    },
    {
      "epoch": 0.6905859117840685,
      "grad_norm": 9.731785774230957,
      "learning_rate": 1.7497051703920065e-05,
      "loss": 2.4388,
      "step": 1049
    },
    {
      "epoch": 0.6912442396313364,
      "grad_norm": 7.712296962738037,
      "learning_rate": 1.7492487671232784e-05,
      "loss": 2.3897,
      "step": 1050
    },
    {
      "epoch": 0.6919025674786043,
      "grad_norm": 14.615619659423828,
      "learning_rate": 1.7487920077580483e-05,
      "loss": 2.6232,
      "step": 1051
    },
    {
      "epoch": 0.6925608953258723,
      "grad_norm": 6.100160121917725,
      "learning_rate": 1.7483348925134003e-05,
      "loss": 2.2405,
      "step": 1052
    },
    {
      "epoch": 0.6932192231731402,
      "grad_norm": 14.421526908874512,
      "learning_rate": 1.7478774216065882e-05,
      "loss": 2.6971,
      "step": 1053
    },
    {
      "epoch": 0.6938775510204082,
      "grad_norm": 5.642997741699219,
      "learning_rate": 1.7474195952550355e-05,
      "loss": 2.2366,
      "step": 1054
    },
    {
      "epoch": 0.6945358788676761,
      "grad_norm": 6.669240951538086,
      "learning_rate": 1.746961413676333e-05,
      "loss": 2.2563,
      "step": 1055
    },
    {
      "epoch": 0.695194206714944,
      "grad_norm": 2.1676628589630127,
      "learning_rate": 1.746502877088242e-05,
      "loss": 1.8821,
      "step": 1056
    },
    {
      "epoch": 0.695852534562212,
      "grad_norm": 3.540645122528076,
      "learning_rate": 1.7460439857086916e-05,
      "loss": 1.9299,
      "step": 1057
    },
    {
      "epoch": 0.6965108624094799,
      "grad_norm": 1.1562204360961914,
      "learning_rate": 1.7455847397557795e-05,
      "loss": 1.8402,
      "step": 1058
    },
    {
      "epoch": 0.6971691902567478,
      "grad_norm": 3.882063150405884,
      "learning_rate": 1.7451251394477727e-05,
      "loss": 1.9343,
      "step": 1059
    },
    {
      "epoch": 0.6978275181040158,
      "grad_norm": 11.36523151397705,
      "learning_rate": 1.7446651850031057e-05,
      "loss": 2.5084,
      "step": 1060
    },
    {
      "epoch": 0.6984858459512837,
      "grad_norm": 11.194286346435547,
      "learning_rate": 1.7442048766403815e-05,
      "loss": 2.558,
      "step": 1061
    },
    {
      "epoch": 0.6991441737985516,
      "grad_norm": 9.022624969482422,
      "learning_rate": 1.743744214578372e-05,
      "loss": 2.5285,
      "step": 1062
    },
    {
      "epoch": 0.6998025016458196,
      "grad_norm": 6.631631851196289,
      "learning_rate": 1.7432831990360163e-05,
      "loss": 2.2315,
      "step": 1063
    },
    {
      "epoch": 0.7004608294930875,
      "grad_norm": 2.2130846977233887,
      "learning_rate": 1.7428218302324222e-05,
      "loss": 1.8606,
      "step": 1064
    },
    {
      "epoch": 0.7011191573403555,
      "grad_norm": 11.292425155639648,
      "learning_rate": 1.742360108386865e-05,
      "loss": 2.5666,
      "step": 1065
    },
    {
      "epoch": 0.7017774851876234,
      "grad_norm": 6.156101226806641,
      "learning_rate": 1.7418980337187874e-05,
      "loss": 2.3562,
      "step": 1066
    },
    {
      "epoch": 0.7024358130348913,
      "grad_norm": 8.099854469299316,
      "learning_rate": 1.741435606447801e-05,
      "loss": 2.4287,
      "step": 1067
    },
    {
      "epoch": 0.7030941408821593,
      "grad_norm": 10.572564125061035,
      "learning_rate": 1.740972826793684e-05,
      "loss": 2.5672,
      "step": 1068
    },
    {
      "epoch": 0.7037524687294272,
      "grad_norm": 9.777559280395508,
      "learning_rate": 1.7405096949763823e-05,
      "loss": 2.56,
      "step": 1069
    },
    {
      "epoch": 0.7044107965766951,
      "grad_norm": 10.038469314575195,
      "learning_rate": 1.7400462112160088e-05,
      "loss": 2.497,
      "step": 1070
    },
    {
      "epoch": 0.7050691244239631,
      "grad_norm": 2.746868133544922,
      "learning_rate": 1.7395823757328445e-05,
      "loss": 1.9013,
      "step": 1071
    },
    {
      "epoch": 0.7057274522712311,
      "grad_norm": 8.011308670043945,
      "learning_rate": 1.7391181887473375e-05,
      "loss": 2.4445,
      "step": 1072
    },
    {
      "epoch": 0.7063857801184991,
      "grad_norm": 5.520678520202637,
      "learning_rate": 1.7386536504801017e-05,
      "loss": 2.3858,
      "step": 1073
    },
    {
      "epoch": 0.707044107965767,
      "grad_norm": 1.5928492546081543,
      "learning_rate": 1.7381887611519195e-05,
      "loss": 1.8636,
      "step": 1074
    },
    {
      "epoch": 0.7077024358130349,
      "grad_norm": 7.5838775634765625,
      "learning_rate": 1.7377235209837392e-05,
      "loss": 2.3592,
      "step": 1075
    },
    {
      "epoch": 0.7083607636603029,
      "grad_norm": 4.552892684936523,
      "learning_rate": 1.7372579301966764e-05,
      "loss": 2.3933,
      "step": 1076
    },
    {
      "epoch": 0.7090190915075708,
      "grad_norm": 5.625459671020508,
      "learning_rate": 1.736791989012013e-05,
      "loss": 1.9985,
      "step": 1077
    },
    {
      "epoch": 0.7096774193548387,
      "grad_norm": 4.998255252838135,
      "learning_rate": 1.7363256976511972e-05,
      "loss": 2.2343,
      "step": 1078
    },
    {
      "epoch": 0.7103357472021067,
      "grad_norm": 7.80155611038208,
      "learning_rate": 1.7358590563358445e-05,
      "loss": 2.2338,
      "step": 1079
    },
    {
      "epoch": 0.7109940750493746,
      "grad_norm": 9.387686729431152,
      "learning_rate": 1.7353920652877358e-05,
      "loss": 2.5138,
      "step": 1080
    },
    {
      "epoch": 0.7116524028966426,
      "grad_norm": 11.113899230957031,
      "learning_rate": 1.7349247247288188e-05,
      "loss": 2.4896,
      "step": 1081
    },
    {
      "epoch": 0.7123107307439105,
      "grad_norm": 1.7045053243637085,
      "learning_rate": 1.7344570348812065e-05,
      "loss": 1.8507,
      "step": 1082
    },
    {
      "epoch": 0.7129690585911784,
      "grad_norm": 4.767375946044922,
      "learning_rate": 1.733988995967179e-05,
      "loss": 2.3136,
      "step": 1083
    },
    {
      "epoch": 0.7136273864384464,
      "grad_norm": 2.0678670406341553,
      "learning_rate": 1.7335206082091824e-05,
      "loss": 1.898,
      "step": 1084
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 1.6077075004577637,
      "learning_rate": 1.7330518718298263e-05,
      "loss": 1.8675,
      "step": 1085
    },
    {
      "epoch": 0.7149440421329822,
      "grad_norm": 4.988500118255615,
      "learning_rate": 1.7325827870518894e-05,
      "loss": 2.1616,
      "step": 1086
    },
    {
      "epoch": 0.7156023699802502,
      "grad_norm": 8.743474960327148,
      "learning_rate": 1.7321133540983134e-05,
      "loss": 2.2869,
      "step": 1087
    },
    {
      "epoch": 0.7162606978275181,
      "grad_norm": 7.535640239715576,
      "learning_rate": 1.731643573192207e-05,
      "loss": 2.3666,
      "step": 1088
    },
    {
      "epoch": 0.716919025674786,
      "grad_norm": 13.663622856140137,
      "learning_rate": 1.7311734445568426e-05,
      "loss": 2.5852,
      "step": 1089
    },
    {
      "epoch": 0.717577353522054,
      "grad_norm": 6.6460652351379395,
      "learning_rate": 1.7307029684156597e-05,
      "loss": 2.2087,
      "step": 1090
    },
    {
      "epoch": 0.7182356813693219,
      "grad_norm": 3.5186095237731934,
      "learning_rate": 1.730232144992262e-05,
      "loss": 2.2053,
      "step": 1091
    },
    {
      "epoch": 0.7188940092165899,
      "grad_norm": 7.316995620727539,
      "learning_rate": 1.7297609745104184e-05,
      "loss": 2.2984,
      "step": 1092
    },
    {
      "epoch": 0.7195523370638578,
      "grad_norm": 2.424400568008423,
      "learning_rate": 1.7292894571940622e-05,
      "loss": 1.9127,
      "step": 1093
    },
    {
      "epoch": 0.7202106649111257,
      "grad_norm": 3.523817777633667,
      "learning_rate": 1.728817593267293e-05,
      "loss": 1.9241,
      "step": 1094
    },
    {
      "epoch": 0.7208689927583937,
      "grad_norm": 6.019672870635986,
      "learning_rate": 1.728345382954374e-05,
      "loss": 2.318,
      "step": 1095
    },
    {
      "epoch": 0.7215273206056616,
      "grad_norm": 5.199909687042236,
      "learning_rate": 1.7278728264797327e-05,
      "loss": 2.1435,
      "step": 1096
    },
    {
      "epoch": 0.7221856484529295,
      "grad_norm": 5.011926174163818,
      "learning_rate": 1.727399924067962e-05,
      "loss": 2.2241,
      "step": 1097
    },
    {
      "epoch": 0.7228439763001975,
      "grad_norm": 14.350464820861816,
      "learning_rate": 1.726926675943819e-05,
      "loss": 2.7697,
      "step": 1098
    },
    {
      "epoch": 0.7235023041474654,
      "grad_norm": 2.9588897228240967,
      "learning_rate": 1.726453082332225e-05,
      "loss": 1.8856,
      "step": 1099
    },
    {
      "epoch": 0.7241606319947334,
      "grad_norm": 2.519742012023926,
      "learning_rate": 1.725979143458265e-05,
      "loss": 1.8748,
      "step": 1100
    },
    {
      "epoch": 0.7248189598420013,
      "grad_norm": 2.951962947845459,
      "learning_rate": 1.725504859547189e-05,
      "loss": 1.8792,
      "step": 1101
    },
    {
      "epoch": 0.7254772876892692,
      "grad_norm": 19.00468635559082,
      "learning_rate": 1.725030230824411e-05,
      "loss": 3.083,
      "step": 1102
    },
    {
      "epoch": 0.7261356155365372,
      "grad_norm": 9.383840560913086,
      "learning_rate": 1.724555257515508e-05,
      "loss": 2.449,
      "step": 1103
    },
    {
      "epoch": 0.7267939433838051,
      "grad_norm": 7.498232841491699,
      "learning_rate": 1.7240799398462208e-05,
      "loss": 2.2764,
      "step": 1104
    },
    {
      "epoch": 0.727452271231073,
      "grad_norm": 6.927722454071045,
      "learning_rate": 1.723604278042455e-05,
      "loss": 2.5861,
      "step": 1105
    },
    {
      "epoch": 0.728110599078341,
      "grad_norm": 6.923252582550049,
      "learning_rate": 1.7231282723302785e-05,
      "loss": 2.2569,
      "step": 1106
    },
    {
      "epoch": 0.7287689269256089,
      "grad_norm": 3.1432530879974365,
      "learning_rate": 1.7226519229359236e-05,
      "loss": 1.916,
      "step": 1107
    },
    {
      "epoch": 0.7294272547728768,
      "grad_norm": 12.713295936584473,
      "learning_rate": 1.7221752300857857e-05,
      "loss": 2.4638,
      "step": 1108
    },
    {
      "epoch": 0.7300855826201448,
      "grad_norm": 6.470331192016602,
      "learning_rate": 1.7216981940064225e-05,
      "loss": 2.2191,
      "step": 1109
    },
    {
      "epoch": 0.7307439104674127,
      "grad_norm": 3.4514124393463135,
      "learning_rate": 1.7212208149245565e-05,
      "loss": 1.9617,
      "step": 1110
    },
    {
      "epoch": 0.7314022383146808,
      "grad_norm": 7.91758394241333,
      "learning_rate": 1.7207430930670718e-05,
      "loss": 2.3987,
      "step": 1111
    },
    {
      "epoch": 0.7320605661619487,
      "grad_norm": 3.9601006507873535,
      "learning_rate": 1.720265028661016e-05,
      "loss": 1.9738,
      "step": 1112
    },
    {
      "epoch": 0.7327188940092166,
      "grad_norm": 6.682672023773193,
      "learning_rate": 1.719786621933599e-05,
      "loss": 2.1816,
      "step": 1113
    },
    {
      "epoch": 0.7333772218564846,
      "grad_norm": 1.7660797834396362,
      "learning_rate": 1.719307873112195e-05,
      "loss": 1.8532,
      "step": 1114
    },
    {
      "epoch": 0.7340355497037525,
      "grad_norm": 2.353288173675537,
      "learning_rate": 1.718828782424338e-05,
      "loss": 1.8605,
      "step": 1115
    },
    {
      "epoch": 0.7346938775510204,
      "grad_norm": 5.060277938842773,
      "learning_rate": 1.7183493500977277e-05,
      "loss": 2.231,
      "step": 1116
    },
    {
      "epoch": 0.7353522053982884,
      "grad_norm": 8.902576446533203,
      "learning_rate": 1.7178695763602237e-05,
      "loss": 2.233,
      "step": 1117
    },
    {
      "epoch": 0.7360105332455563,
      "grad_norm": 10.84935188293457,
      "learning_rate": 1.717389461439849e-05,
      "loss": 2.9007,
      "step": 1118
    },
    {
      "epoch": 0.7366688610928243,
      "grad_norm": 6.66533899307251,
      "learning_rate": 1.716909005564788e-05,
      "loss": 2.2396,
      "step": 1119
    },
    {
      "epoch": 0.7373271889400922,
      "grad_norm": 19.660118103027344,
      "learning_rate": 1.7164282089633885e-05,
      "loss": 2.9413,
      "step": 1120
    },
    {
      "epoch": 0.7379855167873601,
      "grad_norm": 17.06163215637207,
      "learning_rate": 1.7159470718641586e-05,
      "loss": 2.8517,
      "step": 1121
    },
    {
      "epoch": 0.7386438446346281,
      "grad_norm": 3.578010320663452,
      "learning_rate": 1.715465594495769e-05,
      "loss": 2.2132,
      "step": 1122
    },
    {
      "epoch": 0.739302172481896,
      "grad_norm": 17.41290283203125,
      "learning_rate": 1.714983777087053e-05,
      "loss": 3.1275,
      "step": 1123
    },
    {
      "epoch": 0.7399605003291639,
      "grad_norm": 11.498736381530762,
      "learning_rate": 1.714501619867004e-05,
      "loss": 2.7406,
      "step": 1124
    },
    {
      "epoch": 0.7406188281764319,
      "grad_norm": 7.366503715515137,
      "learning_rate": 1.714019123064777e-05,
      "loss": 2.2519,
      "step": 1125
    },
    {
      "epoch": 0.7412771560236998,
      "grad_norm": 7.247001647949219,
      "learning_rate": 1.71353628690969e-05,
      "loss": 2.1343,
      "step": 1126
    },
    {
      "epoch": 0.7419354838709677,
      "grad_norm": 6.227552890777588,
      "learning_rate": 1.7130531116312202e-05,
      "loss": 2.2229,
      "step": 1127
    },
    {
      "epoch": 0.7425938117182357,
      "grad_norm": 3.235908031463623,
      "learning_rate": 1.712569597459008e-05,
      "loss": 1.8635,
      "step": 1128
    },
    {
      "epoch": 0.7432521395655036,
      "grad_norm": 6.453622817993164,
      "learning_rate": 1.712085744622853e-05,
      "loss": 2.2142,
      "step": 1129
    },
    {
      "epoch": 0.7439104674127716,
      "grad_norm": 9.814129829406738,
      "learning_rate": 1.711601553352717e-05,
      "loss": 2.4083,
      "step": 1130
    },
    {
      "epoch": 0.7445687952600395,
      "grad_norm": 6.619205951690674,
      "learning_rate": 1.7111170238787225e-05,
      "loss": 2.3197,
      "step": 1131
    },
    {
      "epoch": 0.7452271231073074,
      "grad_norm": 8.112165451049805,
      "learning_rate": 1.710632156431152e-05,
      "loss": 2.313,
      "step": 1132
    },
    {
      "epoch": 0.7458854509545754,
      "grad_norm": 17.549110412597656,
      "learning_rate": 1.710146951240449e-05,
      "loss": 3.1157,
      "step": 1133
    },
    {
      "epoch": 0.7465437788018433,
      "grad_norm": 6.1387152671813965,
      "learning_rate": 1.7096614085372186e-05,
      "loss": 2.1963,
      "step": 1134
    },
    {
      "epoch": 0.7472021066491112,
      "grad_norm": 2.8119168281555176,
      "learning_rate": 1.709175528552224e-05,
      "loss": 1.8968,
      "step": 1135
    },
    {
      "epoch": 0.7478604344963792,
      "grad_norm": 5.624706745147705,
      "learning_rate": 1.7086893115163912e-05,
      "loss": 1.9144,
      "step": 1136
    },
    {
      "epoch": 0.7485187623436471,
      "grad_norm": 6.608479976654053,
      "learning_rate": 1.708202757660805e-05,
      "loss": 1.9962,
      "step": 1137
    },
    {
      "epoch": 0.749177090190915,
      "grad_norm": 5.812685489654541,
      "learning_rate": 1.70771586721671e-05,
      "loss": 2.1632,
      "step": 1138
    },
    {
      "epoch": 0.749835418038183,
      "grad_norm": 4.90731954574585,
      "learning_rate": 1.7072286404155117e-05,
      "loss": 2.2034,
      "step": 1139
    },
    {
      "epoch": 0.7504937458854509,
      "grad_norm": 10.817073822021484,
      "learning_rate": 1.706741077488775e-05,
      "loss": 2.3612,
      "step": 1140
    },
    {
      "epoch": 0.7511520737327189,
      "grad_norm": 8.190372467041016,
      "learning_rate": 1.7062531786682246e-05,
      "loss": 2.2473,
      "step": 1141
    },
    {
      "epoch": 0.7518104015799868,
      "grad_norm": 7.365073204040527,
      "learning_rate": 1.7057649441857452e-05,
      "loss": 2.1653,
      "step": 1142
    },
    {
      "epoch": 0.7524687294272547,
      "grad_norm": 2.5008723735809326,
      "learning_rate": 1.70527637427338e-05,
      "loss": 1.8853,
      "step": 1143
    },
    {
      "epoch": 0.7531270572745227,
      "grad_norm": 8.768770217895508,
      "learning_rate": 1.7047874691633324e-05,
      "loss": 2.257,
      "step": 1144
    },
    {
      "epoch": 0.7537853851217906,
      "grad_norm": 1.4598876237869263,
      "learning_rate": 1.7042982290879656e-05,
      "loss": 1.8343,
      "step": 1145
    },
    {
      "epoch": 0.7544437129690585,
      "grad_norm": 14.061434745788574,
      "learning_rate": 1.7038086542798012e-05,
      "loss": 2.5448,
      "step": 1146
    },
    {
      "epoch": 0.7551020408163265,
      "grad_norm": 11.72074031829834,
      "learning_rate": 1.7033187449715195e-05,
      "loss": 2.4414,
      "step": 1147
    },
    {
      "epoch": 0.7557603686635944,
      "grad_norm": 18.348047256469727,
      "learning_rate": 1.7028285013959614e-05,
      "loss": 2.5772,
      "step": 1148
    },
    {
      "epoch": 0.7564186965108625,
      "grad_norm": 2.0434470176696777,
      "learning_rate": 1.7023379237861247e-05,
      "loss": 1.8653,
      "step": 1149
    },
    {
      "epoch": 0.7570770243581304,
      "grad_norm": 6.162224292755127,
      "learning_rate": 1.7018470123751677e-05,
      "loss": 2.2812,
      "step": 1150
    },
    {
      "epoch": 0.7577353522053983,
      "grad_norm": 9.817126274108887,
      "learning_rate": 1.701355767396406e-05,
      "loss": 2.2477,
      "step": 1151
    },
    {
      "epoch": 0.7583936800526663,
      "grad_norm": 8.156517028808594,
      "learning_rate": 1.7008641890833144e-05,
      "loss": 2.0345,
      "step": 1152
    },
    {
      "epoch": 0.7590520078999342,
      "grad_norm": 14.496346473693848,
      "learning_rate": 1.7003722776695255e-05,
      "loss": 2.6165,
      "step": 1153
    },
    {
      "epoch": 0.7597103357472021,
      "grad_norm": 16.193645477294922,
      "learning_rate": 1.6998800333888317e-05,
      "loss": 2.3591,
      "step": 1154
    },
    {
      "epoch": 0.7603686635944701,
      "grad_norm": 5.570457458496094,
      "learning_rate": 1.6993874564751824e-05,
      "loss": 2.1176,
      "step": 1155
    },
    {
      "epoch": 0.761026991441738,
      "grad_norm": 1.9251495599746704,
      "learning_rate": 1.6988945471626846e-05,
      "loss": 1.8527,
      "step": 1156
    },
    {
      "epoch": 0.761685319289006,
      "grad_norm": 1.2560304403305054,
      "learning_rate": 1.6984013056856047e-05,
      "loss": 1.8309,
      "step": 1157
    },
    {
      "epoch": 0.7623436471362739,
      "grad_norm": 12.743181228637695,
      "learning_rate": 1.697907732278366e-05,
      "loss": 2.3381,
      "step": 1158
    },
    {
      "epoch": 0.7630019749835418,
      "grad_norm": 5.181886196136475,
      "learning_rate": 1.69741382717555e-05,
      "loss": 2.1533,
      "step": 1159
    },
    {
      "epoch": 0.7636603028308098,
      "grad_norm": 7.210178375244141,
      "learning_rate": 1.6969195906118953e-05,
      "loss": 2.1655,
      "step": 1160
    },
    {
      "epoch": 0.7643186306780777,
      "grad_norm": 9.532925605773926,
      "learning_rate": 1.6964250228222988e-05,
      "loss": 2.2422,
      "step": 1161
    },
    {
      "epoch": 0.7649769585253456,
      "grad_norm": 8.051274299621582,
      "learning_rate": 1.6959301240418144e-05,
      "loss": 2.1614,
      "step": 1162
    },
    {
      "epoch": 0.7656352863726136,
      "grad_norm": 1.6642910242080688,
      "learning_rate": 1.695434894505653e-05,
      "loss": 1.8304,
      "step": 1163
    },
    {
      "epoch": 0.7662936142198815,
      "grad_norm": 3.3031816482543945,
      "learning_rate": 1.6949393344491835e-05,
      "loss": 1.8983,
      "step": 1164
    },
    {
      "epoch": 0.7669519420671495,
      "grad_norm": 7.276454925537109,
      "learning_rate": 1.694443444107931e-05,
      "loss": 2.4678,
      "step": 1165
    },
    {
      "epoch": 0.7676102699144174,
      "grad_norm": 4.821865081787109,
      "learning_rate": 1.6939472237175777e-05,
      "loss": 1.8924,
      "step": 1166
    },
    {
      "epoch": 0.7682685977616853,
      "grad_norm": 10.288305282592773,
      "learning_rate": 1.693450673513964e-05,
      "loss": 2.474,
      "step": 1167
    },
    {
      "epoch": 0.7689269256089533,
      "grad_norm": 10.101271629333496,
      "learning_rate": 1.6929537937330846e-05,
      "loss": 2.3284,
      "step": 1168
    },
    {
      "epoch": 0.7695852534562212,
      "grad_norm": 1.657135248184204,
      "learning_rate": 1.6924565846110933e-05,
      "loss": 1.84,
      "step": 1169
    },
    {
      "epoch": 0.7702435813034891,
      "grad_norm": 16.558971405029297,
      "learning_rate": 1.6919590463842993e-05,
      "loss": 2.93,
      "step": 1170
    },
    {
      "epoch": 0.7709019091507571,
      "grad_norm": 2.2432100772857666,
      "learning_rate": 1.691461179289168e-05,
      "loss": 1.8348,
      "step": 1171
    },
    {
      "epoch": 0.771560236998025,
      "grad_norm": 12.572839736938477,
      "learning_rate": 1.6909629835623214e-05,
      "loss": 2.6274,
      "step": 1172
    },
    {
      "epoch": 0.7722185648452929,
      "grad_norm": 2.3807997703552246,
      "learning_rate": 1.690464459440538e-05,
      "loss": 1.8482,
      "step": 1173
    },
    {
      "epoch": 0.7728768926925609,
      "grad_norm": 6.3616461753845215,
      "learning_rate": 1.6899656071607515e-05,
      "loss": 2.1188,
      "step": 1174
    },
    {
      "epoch": 0.7735352205398288,
      "grad_norm": 3.377803087234497,
      "learning_rate": 1.6894664269600522e-05,
      "loss": 2.0261,
      "step": 1175
    },
    {
      "epoch": 0.7741935483870968,
      "grad_norm": 6.107266426086426,
      "learning_rate": 1.688966919075687e-05,
      "loss": 2.1608,
      "step": 1176
    },
    {
      "epoch": 0.7748518762343647,
      "grad_norm": 16.213624954223633,
      "learning_rate": 1.6884670837450564e-05,
      "loss": 2.5943,
      "step": 1177
    },
    {
      "epoch": 0.7755102040816326,
      "grad_norm": 7.723318099975586,
      "learning_rate": 1.6879669212057187e-05,
      "loss": 2.1904,
      "step": 1178
    },
    {
      "epoch": 0.7761685319289006,
      "grad_norm": 15.78182601928711,
      "learning_rate": 1.687466431695387e-05,
      "loss": 2.7218,
      "step": 1179
    },
    {
      "epoch": 0.7768268597761685,
      "grad_norm": 9.49792194366455,
      "learning_rate": 1.686965615451929e-05,
      "loss": 2.5592,
      "step": 1180
    },
    {
      "epoch": 0.7774851876234364,
      "grad_norm": 7.909852981567383,
      "learning_rate": 1.6864644727133687e-05,
      "loss": 2.0069,
      "step": 1181
    },
    {
      "epoch": 0.7781435154707044,
      "grad_norm": 5.265049934387207,
      "learning_rate": 1.685963003717885e-05,
      "loss": 2.0763,
      "step": 1182
    },
    {
      "epoch": 0.7788018433179723,
      "grad_norm": 3.725571870803833,
      "learning_rate": 1.6854612087038113e-05,
      "loss": 1.898,
      "step": 1183
    },
    {
      "epoch": 0.7794601711652402,
      "grad_norm": 8.67688274383545,
      "learning_rate": 1.6849590879096368e-05,
      "loss": 2.262,
      "step": 1184
    },
    {
      "epoch": 0.7801184990125082,
      "grad_norm": 4.9453558921813965,
      "learning_rate": 1.6844566415740052e-05,
      "loss": 1.9053,
      "step": 1185
    },
    {
      "epoch": 0.7807768268597761,
      "grad_norm": 6.14415168762207,
      "learning_rate": 1.6839538699357145e-05,
      "loss": 2.1178,
      "step": 1186
    },
    {
      "epoch": 0.781435154707044,
      "grad_norm": 6.451635360717773,
      "learning_rate": 1.683450773233718e-05,
      "loss": 2.3256,
      "step": 1187
    },
    {
      "epoch": 0.7820934825543121,
      "grad_norm": 5.615160942077637,
      "learning_rate": 1.682947351707123e-05,
      "loss": 2.0971,
      "step": 1188
    },
    {
      "epoch": 0.78275181040158,
      "grad_norm": 1.677597165107727,
      "learning_rate": 1.6824436055951916e-05,
      "loss": 1.8137,
      "step": 1189
    },
    {
      "epoch": 0.783410138248848,
      "grad_norm": 4.415327072143555,
      "learning_rate": 1.6819395351373397e-05,
      "loss": 1.9493,
      "step": 1190
    },
    {
      "epoch": 0.7840684660961159,
      "grad_norm": 7.965149879455566,
      "learning_rate": 1.6814351405731376e-05,
      "loss": 2.2286,
      "step": 1191
    },
    {
      "epoch": 0.7847267939433838,
      "grad_norm": 16.230751037597656,
      "learning_rate": 1.6809304221423092e-05,
      "loss": 2.5842,
      "step": 1192
    },
    {
      "epoch": 0.7853851217906518,
      "grad_norm": 11.586287498474121,
      "learning_rate": 1.6804253800847337e-05,
      "loss": 2.5648,
      "step": 1193
    },
    {
      "epoch": 0.7860434496379197,
      "grad_norm": 9.063299179077148,
      "learning_rate": 1.6799200146404424e-05,
      "loss": 1.9131,
      "step": 1194
    },
    {
      "epoch": 0.7867017774851877,
      "grad_norm": 9.426728248596191,
      "learning_rate": 1.679414326049621e-05,
      "loss": 2.3227,
      "step": 1195
    },
    {
      "epoch": 0.7873601053324556,
      "grad_norm": 17.16297721862793,
      "learning_rate": 1.6789083145526096e-05,
      "loss": 2.424,
      "step": 1196
    },
    {
      "epoch": 0.7880184331797235,
      "grad_norm": 10.331828117370605,
      "learning_rate": 1.6784019803899e-05,
      "loss": 2.2514,
      "step": 1197
    },
    {
      "epoch": 0.7886767610269915,
      "grad_norm": 5.0326762199401855,
      "learning_rate": 1.6778953238021386e-05,
      "loss": 1.891,
      "step": 1198
    },
    {
      "epoch": 0.7893350888742594,
      "grad_norm": 4.242033958435059,
      "learning_rate": 1.677388345030125e-05,
      "loss": 2.0451,
      "step": 1199
    },
    {
      "epoch": 0.7899934167215273,
      "grad_norm": 2.3699004650115967,
      "learning_rate": 1.6768810443148117e-05,
      "loss": 1.8472,
      "step": 1200
    },
    {
      "epoch": 0.7906517445687953,
      "grad_norm": 7.381030082702637,
      "learning_rate": 1.6763734218973046e-05,
      "loss": 2.2088,
      "step": 1201
    },
    {
      "epoch": 0.7913100724160632,
      "grad_norm": 20.587427139282227,
      "learning_rate": 1.6758654780188614e-05,
      "loss": 2.742,
      "step": 1202
    },
    {
      "epoch": 0.7919684002633312,
      "grad_norm": 10.642446517944336,
      "learning_rate": 1.6753572129208935e-05,
      "loss": 2.2474,
      "step": 1203
    },
    {
      "epoch": 0.7926267281105991,
      "grad_norm": 4.150710105895996,
      "learning_rate": 1.674848626844965e-05,
      "loss": 2.0476,
      "step": 1204
    },
    {
      "epoch": 0.793285055957867,
      "grad_norm": 7.814537048339844,
      "learning_rate": 1.6743397200327918e-05,
      "loss": 2.1508,
      "step": 1205
    },
    {
      "epoch": 0.793943383805135,
      "grad_norm": 4.357695579528809,
      "learning_rate": 1.673830492726243e-05,
      "loss": 1.8761,
      "step": 1206
    },
    {
      "epoch": 0.7946017116524029,
      "grad_norm": 1.824907660484314,
      "learning_rate": 1.6733209451673403e-05,
      "loss": 1.8274,
      "step": 1207
    },
    {
      "epoch": 0.7952600394996708,
      "grad_norm": 11.350985527038574,
      "learning_rate": 1.6728110775982564e-05,
      "loss": 2.2345,
      "step": 1208
    },
    {
      "epoch": 0.7959183673469388,
      "grad_norm": 1.5702013969421387,
      "learning_rate": 1.672300890261317e-05,
      "loss": 1.7983,
      "step": 1209
    },
    {
      "epoch": 0.7965766951942067,
      "grad_norm": 12.300870895385742,
      "learning_rate": 1.6717903833989995e-05,
      "loss": 2.3848,
      "step": 1210
    },
    {
      "epoch": 0.7972350230414746,
      "grad_norm": 4.578351974487305,
      "learning_rate": 1.6712795572539334e-05,
      "loss": 2.102,
      "step": 1211
    },
    {
      "epoch": 0.7978933508887426,
      "grad_norm": 2.8991546630859375,
      "learning_rate": 1.6707684120688994e-05,
      "loss": 1.8608,
      "step": 1212
    },
    {
      "epoch": 0.7985516787360105,
      "grad_norm": 18.505348205566406,
      "learning_rate": 1.6702569480868306e-05,
      "loss": 2.632,
      "step": 1213
    },
    {
      "epoch": 0.7992100065832785,
      "grad_norm": 8.283305168151855,
      "learning_rate": 1.6697451655508108e-05,
      "loss": 2.1999,
      "step": 1214
    },
    {
      "epoch": 0.7998683344305464,
      "grad_norm": 5.057090759277344,
      "learning_rate": 1.669233064704076e-05,
      "loss": 2.0656,
      "step": 1215
    },
    {
      "epoch": 0.8005266622778143,
      "grad_norm": 1.3375494480133057,
      "learning_rate": 1.6687206457900124e-05,
      "loss": 1.8039,
      "step": 1216
    },
    {
      "epoch": 0.8011849901250823,
      "grad_norm": 5.439504146575928,
      "learning_rate": 1.668207909052159e-05,
      "loss": 1.9432,
      "step": 1217
    },
    {
      "epoch": 0.8018433179723502,
      "grad_norm": 8.814346313476562,
      "learning_rate": 1.667694854734204e-05,
      "loss": 2.0495,
      "step": 1218
    },
    {
      "epoch": 0.8025016458196181,
      "grad_norm": 14.935669898986816,
      "learning_rate": 1.6671814830799883e-05,
      "loss": 2.7978,
      "step": 1219
    },
    {
      "epoch": 0.8031599736668861,
      "grad_norm": 1.3340660333633423,
      "learning_rate": 1.6666677943335018e-05,
      "loss": 1.8115,
      "step": 1220
    },
    {
      "epoch": 0.803818301514154,
      "grad_norm": 12.181251525878906,
      "learning_rate": 1.666153788738887e-05,
      "loss": 2.3255,
      "step": 1221
    },
    {
      "epoch": 0.804476629361422,
      "grad_norm": 10.986095428466797,
      "learning_rate": 1.6656394665404356e-05,
      "loss": 2.426,
      "step": 1222
    },
    {
      "epoch": 0.8051349572086899,
      "grad_norm": 3.111196994781494,
      "learning_rate": 1.6651248279825898e-05,
      "loss": 1.8729,
      "step": 1223
    },
    {
      "epoch": 0.8057932850559578,
      "grad_norm": 19.980424880981445,
      "learning_rate": 1.6646098733099434e-05,
      "loss": 3.136,
      "step": 1224
    },
    {
      "epoch": 0.8064516129032258,
      "grad_norm": 1.6472244262695312,
      "learning_rate": 1.6640946027672395e-05,
      "loss": 1.8028,
      "step": 1225
    },
    {
      "epoch": 0.8071099407504937,
      "grad_norm": 3.0366969108581543,
      "learning_rate": 1.6635790165993704e-05,
      "loss": 1.8605,
      "step": 1226
    },
    {
      "epoch": 0.8077682685977617,
      "grad_norm": 3.9918696880340576,
      "learning_rate": 1.663063115051381e-05,
      "loss": 2.0053,
      "step": 1227
    },
    {
      "epoch": 0.8084265964450297,
      "grad_norm": 14.811579704284668,
      "learning_rate": 1.6625468983684635e-05,
      "loss": 2.4604,
      "step": 1228
    },
    {
      "epoch": 0.8090849242922976,
      "grad_norm": 13.094000816345215,
      "learning_rate": 1.662030366795961e-05,
      "loss": 2.5236,
      "step": 1229
    },
    {
      "epoch": 0.8097432521395656,
      "grad_norm": 12.084430694580078,
      "learning_rate": 1.6615135205793667e-05,
      "loss": 2.4007,
      "step": 1230
    },
    {
      "epoch": 0.8104015799868335,
      "grad_norm": 6.736455917358398,
      "learning_rate": 1.6609963599643227e-05,
      "loss": 2.2052,
      "step": 1231
    },
    {
      "epoch": 0.8110599078341014,
      "grad_norm": 1.1529974937438965,
      "learning_rate": 1.66047888519662e-05,
      "loss": 1.7883,
      "step": 1232
    },
    {
      "epoch": 0.8117182356813694,
      "grad_norm": 6.289834022521973,
      "learning_rate": 1.6599610965222002e-05,
      "loss": 2.3302,
      "step": 1233
    },
    {
      "epoch": 0.8123765635286373,
      "grad_norm": 18.22488784790039,
      "learning_rate": 1.6594429941871534e-05,
      "loss": 2.7432,
      "step": 1234
    },
    {
      "epoch": 0.8130348913759052,
      "grad_norm": 1.9265811443328857,
      "learning_rate": 1.6589245784377185e-05,
      "loss": 1.8106,
      "step": 1235
    },
    {
      "epoch": 0.8136932192231732,
      "grad_norm": 13.686070442199707,
      "learning_rate": 1.6584058495202838e-05,
      "loss": 2.49,
      "step": 1236
    },
    {
      "epoch": 0.8143515470704411,
      "grad_norm": 13.93908405303955,
      "learning_rate": 1.6578868076813864e-05,
      "loss": 2.4692,
      "step": 1237
    },
    {
      "epoch": 0.815009874917709,
      "grad_norm": 9.299089431762695,
      "learning_rate": 1.6573674531677122e-05,
      "loss": 2.1396,
      "step": 1238
    },
    {
      "epoch": 0.815668202764977,
      "grad_norm": 2.7444121837615967,
      "learning_rate": 1.6568477862260952e-05,
      "loss": 1.8172,
      "step": 1239
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 2.687033176422119,
      "learning_rate": 1.6563278071035182e-05,
      "loss": 1.797,
      "step": 1240
    },
    {
      "epoch": 0.8169848584595129,
      "grad_norm": 9.411139488220215,
      "learning_rate": 1.655807516047113e-05,
      "loss": 2.3736,
      "step": 1241
    },
    {
      "epoch": 0.8176431863067808,
      "grad_norm": 2.911965847015381,
      "learning_rate": 1.6552869133041582e-05,
      "loss": 1.8084,
      "step": 1242
    },
    {
      "epoch": 0.8183015141540487,
      "grad_norm": 4.615494251251221,
      "learning_rate": 1.654765999122082e-05,
      "loss": 2.0115,
      "step": 1243
    },
    {
      "epoch": 0.8189598420013167,
      "grad_norm": 9.704861640930176,
      "learning_rate": 1.65424477374846e-05,
      "loss": 2.3002,
      "step": 1244
    },
    {
      "epoch": 0.8196181698485846,
      "grad_norm": 8.458075523376465,
      "learning_rate": 1.6537232374310147e-05,
      "loss": 2.1658,
      "step": 1245
    },
    {
      "epoch": 0.8202764976958525,
      "grad_norm": 2.4034793376922607,
      "learning_rate": 1.653201390417619e-05,
      "loss": 1.8261,
      "step": 1246
    },
    {
      "epoch": 0.8209348255431205,
      "grad_norm": 7.19534158706665,
      "learning_rate": 1.652679232956291e-05,
      "loss": 2.1298,
      "step": 1247
    },
    {
      "epoch": 0.8215931533903884,
      "grad_norm": 7.4878010749816895,
      "learning_rate": 1.6521567652951967e-05,
      "loss": 2.1493,
      "step": 1248
    },
    {
      "epoch": 0.8222514812376563,
      "grad_norm": 6.323754787445068,
      "learning_rate": 1.651633987682651e-05,
      "loss": 2.1319,
      "step": 1249
    },
    {
      "epoch": 0.8229098090849243,
      "grad_norm": 15.180346488952637,
      "learning_rate": 1.651110900367115e-05,
      "loss": 2.7469,
      "step": 1250
    },
    {
      "epoch": 0.8235681369321922,
      "grad_norm": 2.4721717834472656,
      "learning_rate": 1.6505875035971967e-05,
      "loss": 1.8038,
      "step": 1251
    },
    {
      "epoch": 0.8242264647794602,
      "grad_norm": 9.129358291625977,
      "learning_rate": 1.6500637976216517e-05,
      "loss": 2.2059,
      "step": 1252
    },
    {
      "epoch": 0.8248847926267281,
      "grad_norm": 10.801142692565918,
      "learning_rate": 1.6495397826893827e-05,
      "loss": 2.3193,
      "step": 1253
    },
    {
      "epoch": 0.825543120473996,
      "grad_norm": 2.1272568702697754,
      "learning_rate": 1.6490154590494383e-05,
      "loss": 1.8091,
      "step": 1254
    },
    {
      "epoch": 0.826201448321264,
      "grad_norm": 7.113810062408447,
      "learning_rate": 1.6484908269510157e-05,
      "loss": 2.1653,
      "step": 1255
    },
    {
      "epoch": 0.8268597761685319,
      "grad_norm": 16.657611846923828,
      "learning_rate": 1.6479658866434568e-05,
      "loss": 2.851,
      "step": 1256
    },
    {
      "epoch": 0.8275181040157998,
      "grad_norm": 11.36074161529541,
      "learning_rate": 1.6474406383762506e-05,
      "loss": 2.0226,
      "step": 1257
    },
    {
      "epoch": 0.8281764318630678,
      "grad_norm": 3.8170528411865234,
      "learning_rate": 1.646915082399033e-05,
      "loss": 2.0077,
      "step": 1258
    },
    {
      "epoch": 0.8288347597103357,
      "grad_norm": 5.9732346534729,
      "learning_rate": 1.6463892189615852e-05,
      "loss": 1.9402,
      "step": 1259
    },
    {
      "epoch": 0.8294930875576036,
      "grad_norm": 3.568079710006714,
      "learning_rate": 1.6458630483138354e-05,
      "loss": 1.8565,
      "step": 1260
    },
    {
      "epoch": 0.8301514154048716,
      "grad_norm": 4.225167751312256,
      "learning_rate": 1.6453365707058576e-05,
      "loss": 1.8703,
      "step": 1261
    },
    {
      "epoch": 0.8308097432521395,
      "grad_norm": 2.1292247772216797,
      "learning_rate": 1.6448097863878716e-05,
      "loss": 1.8145,
      "step": 1262
    },
    {
      "epoch": 0.8314680710994075,
      "grad_norm": 11.344755172729492,
      "learning_rate": 1.6442826956102424e-05,
      "loss": 2.1603,
      "step": 1263
    },
    {
      "epoch": 0.8321263989466754,
      "grad_norm": 18.938230514526367,
      "learning_rate": 1.6437552986234815e-05,
      "loss": 2.5607,
      "step": 1264
    },
    {
      "epoch": 0.8327847267939433,
      "grad_norm": 1.9499716758728027,
      "learning_rate": 1.6432275956782454e-05,
      "loss": 1.7891,
      "step": 1265
    },
    {
      "epoch": 0.8334430546412114,
      "grad_norm": 22.060218811035156,
      "learning_rate": 1.6426995870253363e-05,
      "loss": 2.2925,
      "step": 1266
    },
    {
      "epoch": 0.8341013824884793,
      "grad_norm": 12.744303703308105,
      "learning_rate": 1.642171272915702e-05,
      "loss": 2.5177,
      "step": 1267
    },
    {
      "epoch": 0.8347597103357473,
      "grad_norm": 12.26612663269043,
      "learning_rate": 1.6416426536004345e-05,
      "loss": 2.5603,
      "step": 1268
    },
    {
      "epoch": 0.8354180381830152,
      "grad_norm": 3.151435613632202,
      "learning_rate": 1.6411137293307713e-05,
      "loss": 1.891,
      "step": 1269
    },
    {
      "epoch": 0.8360763660302831,
      "grad_norm": 13.440962791442871,
      "learning_rate": 1.640584500358096e-05,
      "loss": 2.3685,
      "step": 1270
    },
    {
      "epoch": 0.8367346938775511,
      "grad_norm": 20.966230392456055,
      "learning_rate": 1.640054966933935e-05,
      "loss": 3.1105,
      "step": 1271
    },
    {
      "epoch": 0.837393021724819,
      "grad_norm": 14.166476249694824,
      "learning_rate": 1.639525129309961e-05,
      "loss": 2.7872,
      "step": 1272
    },
    {
      "epoch": 0.8380513495720869,
      "grad_norm": 2.529747486114502,
      "learning_rate": 1.63899498773799e-05,
      "loss": 1.7978,
      "step": 1273
    },
    {
      "epoch": 0.8387096774193549,
      "grad_norm": 14.171334266662598,
      "learning_rate": 1.6384645424699835e-05,
      "loss": 2.4831,
      "step": 1274
    },
    {
      "epoch": 0.8393680052666228,
      "grad_norm": 13.713183403015137,
      "learning_rate": 1.6379337937580473e-05,
      "loss": 2.4954,
      "step": 1275
    },
    {
      "epoch": 0.8400263331138907,
      "grad_norm": 7.769886016845703,
      "learning_rate": 1.6374027418544305e-05,
      "loss": 2.0589,
      "step": 1276
    },
    {
      "epoch": 0.8406846609611587,
      "grad_norm": 8.492121696472168,
      "learning_rate": 1.6368713870115273e-05,
      "loss": 2.1916,
      "step": 1277
    },
    {
      "epoch": 0.8413429888084266,
      "grad_norm": 2.97164249420166,
      "learning_rate": 1.636339729481875e-05,
      "loss": 1.8322,
      "step": 1278
    },
    {
      "epoch": 0.8420013166556946,
      "grad_norm": 2.3994014263153076,
      "learning_rate": 1.635807769518156e-05,
      "loss": 1.8133,
      "step": 1279
    },
    {
      "epoch": 0.8426596445029625,
      "grad_norm": 1.605833649635315,
      "learning_rate": 1.6352755073731942e-05,
      "loss": 1.7941,
      "step": 1280
    },
    {
      "epoch": 0.8433179723502304,
      "grad_norm": 3.3182458877563477,
      "learning_rate": 1.6347429432999602e-05,
      "loss": 1.8065,
      "step": 1281
    },
    {
      "epoch": 0.8439763001974984,
      "grad_norm": 1.3917694091796875,
      "learning_rate": 1.6342100775515656e-05,
      "loss": 1.7864,
      "step": 1282
    },
    {
      "epoch": 0.8446346280447663,
      "grad_norm": 10.593568801879883,
      "learning_rate": 1.6336769103812664e-05,
      "loss": 2.3752,
      "step": 1283
    },
    {
      "epoch": 0.8452929558920342,
      "grad_norm": 5.993230819702148,
      "learning_rate": 1.6331434420424618e-05,
      "loss": 2.0222,
      "step": 1284
    },
    {
      "epoch": 0.8459512837393022,
      "grad_norm": 13.346332550048828,
      "learning_rate": 1.632609672788694e-05,
      "loss": 2.58,
      "step": 1285
    },
    {
      "epoch": 0.8466096115865701,
      "grad_norm": 12.726654052734375,
      "learning_rate": 1.6320756028736478e-05,
      "loss": 2.6521,
      "step": 1286
    },
    {
      "epoch": 0.847267939433838,
      "grad_norm": 1.7437421083450317,
      "learning_rate": 1.6315412325511522e-05,
      "loss": 1.8,
      "step": 1287
    },
    {
      "epoch": 0.847926267281106,
      "grad_norm": 11.223846435546875,
      "learning_rate": 1.6310065620751777e-05,
      "loss": 2.3229,
      "step": 1288
    },
    {
      "epoch": 0.8485845951283739,
      "grad_norm": 2.4371306896209717,
      "learning_rate": 1.6304715916998375e-05,
      "loss": 1.8405,
      "step": 1289
    },
    {
      "epoch": 0.8492429229756419,
      "grad_norm": 1.665712833404541,
      "learning_rate": 1.6299363216793886e-05,
      "loss": 1.7935,
      "step": 1290
    },
    {
      "epoch": 0.8499012508229098,
      "grad_norm": 6.33360481262207,
      "learning_rate": 1.6294007522682286e-05,
      "loss": 1.933,
      "step": 1291
    },
    {
      "epoch": 0.8505595786701777,
      "grad_norm": 9.015059471130371,
      "learning_rate": 1.6288648837208988e-05,
      "loss": 2.4047,
      "step": 1292
    },
    {
      "epoch": 0.8512179065174457,
      "grad_norm": 3.25504994392395,
      "learning_rate": 1.628328716292082e-05,
      "loss": 1.8485,
      "step": 1293
    },
    {
      "epoch": 0.8518762343647136,
      "grad_norm": 7.479658603668213,
      "learning_rate": 1.6277922502366034e-05,
      "loss": 1.9882,
      "step": 1294
    },
    {
      "epoch": 0.8525345622119815,
      "grad_norm": 16.18124771118164,
      "learning_rate": 1.62725548580943e-05,
      "loss": 2.9194,
      "step": 1295
    },
    {
      "epoch": 0.8531928900592495,
      "grad_norm": 1.12393057346344,
      "learning_rate": 1.6267184232656707e-05,
      "loss": 1.7812,
      "step": 1296
    },
    {
      "epoch": 0.8538512179065174,
      "grad_norm": 1.688840389251709,
      "learning_rate": 1.6261810628605754e-05,
      "loss": 1.7662,
      "step": 1297
    },
    {
      "epoch": 0.8545095457537853,
      "grad_norm": 15.913049697875977,
      "learning_rate": 1.6256434048495366e-05,
      "loss": 2.3739,
      "step": 1298
    },
    {
      "epoch": 0.8551678736010533,
      "grad_norm": 2.079780101776123,
      "learning_rate": 1.6251054494880878e-05,
      "loss": 1.7694,
      "step": 1299
    },
    {
      "epoch": 0.8558262014483212,
      "grad_norm": 2.7444684505462646,
      "learning_rate": 1.6245671970319033e-05,
      "loss": 1.8046,
      "step": 1300
    },
    {
      "epoch": 0.8564845292955892,
      "grad_norm": 6.773306846618652,
      "learning_rate": 1.6240286477367995e-05,
      "loss": 1.9239,
      "step": 1301
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 13.002197265625,
      "learning_rate": 1.6234898018587336e-05,
      "loss": 2.5906,
      "step": 1302
    },
    {
      "epoch": 0.857801184990125,
      "grad_norm": 1.977997899055481,
      "learning_rate": 1.6229506596538034e-05,
      "loss": 1.7708,
      "step": 1303
    },
    {
      "epoch": 0.858459512837393,
      "grad_norm": 1.504377841949463,
      "learning_rate": 1.622411221378248e-05,
      "loss": 1.7964,
      "step": 1304
    },
    {
      "epoch": 0.859117840684661,
      "grad_norm": 6.979405879974365,
      "learning_rate": 1.6218714872884463e-05,
      "loss": 2.0739,
      "step": 1305
    },
    {
      "epoch": 0.859776168531929,
      "grad_norm": 1.6480854749679565,
      "learning_rate": 1.621331457640919e-05,
      "loss": 1.7591,
      "step": 1306
    },
    {
      "epoch": 0.8604344963791969,
      "grad_norm": 12.77497386932373,
      "learning_rate": 1.620791132692327e-05,
      "loss": 2.3278,
      "step": 1307
    },
    {
      "epoch": 0.8610928242264648,
      "grad_norm": 14.656706809997559,
      "learning_rate": 1.620250512699471e-05,
      "loss": 2.5328,
      "step": 1308
    },
    {
      "epoch": 0.8617511520737328,
      "grad_norm": 4.7437639236450195,
      "learning_rate": 1.619709597919292e-05,
      "loss": 2.0042,
      "step": 1309
    },
    {
      "epoch": 0.8624094799210007,
      "grad_norm": 7.66450834274292,
      "learning_rate": 1.6191683886088715e-05,
      "loss": 2.1627,
      "step": 1310
    },
    {
      "epoch": 0.8630678077682686,
      "grad_norm": 18.79759407043457,
      "learning_rate": 1.6186268850254312e-05,
      "loss": 2.4839,
      "step": 1311
    },
    {
      "epoch": 0.8637261356155366,
      "grad_norm": 1.9449799060821533,
      "learning_rate": 1.6180850874263314e-05,
      "loss": 1.7902,
      "step": 1312
    },
    {
      "epoch": 0.8643844634628045,
      "grad_norm": 13.918742179870605,
      "learning_rate": 1.6175429960690735e-05,
      "loss": 2.4705,
      "step": 1313
    },
    {
      "epoch": 0.8650427913100724,
      "grad_norm": 6.7824835777282715,
      "learning_rate": 1.6170006112112982e-05,
      "loss": 2.0911,
      "step": 1314
    },
    {
      "epoch": 0.8657011191573404,
      "grad_norm": 1.8604724407196045,
      "learning_rate": 1.6164579331107853e-05,
      "loss": 1.7735,
      "step": 1315
    },
    {
      "epoch": 0.8663594470046083,
      "grad_norm": 25.269847869873047,
      "learning_rate": 1.615914962025454e-05,
      "loss": 2.897,
      "step": 1316
    },
    {
      "epoch": 0.8670177748518763,
      "grad_norm": 7.321370601654053,
      "learning_rate": 1.615371698213363e-05,
      "loss": 2.1401,
      "step": 1317
    },
    {
      "epoch": 0.8676761026991442,
      "grad_norm": 2.1978912353515625,
      "learning_rate": 1.6148281419327103e-05,
      "loss": 1.7681,
      "step": 1318
    },
    {
      "epoch": 0.8683344305464121,
      "grad_norm": 1.5152742862701416,
      "learning_rate": 1.6142842934418323e-05,
      "loss": 1.7615,
      "step": 1319
    },
    {
      "epoch": 0.8689927583936801,
      "grad_norm": 10.042667388916016,
      "learning_rate": 1.613740152999205e-05,
      "loss": 2.0214,
      "step": 1320
    },
    {
      "epoch": 0.869651086240948,
      "grad_norm": 7.850175380706787,
      "learning_rate": 1.6131957208634426e-05,
      "loss": 2.2231,
      "step": 1321
    },
    {
      "epoch": 0.8703094140882159,
      "grad_norm": 1.714372158050537,
      "learning_rate": 1.6126509972932984e-05,
      "loss": 1.758,
      "step": 1322
    },
    {
      "epoch": 0.8709677419354839,
      "grad_norm": 11.727275848388672,
      "learning_rate": 1.612105982547663e-05,
      "loss": 2.3057,
      "step": 1323
    },
    {
      "epoch": 0.8716260697827518,
      "grad_norm": 3.341975688934326,
      "learning_rate": 1.611560676885567e-05,
      "loss": 1.7739,
      "step": 1324
    },
    {
      "epoch": 0.8722843976300197,
      "grad_norm": 1.631178379058838,
      "learning_rate": 1.611015080566179e-05,
      "loss": 1.7607,
      "step": 1325
    },
    {
      "epoch": 0.8729427254772877,
      "grad_norm": 1.5065113306045532,
      "learning_rate": 1.6104691938488044e-05,
      "loss": 1.7561,
      "step": 1326
    },
    {
      "epoch": 0.8736010533245556,
      "grad_norm": 15.968194961547852,
      "learning_rate": 1.6099230169928882e-05,
      "loss": 2.5863,
      "step": 1327
    },
    {
      "epoch": 0.8742593811718236,
      "grad_norm": 1.5833178758621216,
      "learning_rate": 1.6093765502580127e-05,
      "loss": 1.7543,
      "step": 1328
    },
    {
      "epoch": 0.8749177090190915,
      "grad_norm": 2.693302631378174,
      "learning_rate": 1.6088297939038975e-05,
      "loss": 1.801,
      "step": 1329
    },
    {
      "epoch": 0.8755760368663594,
      "grad_norm": 4.7392425537109375,
      "learning_rate": 1.6082827481904e-05,
      "loss": 2.0178,
      "step": 1330
    },
    {
      "epoch": 0.8762343647136274,
      "grad_norm": 16.193897247314453,
      "learning_rate": 1.6077354133775163e-05,
      "loss": 2.4953,
      "step": 1331
    },
    {
      "epoch": 0.8768926925608953,
      "grad_norm": 15.729377746582031,
      "learning_rate": 1.607187789725378e-05,
      "loss": 2.4911,
      "step": 1332
    },
    {
      "epoch": 0.8775510204081632,
      "grad_norm": 14.621931076049805,
      "learning_rate": 1.6066398774942556e-05,
      "loss": 2.2493,
      "step": 1333
    },
    {
      "epoch": 0.8782093482554312,
      "grad_norm": 2.085615396499634,
      "learning_rate": 1.6060916769445557e-05,
      "loss": 1.7732,
      "step": 1334
    },
    {
      "epoch": 0.8788676761026991,
      "grad_norm": 1.9631409645080566,
      "learning_rate": 1.6055431883368225e-05,
      "loss": 1.7932,
      "step": 1335
    },
    {
      "epoch": 0.879526003949967,
      "grad_norm": 9.906536102294922,
      "learning_rate": 1.6049944119317375e-05,
      "loss": 2.1019,
      "step": 1336
    },
    {
      "epoch": 0.880184331797235,
      "grad_norm": 8.42004680633545,
      "learning_rate": 1.6044453479901173e-05,
      "loss": 2.0315,
      "step": 1337
    },
    {
      "epoch": 0.8808426596445029,
      "grad_norm": 2.6855645179748535,
      "learning_rate": 1.603895996772917e-05,
      "loss": 1.7981,
      "step": 1338
    },
    {
      "epoch": 0.8815009874917709,
      "grad_norm": 24.510013580322266,
      "learning_rate": 1.6033463585412277e-05,
      "loss": 3.075,
      "step": 1339
    },
    {
      "epoch": 0.8821593153390388,
      "grad_norm": 3.181957244873047,
      "learning_rate": 1.602796433556276e-05,
      "loss": 1.7953,
      "step": 1340
    },
    {
      "epoch": 0.8828176431863067,
      "grad_norm": 1.3352742195129395,
      "learning_rate": 1.6022462220794263e-05,
      "loss": 1.7517,
      "step": 1341
    },
    {
      "epoch": 0.8834759710335747,
      "grad_norm": 12.008381843566895,
      "learning_rate": 1.601695724372178e-05,
      "loss": 2.6529,
      "step": 1342
    },
    {
      "epoch": 0.8841342988808426,
      "grad_norm": 13.760814666748047,
      "learning_rate": 1.6011449406961667e-05,
      "loss": 2.4285,
      "step": 1343
    },
    {
      "epoch": 0.8847926267281107,
      "grad_norm": 12.920219421386719,
      "learning_rate": 1.6005938713131644e-05,
      "loss": 2.3731,
      "step": 1344
    },
    {
      "epoch": 0.8854509545753786,
      "grad_norm": 14.795028686523438,
      "learning_rate": 1.6000425164850787e-05,
      "loss": 2.3787,
      "step": 1345
    },
    {
      "epoch": 0.8861092824226465,
      "grad_norm": 2.343254804611206,
      "learning_rate": 1.5994908764739522e-05,
      "loss": 1.7996,
      "step": 1346
    },
    {
      "epoch": 0.8867676102699145,
      "grad_norm": 12.54514217376709,
      "learning_rate": 1.5989389515419643e-05,
      "loss": 2.5356,
      "step": 1347
    },
    {
      "epoch": 0.8874259381171824,
      "grad_norm": 15.0286865234375,
      "learning_rate": 1.5983867419514288e-05,
      "loss": 2.7909,
      "step": 1348
    },
    {
      "epoch": 0.8880842659644503,
      "grad_norm": 1.4750040769577026,
      "learning_rate": 1.5978342479647955e-05,
      "loss": 1.7523,
      "step": 1349
    },
    {
      "epoch": 0.8887425938117183,
      "grad_norm": 12.14058780670166,
      "learning_rate": 1.5972814698446484e-05,
      "loss": 2.4122,
      "step": 1350
    },
    {
      "epoch": 0.8894009216589862,
      "grad_norm": 7.634997367858887,
      "learning_rate": 1.5967284078537076e-05,
      "loss": 2.1392,
      "step": 1351
    },
    {
      "epoch": 0.8900592495062541,
      "grad_norm": 11.793134689331055,
      "learning_rate": 1.5961750622548274e-05,
      "loss": 2.595,
      "step": 1352
    },
    {
      "epoch": 0.8907175773535221,
      "grad_norm": 9.54214096069336,
      "learning_rate": 1.595621433310997e-05,
      "loss": 2.1856,
      "step": 1353
    },
    {
      "epoch": 0.89137590520079,
      "grad_norm": 1.6536383628845215,
      "learning_rate": 1.5950675212853408e-05,
      "loss": 1.7437,
      "step": 1354
    },
    {
      "epoch": 0.892034233048058,
      "grad_norm": 13.198814392089844,
      "learning_rate": 1.5945133264411165e-05,
      "loss": 2.3985,
      "step": 1355
    },
    {
      "epoch": 0.8926925608953259,
      "grad_norm": 2.6973562240600586,
      "learning_rate": 1.5939588490417182e-05,
      "loss": 1.7735,
      "step": 1356
    },
    {
      "epoch": 0.8933508887425938,
      "grad_norm": 15.435637474060059,
      "learning_rate": 1.5934040893506724e-05,
      "loss": 2.5665,
      "step": 1357
    },
    {
      "epoch": 0.8940092165898618,
      "grad_norm": 10.90636920928955,
      "learning_rate": 1.592849047631641e-05,
      "loss": 2.2512,
      "step": 1358
    },
    {
      "epoch": 0.8946675444371297,
      "grad_norm": 8.899417877197266,
      "learning_rate": 1.5922937241484188e-05,
      "loss": 2.2472,
      "step": 1359
    },
    {
      "epoch": 0.8953258722843976,
      "grad_norm": 13.88516616821289,
      "learning_rate": 1.591738119164936e-05,
      "loss": 2.5767,
      "step": 1360
    },
    {
      "epoch": 0.8959842001316656,
      "grad_norm": 21.48865509033203,
      "learning_rate": 1.591182232945255e-05,
      "loss": 3.1802,
      "step": 1361
    },
    {
      "epoch": 0.8966425279789335,
      "grad_norm": 2.7140467166900635,
      "learning_rate": 1.5906260657535727e-05,
      "loss": 1.7636,
      "step": 1362
    },
    {
      "epoch": 0.8973008558262014,
      "grad_norm": 15.751225471496582,
      "learning_rate": 1.5900696178542196e-05,
      "loss": 2.4126,
      "step": 1363
    },
    {
      "epoch": 0.8979591836734694,
      "grad_norm": 6.6238694190979,
      "learning_rate": 1.58951288951166e-05,
      "loss": 2.2966,
      "step": 1364
    },
    {
      "epoch": 0.8986175115207373,
      "grad_norm": 6.995729923248291,
      "learning_rate": 1.58895588099049e-05,
      "loss": 2.0626,
      "step": 1365
    },
    {
      "epoch": 0.8992758393680053,
      "grad_norm": 5.507295608520508,
      "learning_rate": 1.5883985925554407e-05,
      "loss": 1.8464,
      "step": 1366
    },
    {
      "epoch": 0.8999341672152732,
      "grad_norm": 5.2928338050842285,
      "learning_rate": 1.5878410244713745e-05,
      "loss": 1.8449,
      "step": 1367
    },
    {
      "epoch": 0.9005924950625411,
      "grad_norm": 10.407928466796875,
      "learning_rate": 1.5872831770032885e-05,
      "loss": 2.1264,
      "step": 1368
    },
    {
      "epoch": 0.9012508229098091,
      "grad_norm": 6.1611809730529785,
      "learning_rate": 1.5867250504163107e-05,
      "loss": 2.0982,
      "step": 1369
    },
    {
      "epoch": 0.901909150757077,
      "grad_norm": 7.503368377685547,
      "learning_rate": 1.586166644975703e-05,
      "loss": 1.906,
      "step": 1370
    },
    {
      "epoch": 0.9025674786043449,
      "grad_norm": 8.921416282653809,
      "learning_rate": 1.58560796094686e-05,
      "loss": 2.1989,
      "step": 1371
    },
    {
      "epoch": 0.9032258064516129,
      "grad_norm": 15.408834457397461,
      "learning_rate": 1.5850489985953076e-05,
      "loss": 2.3485,
      "step": 1372
    },
    {
      "epoch": 0.9038841342988808,
      "grad_norm": 17.601186752319336,
      "learning_rate": 1.584489758186705e-05,
      "loss": 2.783,
      "step": 1373
    },
    {
      "epoch": 0.9045424621461488,
      "grad_norm": 7.720793724060059,
      "learning_rate": 1.5839302399868433e-05,
      "loss": 1.9213,
      "step": 1374
    },
    {
      "epoch": 0.9052007899934167,
      "grad_norm": 4.814738750457764,
      "learning_rate": 1.5833704442616455e-05,
      "loss": 2.0964,
      "step": 1375
    },
    {
      "epoch": 0.9058591178406846,
      "grad_norm": 9.009025573730469,
      "learning_rate": 1.5828103712771656e-05,
      "loss": 2.2684,
      "step": 1376
    },
    {
      "epoch": 0.9065174456879526,
      "grad_norm": 9.654634475708008,
      "learning_rate": 1.582250021299591e-05,
      "loss": 2.1249,
      "step": 1377
    },
    {
      "epoch": 0.9071757735352205,
      "grad_norm": 22.950397491455078,
      "learning_rate": 1.5816893945952404e-05,
      "loss": 3.0655,
      "step": 1378
    },
    {
      "epoch": 0.9078341013824884,
      "grad_norm": 11.380916595458984,
      "learning_rate": 1.5811284914305627e-05,
      "loss": 2.4457,
      "step": 1379
    },
    {
      "epoch": 0.9084924292297564,
      "grad_norm": 5.149210453033447,
      "learning_rate": 1.5805673120721395e-05,
      "loss": 1.9571,
      "step": 1380
    },
    {
      "epoch": 0.9091507570770243,
      "grad_norm": 2.5318551063537598,
      "learning_rate": 1.580005856786684e-05,
      "loss": 1.757,
      "step": 1381
    },
    {
      "epoch": 0.9098090849242922,
      "grad_norm": 11.146743774414062,
      "learning_rate": 1.5794441258410387e-05,
      "loss": 2.289,
      "step": 1382
    },
    {
      "epoch": 0.9104674127715603,
      "grad_norm": 13.876885414123535,
      "learning_rate": 1.5788821195021792e-05,
      "loss": 2.3888,
      "step": 1383
    },
    {
      "epoch": 0.9111257406188282,
      "grad_norm": 5.000575542449951,
      "learning_rate": 1.57831983803721e-05,
      "loss": 2.084,
      "step": 1384
    },
    {
      "epoch": 0.9117840684660962,
      "grad_norm": 6.526230812072754,
      "learning_rate": 1.5777572817133683e-05,
      "loss": 1.9832,
      "step": 1385
    },
    {
      "epoch": 0.9124423963133641,
      "grad_norm": 2.954993724822998,
      "learning_rate": 1.5771944507980208e-05,
      "loss": 1.7775,
      "step": 1386
    },
    {
      "epoch": 0.913100724160632,
      "grad_norm": 11.589322090148926,
      "learning_rate": 1.576631345558665e-05,
      "loss": 2.4455,
      "step": 1387
    },
    {
      "epoch": 0.9137590520079,
      "grad_norm": 12.154374122619629,
      "learning_rate": 1.5760679662629283e-05,
      "loss": 2.1524,
      "step": 1388
    },
    {
      "epoch": 0.9144173798551679,
      "grad_norm": 9.735584259033203,
      "learning_rate": 1.5755043131785687e-05,
      "loss": 2.2129,
      "step": 1389
    },
    {
      "epoch": 0.9150757077024358,
      "grad_norm": 19.858028411865234,
      "learning_rate": 1.574940386573475e-05,
      "loss": 3.5606,
      "step": 1390
    },
    {
      "epoch": 0.9157340355497038,
      "grad_norm": 5.637260437011719,
      "learning_rate": 1.5743761867156654e-05,
      "loss": 1.9567,
      "step": 1391
    },
    {
      "epoch": 0.9163923633969717,
      "grad_norm": 13.001128196716309,
      "learning_rate": 1.573811713873287e-05,
      "loss": 2.4003,
      "step": 1392
    },
    {
      "epoch": 0.9170506912442397,
      "grad_norm": 22.535551071166992,
      "learning_rate": 1.5732469683146188e-05,
      "loss": 2.5332,
      "step": 1393
    },
    {
      "epoch": 0.9177090190915076,
      "grad_norm": 8.799814224243164,
      "learning_rate": 1.5726819503080672e-05,
      "loss": 1.9611,
      "step": 1394
    },
    {
      "epoch": 0.9183673469387755,
      "grad_norm": 2.3458127975463867,
      "learning_rate": 1.5721166601221697e-05,
      "loss": 1.8017,
      "step": 1395
    },
    {
      "epoch": 0.9190256747860435,
      "grad_norm": 6.789740562438965,
      "learning_rate": 1.5715510980255925e-05,
      "loss": 2.1876,
      "step": 1396
    },
    {
      "epoch": 0.9196840026333114,
      "grad_norm": 11.898836135864258,
      "learning_rate": 1.5709852642871307e-05,
      "loss": 2.5017,
      "step": 1397
    },
    {
      "epoch": 0.9203423304805793,
      "grad_norm": 7.2261834144592285,
      "learning_rate": 1.5704191591757095e-05,
      "loss": 2.1771,
      "step": 1398
    },
    {
      "epoch": 0.9210006583278473,
      "grad_norm": 18.71288299560547,
      "learning_rate": 1.5698527829603817e-05,
      "loss": 2.9584,
      "step": 1399
    },
    {
      "epoch": 0.9216589861751152,
      "grad_norm": 5.9786057472229,
      "learning_rate": 1.5692861359103304e-05,
      "loss": 2.0224,
      "step": 1400
    },
    {
      "epoch": 0.9223173140223832,
      "grad_norm": 1.8461755514144897,
      "learning_rate": 1.5687192182948663e-05,
      "loss": 1.7393,
      "step": 1401
    },
    {
      "epoch": 0.9229756418696511,
      "grad_norm": 11.434892654418945,
      "learning_rate": 1.5681520303834293e-05,
      "loss": 2.3482,
      "step": 1402
    },
    {
      "epoch": 0.923633969716919,
      "grad_norm": 6.918822765350342,
      "learning_rate": 1.5675845724455875e-05,
      "loss": 2.14,
      "step": 1403
    },
    {
      "epoch": 0.924292297564187,
      "grad_norm": 11.742706298828125,
      "learning_rate": 1.5670168447510376e-05,
      "loss": 2.2521,
      "step": 1404
    },
    {
      "epoch": 0.9249506254114549,
      "grad_norm": 4.37561559677124,
      "learning_rate": 1.5664488475696044e-05,
      "loss": 2.0382,
      "step": 1405
    },
    {
      "epoch": 0.9256089532587228,
      "grad_norm": 9.433731079101562,
      "learning_rate": 1.5658805811712402e-05,
      "loss": 2.447,
      "step": 1406
    },
    {
      "epoch": 0.9262672811059908,
      "grad_norm": 13.142863273620605,
      "learning_rate": 1.5653120458260263e-05,
      "loss": 2.454,
      "step": 1407
    },
    {
      "epoch": 0.9269256089532587,
      "grad_norm": 5.189517974853516,
      "learning_rate": 1.564743241804171e-05,
      "loss": 1.9023,
      "step": 1408
    },
    {
      "epoch": 0.9275839368005266,
      "grad_norm": 19.822006225585938,
      "learning_rate": 1.56417416937601e-05,
      "loss": 3.2466,
      "step": 1409
    },
    {
      "epoch": 0.9282422646477946,
      "grad_norm": 7.156246662139893,
      "learning_rate": 1.563604828812009e-05,
      "loss": 2.1105,
      "step": 1410
    },
    {
      "epoch": 0.9289005924950625,
      "grad_norm": 3.0560848712921143,
      "learning_rate": 1.5630352203827573e-05,
      "loss": 1.7923,
      "step": 1411
    },
    {
      "epoch": 0.9295589203423305,
      "grad_norm": 15.849310874938965,
      "learning_rate": 1.5624653443589743e-05,
      "loss": 2.6803,
      "step": 1412
    },
    {
      "epoch": 0.9302172481895984,
      "grad_norm": 2.245110034942627,
      "learning_rate": 1.5618952010115057e-05,
      "loss": 1.7429,
      "step": 1413
    },
    {
      "epoch": 0.9308755760368663,
      "grad_norm": 4.919018745422363,
      "learning_rate": 1.561324790611324e-05,
      "loss": 1.8005,
      "step": 1414
    },
    {
      "epoch": 0.9315339038841343,
      "grad_norm": 8.885347366333008,
      "learning_rate": 1.56075411342953e-05,
      "loss": 2.454,
      "step": 1415
    },
    {
      "epoch": 0.9321922317314022,
      "grad_norm": 4.065394401550293,
      "learning_rate": 1.5601831697373496e-05,
      "loss": 1.7923,
      "step": 1416
    },
    {
      "epoch": 0.9328505595786701,
      "grad_norm": 4.6268181800842285,
      "learning_rate": 1.5596119598061355e-05,
      "loss": 1.8527,
      "step": 1417
    },
    {
      "epoch": 0.9335088874259381,
      "grad_norm": 5.3418145179748535,
      "learning_rate": 1.5590404839073677e-05,
      "loss": 1.9725,
      "step": 1418
    },
    {
      "epoch": 0.934167215273206,
      "grad_norm": 15.872974395751953,
      "learning_rate": 1.5584687423126528e-05,
      "loss": 2.313,
      "step": 1419
    },
    {
      "epoch": 0.934825543120474,
      "grad_norm": 11.079724311828613,
      "learning_rate": 1.5578967352937227e-05,
      "loss": 2.4324,
      "step": 1420
    },
    {
      "epoch": 0.9354838709677419,
      "grad_norm": 10.488574028015137,
      "learning_rate": 1.5573244631224364e-05,
      "loss": 2.4157,
      "step": 1421
    },
    {
      "epoch": 0.9361421988150099,
      "grad_norm": 9.333179473876953,
      "learning_rate": 1.5567519260707785e-05,
      "loss": 2.1478,
      "step": 1422
    },
    {
      "epoch": 0.9368005266622779,
      "grad_norm": 4.480792999267578,
      "learning_rate": 1.556179124410859e-05,
      "loss": 1.7943,
      "step": 1423
    },
    {
      "epoch": 0.9374588545095458,
      "grad_norm": 9.116419792175293,
      "learning_rate": 1.5556060584149146e-05,
      "loss": 2.3406,
      "step": 1424
    },
    {
      "epoch": 0.9381171823568137,
      "grad_norm": 10.29698657989502,
      "learning_rate": 1.5550327283553068e-05,
      "loss": 2.2129,
      "step": 1425
    },
    {
      "epoch": 0.9387755102040817,
      "grad_norm": 8.44991683959961,
      "learning_rate": 1.554459134504523e-05,
      "loss": 2.181,
      "step": 1426
    },
    {
      "epoch": 0.9394338380513496,
      "grad_norm": 7.063148021697998,
      "learning_rate": 1.5538852771351763e-05,
      "loss": 2.1553,
      "step": 1427
    },
    {
      "epoch": 0.9400921658986175,
      "grad_norm": 3.3539109230041504,
      "learning_rate": 1.5533111565200046e-05,
      "loss": 1.7465,
      "step": 1428
    },
    {
      "epoch": 0.9407504937458855,
      "grad_norm": 6.9155802726745605,
      "learning_rate": 1.5527367729318705e-05,
      "loss": 1.8533,
      "step": 1429
    },
    {
      "epoch": 0.9414088215931534,
      "grad_norm": 1.483103632926941,
      "learning_rate": 1.552162126643763e-05,
      "loss": 1.7321,
      "step": 1430
    },
    {
      "epoch": 0.9420671494404214,
      "grad_norm": 10.734274864196777,
      "learning_rate": 1.5515872179287942e-05,
      "loss": 2.1875,
      "step": 1431
    },
    {
      "epoch": 0.9427254772876893,
      "grad_norm": 11.555147171020508,
      "learning_rate": 1.5510120470602023e-05,
      "loss": 2.2138,
      "step": 1432
    },
    {
      "epoch": 0.9433838051349572,
      "grad_norm": 2.3284413814544678,
      "learning_rate": 1.550436614311349e-05,
      "loss": 1.7423,
      "step": 1433
    },
    {
      "epoch": 0.9440421329822252,
      "grad_norm": 11.559992790222168,
      "learning_rate": 1.5498609199557217e-05,
      "loss": 2.0565,
      "step": 1434
    },
    {
      "epoch": 0.9447004608294931,
      "grad_norm": 2.1307497024536133,
      "learning_rate": 1.549284964266931e-05,
      "loss": 1.7539,
      "step": 1435
    },
    {
      "epoch": 0.945358788676761,
      "grad_norm": 2.8533143997192383,
      "learning_rate": 1.5487087475187124e-05,
      "loss": 1.7577,
      "step": 1436
    },
    {
      "epoch": 0.946017116524029,
      "grad_norm": 2.454684257507324,
      "learning_rate": 1.5481322699849247e-05,
      "loss": 1.7639,
      "step": 1437
    },
    {
      "epoch": 0.9466754443712969,
      "grad_norm": 3.5712578296661377,
      "learning_rate": 1.547555531939552e-05,
      "loss": 1.8003,
      "step": 1438
    },
    {
      "epoch": 0.9473337722185649,
      "grad_norm": 1.7823535203933716,
      "learning_rate": 1.5469785336567008e-05,
      "loss": 1.7245,
      "step": 1439
    },
    {
      "epoch": 0.9479921000658328,
      "grad_norm": 15.19128131866455,
      "learning_rate": 1.5464012754106022e-05,
      "loss": 2.5638,
      "step": 1440
    },
    {
      "epoch": 0.9486504279131007,
      "grad_norm": 3.0543034076690674,
      "learning_rate": 1.5458237574756102e-05,
      "loss": 1.7344,
      "step": 1441
    },
    {
      "epoch": 0.9493087557603687,
      "grad_norm": 1.2425248622894287,
      "learning_rate": 1.545245980126203e-05,
      "loss": 1.7165,
      "step": 1442
    },
    {
      "epoch": 0.9499670836076366,
      "grad_norm": 19.946990966796875,
      "learning_rate": 1.544667943636981e-05,
      "loss": 3.6353,
      "step": 1443
    },
    {
      "epoch": 0.9506254114549045,
      "grad_norm": 13.887561798095703,
      "learning_rate": 1.5440896482826692e-05,
      "loss": 2.4743,
      "step": 1444
    },
    {
      "epoch": 0.9512837393021725,
      "grad_norm": 15.56630802154541,
      "learning_rate": 1.5435110943381144e-05,
      "loss": 2.4403,
      "step": 1445
    },
    {
      "epoch": 0.9519420671494404,
      "grad_norm": 2.2625842094421387,
      "learning_rate": 1.5429322820782863e-05,
      "loss": 1.7627,
      "step": 1446
    },
    {
      "epoch": 0.9526003949967083,
      "grad_norm": 10.557746887207031,
      "learning_rate": 1.5423532117782787e-05,
      "loss": 2.2075,
      "step": 1447
    },
    {
      "epoch": 0.9532587228439763,
      "grad_norm": 11.034051895141602,
      "learning_rate": 1.5417738837133067e-05,
      "loss": 2.4907,
      "step": 1448
    },
    {
      "epoch": 0.9539170506912442,
      "grad_norm": 6.787309646606445,
      "learning_rate": 1.541194298158708e-05,
      "loss": 1.922,
      "step": 1449
    },
    {
      "epoch": 0.9545753785385122,
      "grad_norm": 11.193209648132324,
      "learning_rate": 1.5406144553899437e-05,
      "loss": 2.5966,
      "step": 1450
    },
    {
      "epoch": 0.9552337063857801,
      "grad_norm": 8.278301239013672,
      "learning_rate": 1.540034355682596e-05,
      "loss": 1.9913,
      "step": 1451
    },
    {
      "epoch": 0.955892034233048,
      "grad_norm": 9.555273056030273,
      "learning_rate": 1.5394539993123704e-05,
      "loss": 2.0805,
      "step": 1452
    },
    {
      "epoch": 0.956550362080316,
      "grad_norm": 8.262798309326172,
      "learning_rate": 1.5388733865550928e-05,
      "loss": 2.1943,
      "step": 1453
    },
    {
      "epoch": 0.9572086899275839,
      "grad_norm": 19.095081329345703,
      "learning_rate": 1.5382925176867117e-05,
      "loss": 2.6758,
      "step": 1454
    },
    {
      "epoch": 0.9578670177748518,
      "grad_norm": 11.8326416015625,
      "learning_rate": 1.5377113929832984e-05,
      "loss": 2.1699,
      "step": 1455
    },
    {
      "epoch": 0.9585253456221198,
      "grad_norm": 4.634951591491699,
      "learning_rate": 1.5371300127210447e-05,
      "loss": 1.7784,
      "step": 1456
    },
    {
      "epoch": 0.9591836734693877,
      "grad_norm": 6.891920566558838,
      "learning_rate": 1.536548377176263e-05,
      "loss": 1.9464,
      "step": 1457
    },
    {
      "epoch": 0.9598420013166556,
      "grad_norm": 14.376165390014648,
      "learning_rate": 1.5359664866253893e-05,
      "loss": 2.4754,
      "step": 1458
    },
    {
      "epoch": 0.9605003291639236,
      "grad_norm": 5.3497443199157715,
      "learning_rate": 1.5353843413449788e-05,
      "loss": 1.9961,
      "step": 1459
    },
    {
      "epoch": 0.9611586570111915,
      "grad_norm": 14.701567649841309,
      "learning_rate": 1.5348019416117083e-05,
      "loss": 2.5212,
      "step": 1460
    },
    {
      "epoch": 0.9618169848584596,
      "grad_norm": 1.9360382556915283,
      "learning_rate": 1.5342192877023763e-05,
      "loss": 1.7494,
      "step": 1461
    },
    {
      "epoch": 0.9624753127057275,
      "grad_norm": 6.197115421295166,
      "learning_rate": 1.5336363798939013e-05,
      "loss": 1.9752,
      "step": 1462
    },
    {
      "epoch": 0.9631336405529954,
      "grad_norm": 12.746725082397461,
      "learning_rate": 1.533053218463323e-05,
      "loss": 2.4998,
      "step": 1463
    },
    {
      "epoch": 0.9637919684002634,
      "grad_norm": 19.391780853271484,
      "learning_rate": 1.532469803687801e-05,
      "loss": 2.4693,
      "step": 1464
    },
    {
      "epoch": 0.9644502962475313,
      "grad_norm": 10.357386589050293,
      "learning_rate": 1.5318861358446152e-05,
      "loss": 1.9609,
      "step": 1465
    },
    {
      "epoch": 0.9651086240947993,
      "grad_norm": 15.331169128417969,
      "learning_rate": 1.5313022152111668e-05,
      "loss": 2.5641,
      "step": 1466
    },
    {
      "epoch": 0.9657669519420672,
      "grad_norm": 6.343869686126709,
      "learning_rate": 1.530718042064977e-05,
      "loss": 1.9384,
      "step": 1467
    },
    {
      "epoch": 0.9664252797893351,
      "grad_norm": 8.791536331176758,
      "learning_rate": 1.530133616683686e-05,
      "loss": 2.2846,
      "step": 1468
    },
    {
      "epoch": 0.9670836076366031,
      "grad_norm": 14.15211009979248,
      "learning_rate": 1.529548939345054e-05,
      "loss": 2.2835,
      "step": 1469
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 5.647982120513916,
      "learning_rate": 1.5289640103269626e-05,
      "loss": 2.0997,
      "step": 1470
    },
    {
      "epoch": 0.9684002633311389,
      "grad_norm": 2.1971967220306396,
      "learning_rate": 1.5283788299074108e-05,
      "loss": 1.7093,
      "step": 1471
    },
    {
      "epoch": 0.9690585911784069,
      "grad_norm": 12.920476913452148,
      "learning_rate": 1.5277933983645188e-05,
      "loss": 2.4788,
      "step": 1472
    },
    {
      "epoch": 0.9697169190256748,
      "grad_norm": 1.8858197927474976,
      "learning_rate": 1.527207715976525e-05,
      "loss": 1.7254,
      "step": 1473
    },
    {
      "epoch": 0.9703752468729427,
      "grad_norm": 2.2651915550231934,
      "learning_rate": 1.5266217830217876e-05,
      "loss": 1.7302,
      "step": 1474
    },
    {
      "epoch": 0.9710335747202107,
      "grad_norm": 14.515719413757324,
      "learning_rate": 1.5260355997787838e-05,
      "loss": 2.5471,
      "step": 1475
    },
    {
      "epoch": 0.9716919025674786,
      "grad_norm": 14.996271133422852,
      "learning_rate": 1.52544916652611e-05,
      "loss": 2.5922,
      "step": 1476
    },
    {
      "epoch": 0.9723502304147466,
      "grad_norm": 10.881900787353516,
      "learning_rate": 1.5248624835424805e-05,
      "loss": 2.3146,
      "step": 1477
    },
    {
      "epoch": 0.9730085582620145,
      "grad_norm": 7.0555338859558105,
      "learning_rate": 1.5242755511067296e-05,
      "loss": 2.1644,
      "step": 1478
    },
    {
      "epoch": 0.9736668861092824,
      "grad_norm": 2.4673802852630615,
      "learning_rate": 1.5236883694978094e-05,
      "loss": 1.7377,
      "step": 1479
    },
    {
      "epoch": 0.9743252139565504,
      "grad_norm": 14.572734832763672,
      "learning_rate": 1.5231009389947899e-05,
      "loss": 2.5072,
      "step": 1480
    },
    {
      "epoch": 0.9749835418038183,
      "grad_norm": 20.329347610473633,
      "learning_rate": 1.5225132598768604e-05,
      "loss": 2.4246,
      "step": 1481
    },
    {
      "epoch": 0.9756418696510862,
      "grad_norm": 2.0971577167510986,
      "learning_rate": 1.5219253324233285e-05,
      "loss": 1.7284,
      "step": 1482
    },
    {
      "epoch": 0.9763001974983542,
      "grad_norm": 14.751676559448242,
      "learning_rate": 1.5213371569136182e-05,
      "loss": 2.289,
      "step": 1483
    },
    {
      "epoch": 0.9769585253456221,
      "grad_norm": 5.1005730628967285,
      "learning_rate": 1.5207487336272735e-05,
      "loss": 1.937,
      "step": 1484
    },
    {
      "epoch": 0.97761685319289,
      "grad_norm": 2.221475124359131,
      "learning_rate": 1.5201600628439548e-05,
      "loss": 1.7271,
      "step": 1485
    },
    {
      "epoch": 0.978275181040158,
      "grad_norm": 17.827402114868164,
      "learning_rate": 1.5195711448434402e-05,
      "loss": 2.8436,
      "step": 1486
    },
    {
      "epoch": 0.9789335088874259,
      "grad_norm": 22.33724594116211,
      "learning_rate": 1.5189819799056258e-05,
      "loss": 2.6662,
      "step": 1487
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 5.503838539123535,
      "learning_rate": 1.5183925683105254e-05,
      "loss": 1.9341,
      "step": 1488
    },
    {
      "epoch": 0.9802501645819618,
      "grad_norm": 2.328829526901245,
      "learning_rate": 1.5178029103382684e-05,
      "loss": 1.7511,
      "step": 1489
    },
    {
      "epoch": 0.9809084924292297,
      "grad_norm": 5.52164888381958,
      "learning_rate": 1.517213006269103e-05,
      "loss": 1.9763,
      "step": 1490
    },
    {
      "epoch": 0.9815668202764977,
      "grad_norm": 16.907896041870117,
      "learning_rate": 1.5166228563833933e-05,
      "loss": 2.2161,
      "step": 1491
    },
    {
      "epoch": 0.9822251481237656,
      "grad_norm": 1.3965007066726685,
      "learning_rate": 1.5160324609616214e-05,
      "loss": 1.7029,
      "step": 1492
    },
    {
      "epoch": 0.9828834759710335,
      "grad_norm": 7.258247375488281,
      "learning_rate": 1.515441820284385e-05,
      "loss": 2.17,
      "step": 1493
    },
    {
      "epoch": 0.9835418038183015,
      "grad_norm": 1.7310534715652466,
      "learning_rate": 1.5148509346323983e-05,
      "loss": 1.7013,
      "step": 1494
    },
    {
      "epoch": 0.9842001316655694,
      "grad_norm": 3.0411994457244873,
      "learning_rate": 1.5142598042864926e-05,
      "loss": 1.7593,
      "step": 1495
    },
    {
      "epoch": 0.9848584595128373,
      "grad_norm": 10.03353500366211,
      "learning_rate": 1.5136684295276158e-05,
      "loss": 2.0428,
      "step": 1496
    },
    {
      "epoch": 0.9855167873601053,
      "grad_norm": 13.048258781433105,
      "learning_rate": 1.5130768106368305e-05,
      "loss": 2.3281,
      "step": 1497
    },
    {
      "epoch": 0.9861751152073732,
      "grad_norm": 16.569011688232422,
      "learning_rate": 1.5124849478953168e-05,
      "loss": 2.2274,
      "step": 1498
    },
    {
      "epoch": 0.9868334430546412,
      "grad_norm": 2.0224530696868896,
      "learning_rate": 1.5118928415843704e-05,
      "loss": 1.7212,
      "step": 1499
    },
    {
      "epoch": 0.9874917709019092,
      "grad_norm": 1.652635931968689,
      "learning_rate": 1.5113004919854013e-05,
      "loss": 1.7091,
      "step": 1500
    },
    {
      "epoch": 0.9881500987491771,
      "grad_norm": 2.7575125694274902,
      "learning_rate": 1.5107078993799374e-05,
      "loss": 1.7196,
      "step": 1501
    },
    {
      "epoch": 0.9888084265964451,
      "grad_norm": 6.82883882522583,
      "learning_rate": 1.510115064049621e-05,
      "loss": 2.0825,
      "step": 1502
    },
    {
      "epoch": 0.989466754443713,
      "grad_norm": 6.080304145812988,
      "learning_rate": 1.5095219862762091e-05,
      "loss": 1.8016,
      "step": 1503
    },
    {
      "epoch": 0.990125082290981,
      "grad_norm": 11.648086547851562,
      "learning_rate": 1.508928666341575e-05,
      "loss": 2.3123,
      "step": 1504
    },
    {
      "epoch": 0.9907834101382489,
      "grad_norm": 2.4569618701934814,
      "learning_rate": 1.5083351045277068e-05,
      "loss": 1.7106,
      "step": 1505
    },
    {
      "epoch": 0.9914417379855168,
      "grad_norm": 15.702960014343262,
      "learning_rate": 1.5077413011167072e-05,
      "loss": 2.6914,
      "step": 1506
    },
    {
      "epoch": 0.9921000658327848,
      "grad_norm": 8.61220645904541,
      "learning_rate": 1.5071472563907942e-05,
      "loss": 2.2824,
      "step": 1507
    },
    {
      "epoch": 0.9927583936800527,
      "grad_norm": 1.5368963479995728,
      "learning_rate": 1.5065529706323002e-05,
      "loss": 1.7049,
      "step": 1508
    },
    {
      "epoch": 0.9934167215273206,
      "grad_norm": 2.1298751831054688,
      "learning_rate": 1.5059584441236718e-05,
      "loss": 1.7028,
      "step": 1509
    },
    {
      "epoch": 0.9940750493745886,
      "grad_norm": 15.221353530883789,
      "learning_rate": 1.5053636771474711e-05,
      "loss": 2.469,
      "step": 1510
    },
    {
      "epoch": 0.9947333772218565,
      "grad_norm": 7.659688472747803,
      "learning_rate": 1.5047686699863732e-05,
      "loss": 2.0504,
      "step": 1511
    },
    {
      "epoch": 0.9953917050691244,
      "grad_norm": 1.906122088432312,
      "learning_rate": 1.5041734229231688e-05,
      "loss": 1.7028,
      "step": 1512
    },
    {
      "epoch": 0.9960500329163924,
      "grad_norm": 11.485210418701172,
      "learning_rate": 1.503577936240761e-05,
      "loss": 1.9554,
      "step": 1513
    },
    {
      "epoch": 0.9967083607636603,
      "grad_norm": 1.637703776359558,
      "learning_rate": 1.502982210222168e-05,
      "loss": 1.6911,
      "step": 1514
    },
    {
      "epoch": 0.9973666886109283,
      "grad_norm": 1.1762462854385376,
      "learning_rate": 1.5023862451505208e-05,
      "loss": 1.6808,
      "step": 1515
    },
    {
      "epoch": 0.9980250164581962,
      "grad_norm": 3.421231508255005,
      "learning_rate": 1.5017900413090657e-05,
      "loss": 1.7216,
      "step": 1516
    },
    {
      "epoch": 0.9986833443054641,
      "grad_norm": 1.7366868257522583,
      "learning_rate": 1.5011935989811601e-05,
      "loss": 1.7255,
      "step": 1517
    },
    {
      "epoch": 0.9993416721527321,
      "grad_norm": 14.150371551513672,
      "learning_rate": 1.5005969184502765e-05,
      "loss": 2.2493,
      "step": 1518
    },
    {
      "epoch": 1.0,
      "grad_norm": 24.476743698120117,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 2.8814,
      "step": 1519
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7869822485207101,
      "eval_loss": 2.1601295471191406,
      "eval_runtime": 86.8662,
      "eval_samples_per_second": 1.946,
      "eval_steps_per_second": 1.946,
      "step": 1519
    },
    {
      "epoch": 1.000658327847268,
      "grad_norm": 0.9967989325523376,
      "learning_rate": 1.4994028439140292e-05,
      "loss": 1.692,
      "step": 1520
    },
    {
      "epoch": 1.0013166556945359,
      "grad_norm": 1.9005273580551147,
      "learning_rate": 1.4988054504761747e-05,
      "loss": 1.7149,
      "step": 1521
    },
    {
      "epoch": 1.001974983541804,
      "grad_norm": 11.551591873168945,
      "learning_rate": 1.4982078199703611e-05,
      "loss": 2.1563,
      "step": 1522
    },
    {
      "epoch": 1.0026333113890717,
      "grad_norm": 14.516192436218262,
      "learning_rate": 1.4976099526806247e-05,
      "loss": 2.1976,
      "step": 1523
    },
    {
      "epoch": 1.0032916392363398,
      "grad_norm": 3.730268716812134,
      "learning_rate": 1.497011848891115e-05,
      "loss": 1.7593,
      "step": 1524
    },
    {
      "epoch": 1.0039499670836076,
      "grad_norm": 19.794570922851562,
      "learning_rate": 1.496413508886093e-05,
      "loss": 2.4324,
      "step": 1525
    },
    {
      "epoch": 1.0046082949308757,
      "grad_norm": 1.4829739332199097,
      "learning_rate": 1.4958149329499332e-05,
      "loss": 1.6998,
      "step": 1526
    },
    {
      "epoch": 1.0052666227781435,
      "grad_norm": 15.56960391998291,
      "learning_rate": 1.4952161213671217e-05,
      "loss": 2.1648,
      "step": 1527
    },
    {
      "epoch": 1.0059249506254115,
      "grad_norm": 5.383530139923096,
      "learning_rate": 1.4946170744222559e-05,
      "loss": 1.851,
      "step": 1528
    },
    {
      "epoch": 1.0065832784726794,
      "grad_norm": 2.1571004390716553,
      "learning_rate": 1.4940177924000461e-05,
      "loss": 1.746,
      "step": 1529
    },
    {
      "epoch": 1.0072416063199474,
      "grad_norm": 2.033590316772461,
      "learning_rate": 1.4934182755853141e-05,
      "loss": 1.6983,
      "step": 1530
    },
    {
      "epoch": 1.0078999341672152,
      "grad_norm": 11.901780128479004,
      "learning_rate": 1.4928185242629927e-05,
      "loss": 2.1714,
      "step": 1531
    },
    {
      "epoch": 1.0085582620144833,
      "grad_norm": 23.05106544494629,
      "learning_rate": 1.4922185387181266e-05,
      "loss": 2.586,
      "step": 1532
    },
    {
      "epoch": 1.0092165898617511,
      "grad_norm": 16.681629180908203,
      "learning_rate": 1.4916183192358717e-05,
      "loss": 2.384,
      "step": 1533
    },
    {
      "epoch": 1.0098749177090192,
      "grad_norm": 17.31785011291504,
      "learning_rate": 1.4910178661014954e-05,
      "loss": 2.4208,
      "step": 1534
    },
    {
      "epoch": 1.010533245556287,
      "grad_norm": 33.508453369140625,
      "learning_rate": 1.4904171796003757e-05,
      "loss": 2.9313,
      "step": 1535
    },
    {
      "epoch": 1.011191573403555,
      "grad_norm": 1.9342448711395264,
      "learning_rate": 1.489816260018002e-05,
      "loss": 1.69,
      "step": 1536
    },
    {
      "epoch": 1.0118499012508229,
      "grad_norm": 16.025354385375977,
      "learning_rate": 1.4892151076399736e-05,
      "loss": 2.7557,
      "step": 1537
    },
    {
      "epoch": 1.012508229098091,
      "grad_norm": 1.6741913557052612,
      "learning_rate": 1.4886137227520017e-05,
      "loss": 1.7264,
      "step": 1538
    },
    {
      "epoch": 1.0131665569453587,
      "grad_norm": 2.153742551803589,
      "learning_rate": 1.4880121056399071e-05,
      "loss": 1.7025,
      "step": 1539
    },
    {
      "epoch": 1.0138248847926268,
      "grad_norm": 2.188922166824341,
      "learning_rate": 1.4874102565896206e-05,
      "loss": 1.6872,
      "step": 1540
    },
    {
      "epoch": 1.0144832126398946,
      "grad_norm": 17.9965763092041,
      "learning_rate": 1.4868081758871848e-05,
      "loss": 2.4166,
      "step": 1541
    },
    {
      "epoch": 1.0151415404871627,
      "grad_norm": 9.533121109008789,
      "learning_rate": 1.4862058638187509e-05,
      "loss": 2.0269,
      "step": 1542
    },
    {
      "epoch": 1.0157998683344305,
      "grad_norm": 1.7426974773406982,
      "learning_rate": 1.4856033206705807e-05,
      "loss": 1.6838,
      "step": 1543
    },
    {
      "epoch": 1.0164581961816985,
      "grad_norm": 3.2472872734069824,
      "learning_rate": 1.4850005467290456e-05,
      "loss": 1.7294,
      "step": 1544
    },
    {
      "epoch": 1.0171165240289664,
      "grad_norm": 10.53343677520752,
      "learning_rate": 1.4843975422806269e-05,
      "loss": 2.1076,
      "step": 1545
    },
    {
      "epoch": 1.0177748518762344,
      "grad_norm": 19.5991268157959,
      "learning_rate": 1.4837943076119153e-05,
      "loss": 2.3516,
      "step": 1546
    },
    {
      "epoch": 1.0184331797235022,
      "grad_norm": 3.39570689201355,
      "learning_rate": 1.4831908430096113e-05,
      "loss": 1.8584,
      "step": 1547
    },
    {
      "epoch": 1.0190915075707703,
      "grad_norm": 11.369239807128906,
      "learning_rate": 1.4825871487605238e-05,
      "loss": 2.1484,
      "step": 1548
    },
    {
      "epoch": 1.019749835418038,
      "grad_norm": 1.8339437246322632,
      "learning_rate": 1.481983225151572e-05,
      "loss": 1.6786,
      "step": 1549
    },
    {
      "epoch": 1.0204081632653061,
      "grad_norm": 1.506347894668579,
      "learning_rate": 1.4813790724697832e-05,
      "loss": 1.6951,
      "step": 1550
    },
    {
      "epoch": 1.021066491112574,
      "grad_norm": 4.948267936706543,
      "learning_rate": 1.4807746910022938e-05,
      "loss": 1.8721,
      "step": 1551
    },
    {
      "epoch": 1.021724818959842,
      "grad_norm": 5.751075744628906,
      "learning_rate": 1.4801700810363491e-05,
      "loss": 2.0936,
      "step": 1552
    },
    {
      "epoch": 1.0223831468071098,
      "grad_norm": 13.08823013305664,
      "learning_rate": 1.479565242859303e-05,
      "loss": 2.4265,
      "step": 1553
    },
    {
      "epoch": 1.023041474654378,
      "grad_norm": 7.13596248626709,
      "learning_rate": 1.4789601767586174e-05,
      "loss": 2.1438,
      "step": 1554
    },
    {
      "epoch": 1.0236998025016457,
      "grad_norm": 3.5262410640716553,
      "learning_rate": 1.4783548830218636e-05,
      "loss": 1.731,
      "step": 1555
    },
    {
      "epoch": 1.0243581303489138,
      "grad_norm": 7.38067102432251,
      "learning_rate": 1.4777493619367201e-05,
      "loss": 1.7857,
      "step": 1556
    },
    {
      "epoch": 1.0250164581961816,
      "grad_norm": 9.901531219482422,
      "learning_rate": 1.4771436137909736e-05,
      "loss": 2.1989,
      "step": 1557
    },
    {
      "epoch": 1.0256747860434496,
      "grad_norm": 18.631704330444336,
      "learning_rate": 1.4765376388725185e-05,
      "loss": 2.368,
      "step": 1558
    },
    {
      "epoch": 1.0263331138907177,
      "grad_norm": 2.629709005355835,
      "learning_rate": 1.4759314374693585e-05,
      "loss": 1.7008,
      "step": 1559
    },
    {
      "epoch": 1.0269914417379855,
      "grad_norm": 13.570405960083008,
      "learning_rate": 1.4753250098696026e-05,
      "loss": 2.2806,
      "step": 1560
    },
    {
      "epoch": 1.0276497695852536,
      "grad_norm": 3.2647979259490967,
      "learning_rate": 1.4747183563614692e-05,
      "loss": 1.7135,
      "step": 1561
    },
    {
      "epoch": 1.0283080974325214,
      "grad_norm": 10.702733993530273,
      "learning_rate": 1.4741114772332829e-05,
      "loss": 2.0006,
      "step": 1562
    },
    {
      "epoch": 1.0289664252797894,
      "grad_norm": 18.384620666503906,
      "learning_rate": 1.4735043727734762e-05,
      "loss": 2.3472,
      "step": 1563
    },
    {
      "epoch": 1.0296247531270573,
      "grad_norm": 1.7453110218048096,
      "learning_rate": 1.4728970432705887e-05,
      "loss": 1.6762,
      "step": 1564
    },
    {
      "epoch": 1.0302830809743253,
      "grad_norm": 5.201823711395264,
      "learning_rate": 1.4722894890132661e-05,
      "loss": 1.7183,
      "step": 1565
    },
    {
      "epoch": 1.0309414088215931,
      "grad_norm": 1.5069912672042847,
      "learning_rate": 1.471681710290262e-05,
      "loss": 1.6912,
      "step": 1566
    },
    {
      "epoch": 1.0315997366688612,
      "grad_norm": 18.368465423583984,
      "learning_rate": 1.4710737073904366e-05,
      "loss": 2.4661,
      "step": 1567
    },
    {
      "epoch": 1.032258064516129,
      "grad_norm": 10.454472541809082,
      "learning_rate": 1.4704654806027558e-05,
      "loss": 1.9667,
      "step": 1568
    },
    {
      "epoch": 1.032916392363397,
      "grad_norm": 8.820452690124512,
      "learning_rate": 1.4698570302162922e-05,
      "loss": 2.0676,
      "step": 1569
    },
    {
      "epoch": 1.0335747202106649,
      "grad_norm": 6.951918125152588,
      "learning_rate": 1.4692483565202254e-05,
      "loss": 2.0448,
      "step": 1570
    },
    {
      "epoch": 1.034233048057933,
      "grad_norm": 1.8792846202850342,
      "learning_rate": 1.4686394598038403e-05,
      "loss": 1.6972,
      "step": 1571
    },
    {
      "epoch": 1.0348913759052007,
      "grad_norm": 17.490827560424805,
      "learning_rate": 1.4680303403565279e-05,
      "loss": 2.2952,
      "step": 1572
    },
    {
      "epoch": 1.0355497037524688,
      "grad_norm": 2.649533748626709,
      "learning_rate": 1.4674209984677857e-05,
      "loss": 1.7072,
      "step": 1573
    },
    {
      "epoch": 1.0362080315997366,
      "grad_norm": 7.758599281311035,
      "learning_rate": 1.466811434427216e-05,
      "loss": 2.0326,
      "step": 1574
    },
    {
      "epoch": 1.0368663594470047,
      "grad_norm": 1.100378394126892,
      "learning_rate": 1.4662016485245274e-05,
      "loss": 1.6766,
      "step": 1575
    },
    {
      "epoch": 1.0375246872942725,
      "grad_norm": 25.942365646362305,
      "learning_rate": 1.465591641049534e-05,
      "loss": 3.4204,
      "step": 1576
    },
    {
      "epoch": 1.0381830151415405,
      "grad_norm": 15.75553035736084,
      "learning_rate": 1.4649814122921541e-05,
      "loss": 2.68,
      "step": 1577
    },
    {
      "epoch": 1.0388413429888084,
      "grad_norm": 12.482097625732422,
      "learning_rate": 1.464370962542413e-05,
      "loss": 2.3045,
      "step": 1578
    },
    {
      "epoch": 1.0394996708360764,
      "grad_norm": 1.840010166168213,
      "learning_rate": 1.463760292090439e-05,
      "loss": 1.7034,
      "step": 1579
    },
    {
      "epoch": 1.0401579986833442,
      "grad_norm": 11.811174392700195,
      "learning_rate": 1.4631494012264668e-05,
      "loss": 2.4136,
      "step": 1580
    },
    {
      "epoch": 1.0408163265306123,
      "grad_norm": 10.025636672973633,
      "learning_rate": 1.4625382902408356e-05,
      "loss": 2.0965,
      "step": 1581
    },
    {
      "epoch": 1.0414746543778801,
      "grad_norm": 23.271089553833008,
      "learning_rate": 1.4619269594239882e-05,
      "loss": 3.465,
      "step": 1582
    },
    {
      "epoch": 1.0421329822251482,
      "grad_norm": 23.733970642089844,
      "learning_rate": 1.4613154090664731e-05,
      "loss": 2.4473,
      "step": 1583
    },
    {
      "epoch": 1.042791310072416,
      "grad_norm": 13.997611045837402,
      "learning_rate": 1.460703639458943e-05,
      "loss": 2.2099,
      "step": 1584
    },
    {
      "epoch": 1.043449637919684,
      "grad_norm": 14.502992630004883,
      "learning_rate": 1.4600916508921542e-05,
      "loss": 2.1618,
      "step": 1585
    },
    {
      "epoch": 1.0441079657669519,
      "grad_norm": 6.958011627197266,
      "learning_rate": 1.459479443656967e-05,
      "loss": 1.8493,
      "step": 1586
    },
    {
      "epoch": 1.04476629361422,
      "grad_norm": 10.59860897064209,
      "learning_rate": 1.4588670180443467e-05,
      "loss": 2.1091,
      "step": 1587
    },
    {
      "epoch": 1.0454246214614877,
      "grad_norm": 16.643491744995117,
      "learning_rate": 1.458254374345361e-05,
      "loss": 2.122,
      "step": 1588
    },
    {
      "epoch": 1.0460829493087558,
      "grad_norm": 15.559163093566895,
      "learning_rate": 1.4576415128511823e-05,
      "loss": 2.1733,
      "step": 1589
    },
    {
      "epoch": 1.0467412771560236,
      "grad_norm": 22.058393478393555,
      "learning_rate": 1.4570284338530862e-05,
      "loss": 3.1607,
      "step": 1590
    },
    {
      "epoch": 1.0473996050032917,
      "grad_norm": 10.787014961242676,
      "learning_rate": 1.4564151376424511e-05,
      "loss": 2.0802,
      "step": 1591
    },
    {
      "epoch": 1.0480579328505595,
      "grad_norm": 2.0345497131347656,
      "learning_rate": 1.4558016245107595e-05,
      "loss": 1.687,
      "step": 1592
    },
    {
      "epoch": 1.0487162606978275,
      "grad_norm": 12.226879119873047,
      "learning_rate": 1.455187894749597e-05,
      "loss": 1.9507,
      "step": 1593
    },
    {
      "epoch": 1.0493745885450954,
      "grad_norm": 26.418607711791992,
      "learning_rate": 1.4545739486506511e-05,
      "loss": 2.8459,
      "step": 1594
    },
    {
      "epoch": 1.0500329163923634,
      "grad_norm": 2.561415195465088,
      "learning_rate": 1.4539597865057133e-05,
      "loss": 1.7139,
      "step": 1595
    },
    {
      "epoch": 1.0506912442396312,
      "grad_norm": 1.5338786840438843,
      "learning_rate": 1.4533454086066772e-05,
      "loss": 1.6884,
      "step": 1596
    },
    {
      "epoch": 1.0513495720868993,
      "grad_norm": 2.546710968017578,
      "learning_rate": 1.4527308152455388e-05,
      "loss": 1.6772,
      "step": 1597
    },
    {
      "epoch": 1.0520078999341673,
      "grad_norm": 12.763628005981445,
      "learning_rate": 1.4521160067143973e-05,
      "loss": 2.3785,
      "step": 1598
    },
    {
      "epoch": 1.0526662277814351,
      "grad_norm": 12.328516006469727,
      "learning_rate": 1.4515009833054527e-05,
      "loss": 2.2401,
      "step": 1599
    },
    {
      "epoch": 1.0533245556287032,
      "grad_norm": 1.6110442876815796,
      "learning_rate": 1.4508857453110089e-05,
      "loss": 1.6939,
      "step": 1600
    },
    {
      "epoch": 1.053982883475971,
      "grad_norm": 2.0307793617248535,
      "learning_rate": 1.4502702930234705e-05,
      "loss": 1.7236,
      "step": 1601
    },
    {
      "epoch": 1.054641211323239,
      "grad_norm": 9.994885444641113,
      "learning_rate": 1.4496546267353444e-05,
      "loss": 2.1324,
      "step": 1602
    },
    {
      "epoch": 1.055299539170507,
      "grad_norm": 4.915426731109619,
      "learning_rate": 1.4490387467392391e-05,
      "loss": 2.0638,
      "step": 1603
    },
    {
      "epoch": 1.055957867017775,
      "grad_norm": 9.225076675415039,
      "learning_rate": 1.4484226533278649e-05,
      "loss": 2.2489,
      "step": 1604
    },
    {
      "epoch": 1.0566161948650428,
      "grad_norm": 8.637768745422363,
      "learning_rate": 1.4478063467940333e-05,
      "loss": 1.9846,
      "step": 1605
    },
    {
      "epoch": 1.0572745227123108,
      "grad_norm": 3.5137827396392822,
      "learning_rate": 1.4471898274306572e-05,
      "loss": 1.7085,
      "step": 1606
    },
    {
      "epoch": 1.0579328505595786,
      "grad_norm": 11.592036247253418,
      "learning_rate": 1.4465730955307507e-05,
      "loss": 2.2722,
      "step": 1607
    },
    {
      "epoch": 1.0585911784068467,
      "grad_norm": 16.675756454467773,
      "learning_rate": 1.4459561513874285e-05,
      "loss": 2.098,
      "step": 1608
    },
    {
      "epoch": 1.0592495062541145,
      "grad_norm": 15.110020637512207,
      "learning_rate": 1.4453389952939067e-05,
      "loss": 2.2237,
      "step": 1609
    },
    {
      "epoch": 1.0599078341013826,
      "grad_norm": 27.170583724975586,
      "learning_rate": 1.4447216275435023e-05,
      "loss": 3.934,
      "step": 1610
    },
    {
      "epoch": 1.0605661619486504,
      "grad_norm": 17.741079330444336,
      "learning_rate": 1.4441040484296318e-05,
      "loss": 2.5622,
      "step": 1611
    },
    {
      "epoch": 1.0612244897959184,
      "grad_norm": 7.288865089416504,
      "learning_rate": 1.4434862582458136e-05,
      "loss": 2.0465,
      "step": 1612
    },
    {
      "epoch": 1.0618828176431863,
      "grad_norm": 11.377060890197754,
      "learning_rate": 1.4428682572856651e-05,
      "loss": 2.4735,
      "step": 1613
    },
    {
      "epoch": 1.0625411454904543,
      "grad_norm": 1.9301809072494507,
      "learning_rate": 1.442250045842905e-05,
      "loss": 1.6771,
      "step": 1614
    },
    {
      "epoch": 1.0631994733377221,
      "grad_norm": 6.502710819244385,
      "learning_rate": 1.441631624211351e-05,
      "loss": 2.0409,
      "step": 1615
    },
    {
      "epoch": 1.0638578011849902,
      "grad_norm": 7.817668914794922,
      "learning_rate": 1.4410129926849216e-05,
      "loss": 2.0061,
      "step": 1616
    },
    {
      "epoch": 1.064516129032258,
      "grad_norm": 18.7595272064209,
      "learning_rate": 1.4403941515576344e-05,
      "loss": 2.529,
      "step": 1617
    },
    {
      "epoch": 1.065174456879526,
      "grad_norm": 9.810445785522461,
      "learning_rate": 1.4397751011236072e-05,
      "loss": 1.9365,
      "step": 1618
    },
    {
      "epoch": 1.0658327847267939,
      "grad_norm": 9.460236549377441,
      "learning_rate": 1.4391558416770566e-05,
      "loss": 2.2298,
      "step": 1619
    },
    {
      "epoch": 1.066491112574062,
      "grad_norm": 7.709314823150635,
      "learning_rate": 1.438536373512299e-05,
      "loss": 2.0512,
      "step": 1620
    },
    {
      "epoch": 1.0671494404213298,
      "grad_norm": 10.202515602111816,
      "learning_rate": 1.43791669692375e-05,
      "loss": 2.223,
      "step": 1621
    },
    {
      "epoch": 1.0678077682685978,
      "grad_norm": 6.1393141746521,
      "learning_rate": 1.4372968122059244e-05,
      "loss": 1.9424,
      "step": 1622
    },
    {
      "epoch": 1.0684660961158656,
      "grad_norm": 14.80754280090332,
      "learning_rate": 1.436676719653435e-05,
      "loss": 1.9447,
      "step": 1623
    },
    {
      "epoch": 1.0691244239631337,
      "grad_norm": 1.7694611549377441,
      "learning_rate": 1.4360564195609948e-05,
      "loss": 1.7036,
      "step": 1624
    },
    {
      "epoch": 1.0697827518104015,
      "grad_norm": 5.938704013824463,
      "learning_rate": 1.4354359122234138e-05,
      "loss": 1.9511,
      "step": 1625
    },
    {
      "epoch": 1.0704410796576695,
      "grad_norm": 13.351210594177246,
      "learning_rate": 1.4348151979356021e-05,
      "loss": 2.3351,
      "step": 1626
    },
    {
      "epoch": 1.0710994075049374,
      "grad_norm": 18.637514114379883,
      "learning_rate": 1.4341942769925677e-05,
      "loss": 2.3903,
      "step": 1627
    },
    {
      "epoch": 1.0717577353522054,
      "grad_norm": 18.26291275024414,
      "learning_rate": 1.4335731496894156e-05,
      "loss": 2.2972,
      "step": 1628
    },
    {
      "epoch": 1.0724160631994732,
      "grad_norm": 10.545997619628906,
      "learning_rate": 1.4329518163213503e-05,
      "loss": 2.2336,
      "step": 1629
    },
    {
      "epoch": 1.0730743910467413,
      "grad_norm": 11.93476390838623,
      "learning_rate": 1.4323302771836743e-05,
      "loss": 2.188,
      "step": 1630
    },
    {
      "epoch": 1.0737327188940091,
      "grad_norm": 8.00173282623291,
      "learning_rate": 1.4317085325717863e-05,
      "loss": 2.0878,
      "step": 1631
    },
    {
      "epoch": 1.0743910467412772,
      "grad_norm": 9.024178504943848,
      "learning_rate": 1.4310865827811848e-05,
      "loss": 2.1162,
      "step": 1632
    },
    {
      "epoch": 1.075049374588545,
      "grad_norm": 10.711618423461914,
      "learning_rate": 1.430464428107464e-05,
      "loss": 2.115,
      "step": 1633
    },
    {
      "epoch": 1.075707702435813,
      "grad_norm": 2.6489179134368896,
      "learning_rate": 1.4298420688463163e-05,
      "loss": 1.6772,
      "step": 1634
    },
    {
      "epoch": 1.076366030283081,
      "grad_norm": 8.089950561523438,
      "learning_rate": 1.4292195052935319e-05,
      "loss": 2.015,
      "step": 1635
    },
    {
      "epoch": 1.077024358130349,
      "grad_norm": 4.13426399230957,
      "learning_rate": 1.4285967377449965e-05,
      "loss": 1.7215,
      "step": 1636
    },
    {
      "epoch": 1.0776826859776167,
      "grad_norm": 7.189306259155273,
      "learning_rate": 1.4279737664966942e-05,
      "loss": 2.0244,
      "step": 1637
    },
    {
      "epoch": 1.0783410138248848,
      "grad_norm": 4.9655561447143555,
      "learning_rate": 1.4273505918447054e-05,
      "loss": 1.7303,
      "step": 1638
    },
    {
      "epoch": 1.0789993416721528,
      "grad_norm": 11.976630210876465,
      "learning_rate": 1.4267272140852074e-05,
      "loss": 1.8429,
      "step": 1639
    },
    {
      "epoch": 1.0796576695194207,
      "grad_norm": 18.06719398498535,
      "learning_rate": 1.4261036335144733e-05,
      "loss": 2.6124,
      "step": 1640
    },
    {
      "epoch": 1.0803159973666887,
      "grad_norm": 11.495634078979492,
      "learning_rate": 1.4254798504288737e-05,
      "loss": 2.0867,
      "step": 1641
    },
    {
      "epoch": 1.0809743252139565,
      "grad_norm": 7.002506732940674,
      "learning_rate": 1.424855865124874e-05,
      "loss": 1.9228,
      "step": 1642
    },
    {
      "epoch": 1.0816326530612246,
      "grad_norm": 1.5558847188949585,
      "learning_rate": 1.4242316778990373e-05,
      "loss": 1.6646,
      "step": 1643
    },
    {
      "epoch": 1.0822909809084924,
      "grad_norm": 1.3953697681427002,
      "learning_rate": 1.4236072890480219e-05,
      "loss": 1.6687,
      "step": 1644
    },
    {
      "epoch": 1.0829493087557605,
      "grad_norm": 9.513219833374023,
      "learning_rate": 1.422982698868582e-05,
      "loss": 2.028,
      "step": 1645
    },
    {
      "epoch": 1.0836076366030283,
      "grad_norm": 9.001358032226562,
      "learning_rate": 1.4223579076575664e-05,
      "loss": 1.9084,
      "step": 1646
    },
    {
      "epoch": 1.0842659644502963,
      "grad_norm": 15.38205623626709,
      "learning_rate": 1.4217329157119219e-05,
      "loss": 2.0882,
      "step": 1647
    },
    {
      "epoch": 1.0849242922975642,
      "grad_norm": 10.122753143310547,
      "learning_rate": 1.4211077233286886e-05,
      "loss": 2.1351,
      "step": 1648
    },
    {
      "epoch": 1.0855826201448322,
      "grad_norm": 1.9417831897735596,
      "learning_rate": 1.4204823308050026e-05,
      "loss": 1.6772,
      "step": 1649
    },
    {
      "epoch": 1.0862409479921,
      "grad_norm": 11.179678916931152,
      "learning_rate": 1.419856738438095e-05,
      "loss": 2.0567,
      "step": 1650
    },
    {
      "epoch": 1.086899275839368,
      "grad_norm": 17.15338897705078,
      "learning_rate": 1.419230946525292e-05,
      "loss": 2.2805,
      "step": 1651
    },
    {
      "epoch": 1.087557603686636,
      "grad_norm": 4.462803363800049,
      "learning_rate": 1.4186049553640152e-05,
      "loss": 1.8585,
      "step": 1652
    },
    {
      "epoch": 1.088215931533904,
      "grad_norm": 1.1892141103744507,
      "learning_rate": 1.4179787652517791e-05,
      "loss": 1.6651,
      "step": 1653
    },
    {
      "epoch": 1.0888742593811718,
      "grad_norm": 2.4644320011138916,
      "learning_rate": 1.417352376486195e-05,
      "loss": 1.7133,
      "step": 1654
    },
    {
      "epoch": 1.0895325872284398,
      "grad_norm": 4.6223530769348145,
      "learning_rate": 1.4167257893649668e-05,
      "loss": 1.7043,
      "step": 1655
    },
    {
      "epoch": 1.0901909150757076,
      "grad_norm": 9.578054428100586,
      "learning_rate": 1.4160990041858942e-05,
      "loss": 2.1653,
      "step": 1656
    },
    {
      "epoch": 1.0908492429229757,
      "grad_norm": 16.972991943359375,
      "learning_rate": 1.4154720212468696e-05,
      "loss": 2.976,
      "step": 1657
    },
    {
      "epoch": 1.0915075707702435,
      "grad_norm": 5.593091011047363,
      "learning_rate": 1.4148448408458804e-05,
      "loss": 1.799,
      "step": 1658
    },
    {
      "epoch": 1.0921658986175116,
      "grad_norm": 1.3277466297149658,
      "learning_rate": 1.4142174632810074e-05,
      "loss": 1.6629,
      "step": 1659
    },
    {
      "epoch": 1.0928242264647794,
      "grad_norm": 5.400049209594727,
      "learning_rate": 1.4135898888504249e-05,
      "loss": 1.8638,
      "step": 1660
    },
    {
      "epoch": 1.0934825543120474,
      "grad_norm": 22.89813232421875,
      "learning_rate": 1.4129621178524018e-05,
      "loss": 2.4758,
      "step": 1661
    },
    {
      "epoch": 1.0941408821593153,
      "grad_norm": 2.299203872680664,
      "learning_rate": 1.4123341505852992e-05,
      "loss": 1.6621,
      "step": 1662
    },
    {
      "epoch": 1.0947992100065833,
      "grad_norm": 21.950231552124023,
      "learning_rate": 1.4117059873475715e-05,
      "loss": 2.6196,
      "step": 1663
    },
    {
      "epoch": 1.0954575378538511,
      "grad_norm": 12.324789047241211,
      "learning_rate": 1.4110776284377681e-05,
      "loss": 2.1081,
      "step": 1664
    },
    {
      "epoch": 1.0961158657011192,
      "grad_norm": 2.0326664447784424,
      "learning_rate": 1.4104490741545286e-05,
      "loss": 1.6775,
      "step": 1665
    },
    {
      "epoch": 1.096774193548387,
      "grad_norm": 21.241180419921875,
      "learning_rate": 1.4098203247965876e-05,
      "loss": 2.4958,
      "step": 1666
    },
    {
      "epoch": 1.097432521395655,
      "grad_norm": 1.7041887044906616,
      "learning_rate": 1.4091913806627716e-05,
      "loss": 1.6498,
      "step": 1667
    },
    {
      "epoch": 1.0980908492429229,
      "grad_norm": 11.38552188873291,
      "learning_rate": 1.4085622420519996e-05,
      "loss": 1.896,
      "step": 1668
    },
    {
      "epoch": 1.098749177090191,
      "grad_norm": 11.149231910705566,
      "learning_rate": 1.4079329092632835e-05,
      "loss": 2.0171,
      "step": 1669
    },
    {
      "epoch": 1.0994075049374588,
      "grad_norm": 7.436975002288818,
      "learning_rate": 1.4073033825957266e-05,
      "loss": 1.9808,
      "step": 1670
    },
    {
      "epoch": 1.1000658327847268,
      "grad_norm": 13.320795059204102,
      "learning_rate": 1.406673662348525e-05,
      "loss": 2.0513,
      "step": 1671
    },
    {
      "epoch": 1.1007241606319946,
      "grad_norm": 13.036723136901855,
      "learning_rate": 1.4060437488209675e-05,
      "loss": 2.0002,
      "step": 1672
    },
    {
      "epoch": 1.1013824884792627,
      "grad_norm": 7.175157070159912,
      "learning_rate": 1.4054136423124331e-05,
      "loss": 1.8345,
      "step": 1673
    },
    {
      "epoch": 1.1020408163265305,
      "grad_norm": 24.480819702148438,
      "learning_rate": 1.4047833431223938e-05,
      "loss": 3.036,
      "step": 1674
    },
    {
      "epoch": 1.1026991441737986,
      "grad_norm": 9.283326148986816,
      "learning_rate": 1.404152851550413e-05,
      "loss": 1.8829,
      "step": 1675
    },
    {
      "epoch": 1.1033574720210666,
      "grad_norm": 25.931987762451172,
      "learning_rate": 1.4035221678961446e-05,
      "loss": 3.6042,
      "step": 1676
    },
    {
      "epoch": 1.1040157998683344,
      "grad_norm": 8.63277816772461,
      "learning_rate": 1.402891292459335e-05,
      "loss": 1.9058,
      "step": 1677
    },
    {
      "epoch": 1.1046741277156025,
      "grad_norm": 12.955074310302734,
      "learning_rate": 1.4022602255398212e-05,
      "loss": 2.1108,
      "step": 1678
    },
    {
      "epoch": 1.1053324555628703,
      "grad_norm": 30.311065673828125,
      "learning_rate": 1.4016289674375315e-05,
      "loss": 2.9556,
      "step": 1679
    },
    {
      "epoch": 1.1059907834101383,
      "grad_norm": 1.787973165512085,
      "learning_rate": 1.400997518452484e-05,
      "loss": 1.6497,
      "step": 1680
    },
    {
      "epoch": 1.1066491112574062,
      "grad_norm": 26.279972076416016,
      "learning_rate": 1.4003658788847892e-05,
      "loss": 3.1446,
      "step": 1681
    },
    {
      "epoch": 1.1073074391046742,
      "grad_norm": 13.662239074707031,
      "learning_rate": 1.3997340490346467e-05,
      "loss": 2.2473,
      "step": 1682
    },
    {
      "epoch": 1.107965766951942,
      "grad_norm": 4.309826374053955,
      "learning_rate": 1.3991020292023474e-05,
      "loss": 1.7118,
      "step": 1683
    },
    {
      "epoch": 1.10862409479921,
      "grad_norm": 9.368520736694336,
      "learning_rate": 1.3984698196882725e-05,
      "loss": 1.8049,
      "step": 1684
    },
    {
      "epoch": 1.109282422646478,
      "grad_norm": 13.989630699157715,
      "learning_rate": 1.3978374207928927e-05,
      "loss": 2.0298,
      "step": 1685
    },
    {
      "epoch": 1.109940750493746,
      "grad_norm": 13.818750381469727,
      "learning_rate": 1.3972048328167693e-05,
      "loss": 2.2672,
      "step": 1686
    },
    {
      "epoch": 1.1105990783410138,
      "grad_norm": 10.514667510986328,
      "learning_rate": 1.3965720560605533e-05,
      "loss": 2.025,
      "step": 1687
    },
    {
      "epoch": 1.1112574061882818,
      "grad_norm": 23.16960334777832,
      "learning_rate": 1.3959390908249853e-05,
      "loss": 3.1246,
      "step": 1688
    },
    {
      "epoch": 1.1119157340355497,
      "grad_norm": 1.897141456604004,
      "learning_rate": 1.3953059374108955e-05,
      "loss": 1.6444,
      "step": 1689
    },
    {
      "epoch": 1.1125740618828177,
      "grad_norm": 16.567745208740234,
      "learning_rate": 1.3946725961192036e-05,
      "loss": 2.4982,
      "step": 1690
    },
    {
      "epoch": 1.1132323897300855,
      "grad_norm": 4.97568416595459,
      "learning_rate": 1.3940390672509185e-05,
      "loss": 1.8735,
      "step": 1691
    },
    {
      "epoch": 1.1138907175773536,
      "grad_norm": 3.4045462608337402,
      "learning_rate": 1.393405351107139e-05,
      "loss": 1.6871,
      "step": 1692
    },
    {
      "epoch": 1.1145490454246214,
      "grad_norm": 8.413626670837402,
      "learning_rate": 1.3927714479890515e-05,
      "loss": 2.1061,
      "step": 1693
    },
    {
      "epoch": 1.1152073732718895,
      "grad_norm": 5.07010555267334,
      "learning_rate": 1.3921373581979323e-05,
      "loss": 1.7001,
      "step": 1694
    },
    {
      "epoch": 1.1158657011191573,
      "grad_norm": 4.921260356903076,
      "learning_rate": 1.3915030820351462e-05,
      "loss": 1.7198,
      "step": 1695
    },
    {
      "epoch": 1.1165240289664253,
      "grad_norm": 4.990451812744141,
      "learning_rate": 1.3908686198021464e-05,
      "loss": 1.7255,
      "step": 1696
    },
    {
      "epoch": 1.1171823568136932,
      "grad_norm": 15.664505004882812,
      "learning_rate": 1.3902339718004744e-05,
      "loss": 2.2282,
      "step": 1697
    },
    {
      "epoch": 1.1178406846609612,
      "grad_norm": 15.480743408203125,
      "learning_rate": 1.389599138331761e-05,
      "loss": 2.3876,
      "step": 1698
    },
    {
      "epoch": 1.118499012508229,
      "grad_norm": 6.445571422576904,
      "learning_rate": 1.388964119697724e-05,
      "loss": 1.9775,
      "step": 1699
    },
    {
      "epoch": 1.119157340355497,
      "grad_norm": 10.996891021728516,
      "learning_rate": 1.3883289162001688e-05,
      "loss": 2.22,
      "step": 1700
    },
    {
      "epoch": 1.119815668202765,
      "grad_norm": 23.442590713500977,
      "learning_rate": 1.3876935281409907e-05,
      "loss": 2.4213,
      "step": 1701
    },
    {
      "epoch": 1.120473996050033,
      "grad_norm": 7.403151035308838,
      "learning_rate": 1.3870579558221709e-05,
      "loss": 1.9905,
      "step": 1702
    },
    {
      "epoch": 1.1211323238973008,
      "grad_norm": 29.225818634033203,
      "learning_rate": 1.3864221995457783e-05,
      "loss": 2.802,
      "step": 1703
    },
    {
      "epoch": 1.1217906517445688,
      "grad_norm": 9.16978645324707,
      "learning_rate": 1.3857862596139704e-05,
      "loss": 2.0123,
      "step": 1704
    },
    {
      "epoch": 1.1224489795918366,
      "grad_norm": 5.134195327758789,
      "learning_rate": 1.3851501363289907e-05,
      "loss": 1.7066,
      "step": 1705
    },
    {
      "epoch": 1.1231073074391047,
      "grad_norm": 14.92072582244873,
      "learning_rate": 1.3845138299931713e-05,
      "loss": 2.4461,
      "step": 1706
    },
    {
      "epoch": 1.1237656352863725,
      "grad_norm": 3.0555899143218994,
      "learning_rate": 1.3838773409089292e-05,
      "loss": 1.7519,
      "step": 1707
    },
    {
      "epoch": 1.1244239631336406,
      "grad_norm": 2.393404245376587,
      "learning_rate": 1.38324066937877e-05,
      "loss": 1.6915,
      "step": 1708
    },
    {
      "epoch": 1.1250822909809086,
      "grad_norm": 6.35651159286499,
      "learning_rate": 1.382603815705286e-05,
      "loss": 1.9718,
      "step": 1709
    },
    {
      "epoch": 1.1257406188281764,
      "grad_norm": 16.903745651245117,
      "learning_rate": 1.3819667801911545e-05,
      "loss": 2.2736,
      "step": 1710
    },
    {
      "epoch": 1.1263989466754443,
      "grad_norm": 6.9963698387146,
      "learning_rate": 1.3813295631391408e-05,
      "loss": 1.7663,
      "step": 1711
    },
    {
      "epoch": 1.1270572745227123,
      "grad_norm": 14.858177185058594,
      "learning_rate": 1.3806921648520963e-05,
      "loss": 2.1448,
      "step": 1712
    },
    {
      "epoch": 1.1277156023699804,
      "grad_norm": 2.7982592582702637,
      "learning_rate": 1.3800545856329572e-05,
      "loss": 1.6837,
      "step": 1713
    },
    {
      "epoch": 1.1283739302172482,
      "grad_norm": 8.691100120544434,
      "learning_rate": 1.3794168257847476e-05,
      "loss": 2.1772,
      "step": 1714
    },
    {
      "epoch": 1.129032258064516,
      "grad_norm": 1.6037020683288574,
      "learning_rate": 1.3787788856105762e-05,
      "loss": 1.6717,
      "step": 1715
    },
    {
      "epoch": 1.129690585911784,
      "grad_norm": 7.872560501098633,
      "learning_rate": 1.3781407654136376e-05,
      "loss": 1.789,
      "step": 1716
    },
    {
      "epoch": 1.130348913759052,
      "grad_norm": 6.874677658081055,
      "learning_rate": 1.3775024654972118e-05,
      "loss": 1.9646,
      "step": 1717
    },
    {
      "epoch": 1.13100724160632,
      "grad_norm": 9.836590766906738,
      "learning_rate": 1.3768639861646651e-05,
      "loss": 2.2421,
      "step": 1718
    },
    {
      "epoch": 1.131665569453588,
      "grad_norm": 12.844285011291504,
      "learning_rate": 1.3762253277194481e-05,
      "loss": 2.0966,
      "step": 1719
    },
    {
      "epoch": 1.1323238973008558,
      "grad_norm": 2.242692708969116,
      "learning_rate": 1.3755864904650969e-05,
      "loss": 1.6759,
      "step": 1720
    },
    {
      "epoch": 1.1329822251481239,
      "grad_norm": 1.7360243797302246,
      "learning_rate": 1.3749474747052327e-05,
      "loss": 1.6693,
      "step": 1721
    },
    {
      "epoch": 1.1336405529953917,
      "grad_norm": 6.595155715942383,
      "learning_rate": 1.3743082807435615e-05,
      "loss": 1.7972,
      "step": 1722
    },
    {
      "epoch": 1.1342988808426597,
      "grad_norm": 2.634596109390259,
      "learning_rate": 1.3736689088838733e-05,
      "loss": 1.6762,
      "step": 1723
    },
    {
      "epoch": 1.1349572086899276,
      "grad_norm": 18.46588706970215,
      "learning_rate": 1.3730293594300439e-05,
      "loss": 2.646,
      "step": 1724
    },
    {
      "epoch": 1.1356155365371956,
      "grad_norm": 7.436661720275879,
      "learning_rate": 1.3723896326860324e-05,
      "loss": 2.0954,
      "step": 1725
    },
    {
      "epoch": 1.1362738643844634,
      "grad_norm": 13.768329620361328,
      "learning_rate": 1.3717497289558832e-05,
      "loss": 2.3737,
      "step": 1726
    },
    {
      "epoch": 1.1369321922317315,
      "grad_norm": 19.23149871826172,
      "learning_rate": 1.3711096485437239e-05,
      "loss": 2.4477,
      "step": 1727
    },
    {
      "epoch": 1.1375905200789993,
      "grad_norm": 22.613115310668945,
      "learning_rate": 1.3704693917537661e-05,
      "loss": 2.4361,
      "step": 1728
    },
    {
      "epoch": 1.1382488479262673,
      "grad_norm": 22.42805290222168,
      "learning_rate": 1.3698289588903062e-05,
      "loss": 2.4699,
      "step": 1729
    },
    {
      "epoch": 1.1389071757735352,
      "grad_norm": 13.957030296325684,
      "learning_rate": 1.369188350257723e-05,
      "loss": 2.3175,
      "step": 1730
    },
    {
      "epoch": 1.1395655036208032,
      "grad_norm": 5.9900054931640625,
      "learning_rate": 1.3685475661604797e-05,
      "loss": 2.0314,
      "step": 1731
    },
    {
      "epoch": 1.140223831468071,
      "grad_norm": 13.995686531066895,
      "learning_rate": 1.3679066069031228e-05,
      "loss": 1.9862,
      "step": 1732
    },
    {
      "epoch": 1.140882159315339,
      "grad_norm": 9.958287239074707,
      "learning_rate": 1.367265472790282e-05,
      "loss": 2.06,
      "step": 1733
    },
    {
      "epoch": 1.141540487162607,
      "grad_norm": 7.486065864562988,
      "learning_rate": 1.3666241641266691e-05,
      "loss": 2.1015,
      "step": 1734
    },
    {
      "epoch": 1.142198815009875,
      "grad_norm": 2.9375643730163574,
      "learning_rate": 1.365982681217081e-05,
      "loss": 1.6635,
      "step": 1735
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 1.3246861696243286,
      "learning_rate": 1.3653410243663953e-05,
      "loss": 1.6472,
      "step": 1736
    },
    {
      "epoch": 1.1435154707044108,
      "grad_norm": 1.9457231760025024,
      "learning_rate": 1.3646991938795733e-05,
      "loss": 1.6519,
      "step": 1737
    },
    {
      "epoch": 1.1441737985516787,
      "grad_norm": 10.688650131225586,
      "learning_rate": 1.364057190061659e-05,
      "loss": 1.9387,
      "step": 1738
    },
    {
      "epoch": 1.1448321263989467,
      "grad_norm": 1.8801347017288208,
      "learning_rate": 1.3634150132177784e-05,
      "loss": 1.6559,
      "step": 1739
    },
    {
      "epoch": 1.1454904542462145,
      "grad_norm": 7.077484607696533,
      "learning_rate": 1.3627726636531393e-05,
      "loss": 1.9723,
      "step": 1740
    },
    {
      "epoch": 1.1461487820934826,
      "grad_norm": 1.9558234214782715,
      "learning_rate": 1.3621301416730324e-05,
      "loss": 1.6649,
      "step": 1741
    },
    {
      "epoch": 1.1468071099407504,
      "grad_norm": 5.145341873168945,
      "learning_rate": 1.3614874475828301e-05,
      "loss": 1.8568,
      "step": 1742
    },
    {
      "epoch": 1.1474654377880185,
      "grad_norm": 17.276853561401367,
      "learning_rate": 1.3608445816879865e-05,
      "loss": 2.3477,
      "step": 1743
    },
    {
      "epoch": 1.1481237656352863,
      "grad_norm": 3.581662178039551,
      "learning_rate": 1.3602015442940371e-05,
      "loss": 1.693,
      "step": 1744
    },
    {
      "epoch": 1.1487820934825543,
      "grad_norm": 13.870769500732422,
      "learning_rate": 1.3595583357065993e-05,
      "loss": 2.1138,
      "step": 1745
    },
    {
      "epoch": 1.1494404213298222,
      "grad_norm": 2.410217046737671,
      "learning_rate": 1.358914956231372e-05,
      "loss": 1.6684,
      "step": 1746
    },
    {
      "epoch": 1.1500987491770902,
      "grad_norm": 15.668486595153809,
      "learning_rate": 1.3582714061741349e-05,
      "loss": 2.2594,
      "step": 1747
    },
    {
      "epoch": 1.150757077024358,
      "grad_norm": 19.151947021484375,
      "learning_rate": 1.3576276858407487e-05,
      "loss": 3.2967,
      "step": 1748
    },
    {
      "epoch": 1.151415404871626,
      "grad_norm": 30.56963348388672,
      "learning_rate": 1.356983795537156e-05,
      "loss": 2.7189,
      "step": 1749
    },
    {
      "epoch": 1.1520737327188941,
      "grad_norm": 11.923335075378418,
      "learning_rate": 1.3563397355693783e-05,
      "loss": 2.3245,
      "step": 1750
    },
    {
      "epoch": 1.152732060566162,
      "grad_norm": 2.5975968837738037,
      "learning_rate": 1.3556955062435193e-05,
      "loss": 1.6665,
      "step": 1751
    },
    {
      "epoch": 1.1533903884134298,
      "grad_norm": 2.184748411178589,
      "learning_rate": 1.3550511078657633e-05,
      "loss": 1.7246,
      "step": 1752
    },
    {
      "epoch": 1.1540487162606978,
      "grad_norm": 7.868496894836426,
      "learning_rate": 1.3544065407423741e-05,
      "loss": 1.9828,
      "step": 1753
    },
    {
      "epoch": 1.1547070441079659,
      "grad_norm": 18.45574378967285,
      "learning_rate": 1.3537618051796954e-05,
      "loss": 2.1854,
      "step": 1754
    },
    {
      "epoch": 1.1553653719552337,
      "grad_norm": 1.3348515033721924,
      "learning_rate": 1.3531169014841527e-05,
      "loss": 1.6376,
      "step": 1755
    },
    {
      "epoch": 1.1560236998025015,
      "grad_norm": 1.9475185871124268,
      "learning_rate": 1.3524718299622496e-05,
      "loss": 1.6494,
      "step": 1756
    },
    {
      "epoch": 1.1566820276497696,
      "grad_norm": 26.44307518005371,
      "learning_rate": 1.3518265909205704e-05,
      "loss": 3.0695,
      "step": 1757
    },
    {
      "epoch": 1.1573403554970376,
      "grad_norm": 15.950087547302246,
      "learning_rate": 1.3511811846657781e-05,
      "loss": 1.9739,
      "step": 1758
    },
    {
      "epoch": 1.1579986833443054,
      "grad_norm": 13.638603210449219,
      "learning_rate": 1.3505356115046167e-05,
      "loss": 2.1053,
      "step": 1759
    },
    {
      "epoch": 1.1586570111915735,
      "grad_norm": 7.002621173858643,
      "learning_rate": 1.349889871743908e-05,
      "loss": 1.9781,
      "step": 1760
    },
    {
      "epoch": 1.1593153390388413,
      "grad_norm": 2.5156919956207275,
      "learning_rate": 1.3492439656905538e-05,
      "loss": 1.6757,
      "step": 1761
    },
    {
      "epoch": 1.1599736668861094,
      "grad_norm": 15.853262901306152,
      "learning_rate": 1.3485978936515353e-05,
      "loss": 2.224,
      "step": 1762
    },
    {
      "epoch": 1.1606319947333772,
      "grad_norm": 18.286989212036133,
      "learning_rate": 1.3479516559339112e-05,
      "loss": 2.1427,
      "step": 1763
    },
    {
      "epoch": 1.1612903225806452,
      "grad_norm": 20.857803344726562,
      "learning_rate": 1.3473052528448203e-05,
      "loss": 2.2819,
      "step": 1764
    },
    {
      "epoch": 1.161948650427913,
      "grad_norm": 17.949132919311523,
      "learning_rate": 1.3466586846914795e-05,
      "loss": 2.3329,
      "step": 1765
    },
    {
      "epoch": 1.1626069782751811,
      "grad_norm": 7.822042942047119,
      "learning_rate": 1.3460119517811843e-05,
      "loss": 1.8971,
      "step": 1766
    },
    {
      "epoch": 1.163265306122449,
      "grad_norm": 3.305546998977661,
      "learning_rate": 1.3453650544213078e-05,
      "loss": 1.6773,
      "step": 1767
    },
    {
      "epoch": 1.163923633969717,
      "grad_norm": 6.391723155975342,
      "learning_rate": 1.3447179929193022e-05,
      "loss": 1.7216,
      "step": 1768
    },
    {
      "epoch": 1.1645819618169848,
      "grad_norm": 23.0091609954834,
      "learning_rate": 1.3440707675826971e-05,
      "loss": 2.6466,
      "step": 1769
    },
    {
      "epoch": 1.1652402896642529,
      "grad_norm": 3.9843459129333496,
      "learning_rate": 1.3434233787191006e-05,
      "loss": 1.684,
      "step": 1770
    },
    {
      "epoch": 1.1658986175115207,
      "grad_norm": 1.7458959817886353,
      "learning_rate": 1.3427758266361974e-05,
      "loss": 1.6473,
      "step": 1771
    },
    {
      "epoch": 1.1665569453587887,
      "grad_norm": 20.123001098632812,
      "learning_rate": 1.3421281116417512e-05,
      "loss": 2.4134,
      "step": 1772
    },
    {
      "epoch": 1.1672152732060566,
      "grad_norm": 18.143001556396484,
      "learning_rate": 1.3414802340436022e-05,
      "loss": 2.6094,
      "step": 1773
    },
    {
      "epoch": 1.1678736010533246,
      "grad_norm": 1.4390943050384521,
      "learning_rate": 1.3408321941496678e-05,
      "loss": 1.6355,
      "step": 1774
    },
    {
      "epoch": 1.1685319289005924,
      "grad_norm": 3.386909008026123,
      "learning_rate": 1.3401839922679431e-05,
      "loss": 1.6581,
      "step": 1775
    },
    {
      "epoch": 1.1691902567478605,
      "grad_norm": 0.9633329510688782,
      "learning_rate": 1.3395356287065e-05,
      "loss": 1.6289,
      "step": 1776
    },
    {
      "epoch": 1.1698485845951283,
      "grad_norm": 21.972248077392578,
      "learning_rate": 1.3388871037734866e-05,
      "loss": 2.7735,
      "step": 1777
    },
    {
      "epoch": 1.1705069124423964,
      "grad_norm": 22.799760818481445,
      "learning_rate": 1.3382384177771286e-05,
      "loss": 2.4298,
      "step": 1778
    },
    {
      "epoch": 1.1711652402896642,
      "grad_norm": 6.409982681274414,
      "learning_rate": 1.3375895710257282e-05,
      "loss": 1.891,
      "step": 1779
    },
    {
      "epoch": 1.1718235681369322,
      "grad_norm": 7.807604789733887,
      "learning_rate": 1.336940563827663e-05,
      "loss": 1.9505,
      "step": 1780
    },
    {
      "epoch": 1.1724818959842,
      "grad_norm": 8.952757835388184,
      "learning_rate": 1.3362913964913883e-05,
      "loss": 2.1681,
      "step": 1781
    },
    {
      "epoch": 1.173140223831468,
      "grad_norm": 10.68782901763916,
      "learning_rate": 1.3356420693254344e-05,
      "loss": 2.1496,
      "step": 1782
    },
    {
      "epoch": 1.173798551678736,
      "grad_norm": 12.137029647827148,
      "learning_rate": 1.334992582638408e-05,
      "loss": 1.9197,
      "step": 1783
    },
    {
      "epoch": 1.174456879526004,
      "grad_norm": 13.649086952209473,
      "learning_rate": 1.3343429367389913e-05,
      "loss": 2.311,
      "step": 1784
    },
    {
      "epoch": 1.1751152073732718,
      "grad_norm": 5.938873291015625,
      "learning_rate": 1.3336931319359428e-05,
      "loss": 1.9036,
      "step": 1785
    },
    {
      "epoch": 1.1757735352205398,
      "grad_norm": 10.097585678100586,
      "learning_rate": 1.3330431685380958e-05,
      "loss": 2.2497,
      "step": 1786
    },
    {
      "epoch": 1.176431863067808,
      "grad_norm": 5.679737091064453,
      "learning_rate": 1.3323930468543595e-05,
      "loss": 1.7005,
      "step": 1787
    },
    {
      "epoch": 1.1770901909150757,
      "grad_norm": 14.739688873291016,
      "learning_rate": 1.3317427671937177e-05,
      "loss": 2.2954,
      "step": 1788
    },
    {
      "epoch": 1.1777485187623435,
      "grad_norm": 9.964609146118164,
      "learning_rate": 1.3310923298652307e-05,
      "loss": 1.7954,
      "step": 1789
    },
    {
      "epoch": 1.1784068466096116,
      "grad_norm": 18.568326950073242,
      "learning_rate": 1.3304417351780319e-05,
      "loss": 2.5564,
      "step": 1790
    },
    {
      "epoch": 1.1790651744568796,
      "grad_norm": 19.461368560791016,
      "learning_rate": 1.3297909834413303e-05,
      "loss": 2.7893,
      "step": 1791
    },
    {
      "epoch": 1.1797235023041475,
      "grad_norm": 24.68321418762207,
      "learning_rate": 1.32914007496441e-05,
      "loss": 2.4596,
      "step": 1792
    },
    {
      "epoch": 1.1803818301514153,
      "grad_norm": 7.401484489440918,
      "learning_rate": 1.3284890100566294e-05,
      "loss": 1.9203,
      "step": 1793
    },
    {
      "epoch": 1.1810401579986833,
      "grad_norm": 8.103160858154297,
      "learning_rate": 1.3278377890274203e-05,
      "loss": 2.1148,
      "step": 1794
    },
    {
      "epoch": 1.1816984858459514,
      "grad_norm": 8.520626068115234,
      "learning_rate": 1.3271864121862898e-05,
      "loss": 1.7547,
      "step": 1795
    },
    {
      "epoch": 1.1823568136932192,
      "grad_norm": 11.612844467163086,
      "learning_rate": 1.326534879842819e-05,
      "loss": 2.159,
      "step": 1796
    },
    {
      "epoch": 1.1830151415404873,
      "grad_norm": 1.6700061559677124,
      "learning_rate": 1.325883192306662e-05,
      "loss": 1.6352,
      "step": 1797
    },
    {
      "epoch": 1.183673469387755,
      "grad_norm": 28.672395706176758,
      "learning_rate": 1.3252313498875473e-05,
      "loss": 3.5155,
      "step": 1798
    },
    {
      "epoch": 1.1843317972350231,
      "grad_norm": 3.3121337890625,
      "learning_rate": 1.3245793528952775e-05,
      "loss": 1.6847,
      "step": 1799
    },
    {
      "epoch": 1.184990125082291,
      "grad_norm": 21.613075256347656,
      "learning_rate": 1.3239272016397273e-05,
      "loss": 2.6308,
      "step": 1800
    },
    {
      "epoch": 1.185648452929559,
      "grad_norm": 15.712318420410156,
      "learning_rate": 1.3232748964308461e-05,
      "loss": 2.0853,
      "step": 1801
    },
    {
      "epoch": 1.1863067807768268,
      "grad_norm": 5.769379138946533,
      "learning_rate": 1.3226224375786561e-05,
      "loss": 1.6655,
      "step": 1802
    },
    {
      "epoch": 1.1869651086240949,
      "grad_norm": 9.215944290161133,
      "learning_rate": 1.3219698253932518e-05,
      "loss": 2.0221,
      "step": 1803
    },
    {
      "epoch": 1.1876234364713627,
      "grad_norm": 2.3888864517211914,
      "learning_rate": 1.3213170601848016e-05,
      "loss": 1.6541,
      "step": 1804
    },
    {
      "epoch": 1.1882817643186308,
      "grad_norm": 14.038139343261719,
      "learning_rate": 1.3206641422635453e-05,
      "loss": 2.228,
      "step": 1805
    },
    {
      "epoch": 1.1889400921658986,
      "grad_norm": 1.6859817504882812,
      "learning_rate": 1.3200110719397969e-05,
      "loss": 1.6464,
      "step": 1806
    },
    {
      "epoch": 1.1895984200131666,
      "grad_norm": 1.3659733533859253,
      "learning_rate": 1.319357849523942e-05,
      "loss": 1.6382,
      "step": 1807
    },
    {
      "epoch": 1.1902567478604344,
      "grad_norm": 11.601051330566406,
      "learning_rate": 1.3187044753264377e-05,
      "loss": 1.9415,
      "step": 1808
    },
    {
      "epoch": 1.1909150757077025,
      "grad_norm": 1.9999827146530151,
      "learning_rate": 1.3180509496578156e-05,
      "loss": 1.6527,
      "step": 1809
    },
    {
      "epoch": 1.1915734035549703,
      "grad_norm": 12.002318382263184,
      "learning_rate": 1.3173972728286763e-05,
      "loss": 1.9171,
      "step": 1810
    },
    {
      "epoch": 1.1922317314022384,
      "grad_norm": 14.174642562866211,
      "learning_rate": 1.3167434451496942e-05,
      "loss": 1.9243,
      "step": 1811
    },
    {
      "epoch": 1.1928900592495062,
      "grad_norm": 1.4430205821990967,
      "learning_rate": 1.3160894669316148e-05,
      "loss": 1.6371,
      "step": 1812
    },
    {
      "epoch": 1.1935483870967742,
      "grad_norm": 6.698953151702881,
      "learning_rate": 1.3154353384852559e-05,
      "loss": 1.863,
      "step": 1813
    },
    {
      "epoch": 1.194206714944042,
      "grad_norm": 16.582475662231445,
      "learning_rate": 1.3147810601215047e-05,
      "loss": 2.2201,
      "step": 1814
    },
    {
      "epoch": 1.1948650427913101,
      "grad_norm": 29.57261085510254,
      "learning_rate": 1.3141266321513221e-05,
      "loss": 4.0828,
      "step": 1815
    },
    {
      "epoch": 1.195523370638578,
      "grad_norm": 7.437309265136719,
      "learning_rate": 1.313472054885739e-05,
      "loss": 1.7776,
      "step": 1816
    },
    {
      "epoch": 1.196181698485846,
      "grad_norm": 7.9081573486328125,
      "learning_rate": 1.3128173286358565e-05,
      "loss": 1.9844,
      "step": 1817
    },
    {
      "epoch": 1.1968400263331138,
      "grad_norm": 11.845954895019531,
      "learning_rate": 1.3121624537128477e-05,
      "loss": 2.1242,
      "step": 1818
    },
    {
      "epoch": 1.1974983541803819,
      "grad_norm": 2.1735620498657227,
      "learning_rate": 1.3115074304279562e-05,
      "loss": 1.6364,
      "step": 1819
    },
    {
      "epoch": 1.1981566820276497,
      "grad_norm": 2.6112685203552246,
      "learning_rate": 1.3108522590924953e-05,
      "loss": 1.6657,
      "step": 1820
    },
    {
      "epoch": 1.1988150098749177,
      "grad_norm": 18.720314025878906,
      "learning_rate": 1.3101969400178491e-05,
      "loss": 2.277,
      "step": 1821
    },
    {
      "epoch": 1.1994733377221856,
      "grad_norm": 10.211339950561523,
      "learning_rate": 1.3095414735154726e-05,
      "loss": 2.0455,
      "step": 1822
    },
    {
      "epoch": 1.2001316655694536,
      "grad_norm": 9.499634742736816,
      "learning_rate": 1.30888585989689e-05,
      "loss": 1.9744,
      "step": 1823
    },
    {
      "epoch": 1.2007899934167214,
      "grad_norm": 17.81464958190918,
      "learning_rate": 1.3082300994736959e-05,
      "loss": 2.1555,
      "step": 1824
    },
    {
      "epoch": 1.2014483212639895,
      "grad_norm": 1.8861523866653442,
      "learning_rate": 1.3075741925575536e-05,
      "loss": 1.6263,
      "step": 1825
    },
    {
      "epoch": 1.2021066491112573,
      "grad_norm": 2.2877633571624756,
      "learning_rate": 1.3069181394601979e-05,
      "loss": 1.652,
      "step": 1826
    },
    {
      "epoch": 1.2027649769585254,
      "grad_norm": 2.4709973335266113,
      "learning_rate": 1.3062619404934319e-05,
      "loss": 1.6818,
      "step": 1827
    },
    {
      "epoch": 1.2034233048057934,
      "grad_norm": 14.885087966918945,
      "learning_rate": 1.3056055959691276e-05,
      "loss": 2.3984,
      "step": 1828
    },
    {
      "epoch": 1.2040816326530612,
      "grad_norm": 7.340738773345947,
      "learning_rate": 1.3049491061992274e-05,
      "loss": 2.0381,
      "step": 1829
    },
    {
      "epoch": 1.204739960500329,
      "grad_norm": 15.780899047851562,
      "learning_rate": 1.304292471495742e-05,
      "loss": 2.1671,
      "step": 1830
    },
    {
      "epoch": 1.205398288347597,
      "grad_norm": 14.623188972473145,
      "learning_rate": 1.3036356921707505e-05,
      "loss": 1.9198,
      "step": 1831
    },
    {
      "epoch": 1.2060566161948651,
      "grad_norm": 9.46024227142334,
      "learning_rate": 1.3029787685364022e-05,
      "loss": 2.1211,
      "step": 1832
    },
    {
      "epoch": 1.206714944042133,
      "grad_norm": 26.396728515625,
      "learning_rate": 1.3023217009049133e-05,
      "loss": 3.1029,
      "step": 1833
    },
    {
      "epoch": 1.2073732718894008,
      "grad_norm": 6.840458869934082,
      "learning_rate": 1.30166448958857e-05,
      "loss": 1.8826,
      "step": 1834
    },
    {
      "epoch": 1.2080315997366688,
      "grad_norm": 3.586437940597534,
      "learning_rate": 1.3010071348997256e-05,
      "loss": 1.6542,
      "step": 1835
    },
    {
      "epoch": 1.208689927583937,
      "grad_norm": 12.781339645385742,
      "learning_rate": 1.300349637150802e-05,
      "loss": 1.9938,
      "step": 1836
    },
    {
      "epoch": 1.2093482554312047,
      "grad_norm": 2.5596883296966553,
      "learning_rate": 1.2996919966542889e-05,
      "loss": 1.6684,
      "step": 1837
    },
    {
      "epoch": 1.2100065832784728,
      "grad_norm": 15.483268737792969,
      "learning_rate": 1.2990342137227443e-05,
      "loss": 1.8707,
      "step": 1838
    },
    {
      "epoch": 1.2106649111257406,
      "grad_norm": 18.687440872192383,
      "learning_rate": 1.2983762886687938e-05,
      "loss": 2.4225,
      "step": 1839
    },
    {
      "epoch": 1.2113232389730086,
      "grad_norm": 7.773602485656738,
      "learning_rate": 1.2977182218051298e-05,
      "loss": 1.8872,
      "step": 1840
    },
    {
      "epoch": 1.2119815668202765,
      "grad_norm": 11.220565795898438,
      "learning_rate": 1.2970600134445133e-05,
      "loss": 1.9176,
      "step": 1841
    },
    {
      "epoch": 1.2126398946675445,
      "grad_norm": 1.2625871896743774,
      "learning_rate": 1.296401663899771e-05,
      "loss": 1.6373,
      "step": 1842
    },
    {
      "epoch": 1.2132982225148123,
      "grad_norm": 9.851313591003418,
      "learning_rate": 1.2957431734837986e-05,
      "loss": 2.0325,
      "step": 1843
    },
    {
      "epoch": 1.2139565503620804,
      "grad_norm": 19.70118522644043,
      "learning_rate": 1.2950845425095573e-05,
      "loss": 2.0713,
      "step": 1844
    },
    {
      "epoch": 1.2146148782093482,
      "grad_norm": 6.530334949493408,
      "learning_rate": 1.2944257712900753e-05,
      "loss": 1.9282,
      "step": 1845
    },
    {
      "epoch": 1.2152732060566163,
      "grad_norm": 11.692984580993652,
      "learning_rate": 1.2937668601384479e-05,
      "loss": 2.0488,
      "step": 1846
    },
    {
      "epoch": 1.215931533903884,
      "grad_norm": 9.785593032836914,
      "learning_rate": 1.2931078093678373e-05,
      "loss": 2.0228,
      "step": 1847
    },
    {
      "epoch": 1.2165898617511521,
      "grad_norm": 8.749391555786133,
      "learning_rate": 1.2924486192914705e-05,
      "loss": 1.8018,
      "step": 1848
    },
    {
      "epoch": 1.21724818959842,
      "grad_norm": 5.050635814666748,
      "learning_rate": 1.291789290222642e-05,
      "loss": 1.8394,
      "step": 1849
    },
    {
      "epoch": 1.217906517445688,
      "grad_norm": 18.62893295288086,
      "learning_rate": 1.2911298224747127e-05,
      "loss": 2.5524,
      "step": 1850
    },
    {
      "epoch": 1.2185648452929558,
      "grad_norm": 7.520800590515137,
      "learning_rate": 1.290470216361108e-05,
      "loss": 1.9662,
      "step": 1851
    },
    {
      "epoch": 1.2192231731402239,
      "grad_norm": 12.268111228942871,
      "learning_rate": 1.28981047219532e-05,
      "loss": 1.8378,
      "step": 1852
    },
    {
      "epoch": 1.2198815009874917,
      "grad_norm": 8.962813377380371,
      "learning_rate": 1.2891505902909065e-05,
      "loss": 1.9087,
      "step": 1853
    },
    {
      "epoch": 1.2205398288347598,
      "grad_norm": 5.3523712158203125,
      "learning_rate": 1.28849057096149e-05,
      "loss": 1.6954,
      "step": 1854
    },
    {
      "epoch": 1.2211981566820276,
      "grad_norm": 34.03230285644531,
      "learning_rate": 1.2878304145207593e-05,
      "loss": 2.0783,
      "step": 1855
    },
    {
      "epoch": 1.2218564845292956,
      "grad_norm": 11.355072021484375,
      "learning_rate": 1.2871701212824673e-05,
      "loss": 1.9871,
      "step": 1856
    },
    {
      "epoch": 1.2225148123765635,
      "grad_norm": 23.185894012451172,
      "learning_rate": 1.286509691560433e-05,
      "loss": 2.7236,
      "step": 1857
    },
    {
      "epoch": 1.2231731402238315,
      "grad_norm": 18.278350830078125,
      "learning_rate": 1.2858491256685399e-05,
      "loss": 2.1526,
      "step": 1858
    },
    {
      "epoch": 1.2238314680710993,
      "grad_norm": 10.398270606994629,
      "learning_rate": 1.2851884239207355e-05,
      "loss": 2.1457,
      "step": 1859
    },
    {
      "epoch": 1.2244897959183674,
      "grad_norm": 1.569334626197815,
      "learning_rate": 1.2845275866310325e-05,
      "loss": 1.6217,
      "step": 1860
    },
    {
      "epoch": 1.2251481237656352,
      "grad_norm": 2.920438528060913,
      "learning_rate": 1.2838666141135086e-05,
      "loss": 1.6406,
      "step": 1861
    },
    {
      "epoch": 1.2258064516129032,
      "grad_norm": 2.8338406085968018,
      "learning_rate": 1.283205506682304e-05,
      "loss": 1.6404,
      "step": 1862
    },
    {
      "epoch": 1.226464779460171,
      "grad_norm": 24.320419311523438,
      "learning_rate": 1.2825442646516253e-05,
      "loss": 2.4612,
      "step": 1863
    },
    {
      "epoch": 1.2271231073074391,
      "grad_norm": 9.528409004211426,
      "learning_rate": 1.2818828883357415e-05,
      "loss": 1.9815,
      "step": 1864
    },
    {
      "epoch": 1.2277814351547072,
      "grad_norm": 14.05683708190918,
      "learning_rate": 1.2812213780489856e-05,
      "loss": 2.0503,
      "step": 1865
    },
    {
      "epoch": 1.228439763001975,
      "grad_norm": 22.891469955444336,
      "learning_rate": 1.2805597341057547e-05,
      "loss": 2.6039,
      "step": 1866
    },
    {
      "epoch": 1.2290980908492428,
      "grad_norm": 23.641738891601562,
      "learning_rate": 1.2798979568205091e-05,
      "loss": 2.1371,
      "step": 1867
    },
    {
      "epoch": 1.2297564186965109,
      "grad_norm": 2.796910285949707,
      "learning_rate": 1.279236046507773e-05,
      "loss": 1.6481,
      "step": 1868
    },
    {
      "epoch": 1.230414746543779,
      "grad_norm": 2.104156970977783,
      "learning_rate": 1.278574003482133e-05,
      "loss": 1.7161,
      "step": 1869
    },
    {
      "epoch": 1.2310730743910467,
      "grad_norm": 1.5256881713867188,
      "learning_rate": 1.2779118280582395e-05,
      "loss": 1.63,
      "step": 1870
    },
    {
      "epoch": 1.2317314022383146,
      "grad_norm": 19.175350189208984,
      "learning_rate": 1.2772495205508053e-05,
      "loss": 2.376,
      "step": 1871
    },
    {
      "epoch": 1.2323897300855826,
      "grad_norm": 15.989566802978516,
      "learning_rate": 1.2765870812746062e-05,
      "loss": 2.0967,
      "step": 1872
    },
    {
      "epoch": 1.2330480579328507,
      "grad_norm": 11.018048286437988,
      "learning_rate": 1.275924510544481e-05,
      "loss": 2.1604,
      "step": 1873
    },
    {
      "epoch": 1.2337063857801185,
      "grad_norm": 1.6795477867126465,
      "learning_rate": 1.2752618086753304e-05,
      "loss": 1.6218,
      "step": 1874
    },
    {
      "epoch": 1.2343647136273865,
      "grad_norm": 8.273041725158691,
      "learning_rate": 1.2745989759821176e-05,
      "loss": 1.9538,
      "step": 1875
    },
    {
      "epoch": 1.2350230414746544,
      "grad_norm": 21.646024703979492,
      "learning_rate": 1.2739360127798677e-05,
      "loss": 2.5905,
      "step": 1876
    },
    {
      "epoch": 1.2356813693219224,
      "grad_norm": 14.433485984802246,
      "learning_rate": 1.2732729193836685e-05,
      "loss": 2.0809,
      "step": 1877
    },
    {
      "epoch": 1.2363396971691902,
      "grad_norm": 2.93454647064209,
      "learning_rate": 1.2726096961086695e-05,
      "loss": 1.6406,
      "step": 1878
    },
    {
      "epoch": 1.2369980250164583,
      "grad_norm": 15.705464363098145,
      "learning_rate": 1.2719463432700816e-05,
      "loss": 2.3237,
      "step": 1879
    },
    {
      "epoch": 1.237656352863726,
      "grad_norm": 2.1303234100341797,
      "learning_rate": 1.271282861183177e-05,
      "loss": 1.6645,
      "step": 1880
    },
    {
      "epoch": 1.2383146807109942,
      "grad_norm": 2.0166773796081543,
      "learning_rate": 1.2706192501632905e-05,
      "loss": 1.6147,
      "step": 1881
    },
    {
      "epoch": 1.238973008558262,
      "grad_norm": 15.17516803741455,
      "learning_rate": 1.2699555105258168e-05,
      "loss": 2.2779,
      "step": 1882
    },
    {
      "epoch": 1.23963133640553,
      "grad_norm": 2.0920679569244385,
      "learning_rate": 1.2692916425862125e-05,
      "loss": 1.6548,
      "step": 1883
    },
    {
      "epoch": 1.2402896642527979,
      "grad_norm": 1.7852221727371216,
      "learning_rate": 1.2686276466599954e-05,
      "loss": 1.614,
      "step": 1884
    },
    {
      "epoch": 1.240947992100066,
      "grad_norm": 17.840158462524414,
      "learning_rate": 1.2679635230627434e-05,
      "loss": 2.566,
      "step": 1885
    },
    {
      "epoch": 1.2416063199473337,
      "grad_norm": 13.464173316955566,
      "learning_rate": 1.2672992721100954e-05,
      "loss": 1.992,
      "step": 1886
    },
    {
      "epoch": 1.2422646477946018,
      "grad_norm": 14.123619079589844,
      "learning_rate": 1.2666348941177512e-05,
      "loss": 2.526,
      "step": 1887
    },
    {
      "epoch": 1.2429229756418696,
      "grad_norm": 3.4508798122406006,
      "learning_rate": 1.2659703894014705e-05,
      "loss": 1.6458,
      "step": 1888
    },
    {
      "epoch": 1.2435813034891376,
      "grad_norm": 1.8745615482330322,
      "learning_rate": 1.2653057582770731e-05,
      "loss": 1.6431,
      "step": 1889
    },
    {
      "epoch": 1.2442396313364055,
      "grad_norm": 6.383483409881592,
      "learning_rate": 1.2646410010604397e-05,
      "loss": 1.8534,
      "step": 1890
    },
    {
      "epoch": 1.2448979591836735,
      "grad_norm": 4.570650100708008,
      "learning_rate": 1.2639761180675098e-05,
      "loss": 1.6853,
      "step": 1891
    },
    {
      "epoch": 1.2455562870309413,
      "grad_norm": 5.399219036102295,
      "learning_rate": 1.2633111096142838e-05,
      "loss": 1.9502,
      "step": 1892
    },
    {
      "epoch": 1.2462146148782094,
      "grad_norm": 13.024372100830078,
      "learning_rate": 1.262645976016821e-05,
      "loss": 2.0571,
      "step": 1893
    },
    {
      "epoch": 1.2468729427254772,
      "grad_norm": 15.876761436462402,
      "learning_rate": 1.2619807175912402e-05,
      "loss": 2.1781,
      "step": 1894
    },
    {
      "epoch": 1.2475312705727453,
      "grad_norm": 16.319955825805664,
      "learning_rate": 1.2613153346537199e-05,
      "loss": 2.3688,
      "step": 1895
    },
    {
      "epoch": 1.248189598420013,
      "grad_norm": 2.8142261505126953,
      "learning_rate": 1.2606498275204972e-05,
      "loss": 1.6344,
      "step": 1896
    },
    {
      "epoch": 1.2488479262672811,
      "grad_norm": 3.3394856452941895,
      "learning_rate": 1.2599841965078689e-05,
      "loss": 1.6621,
      "step": 1897
    },
    {
      "epoch": 1.249506254114549,
      "grad_norm": 18.461156845092773,
      "learning_rate": 1.2593184419321908e-05,
      "loss": 2.1487,
      "step": 1898
    },
    {
      "epoch": 1.250164581961817,
      "grad_norm": 9.521698951721191,
      "learning_rate": 1.2586525641098764e-05,
      "loss": 2.0291,
      "step": 1899
    },
    {
      "epoch": 1.2508229098090848,
      "grad_norm": 1.7324780225753784,
      "learning_rate": 1.2579865633573982e-05,
      "loss": 1.6168,
      "step": 1900
    },
    {
      "epoch": 1.2514812376563529,
      "grad_norm": 16.71480941772461,
      "learning_rate": 1.257320439991288e-05,
      "loss": 2.0936,
      "step": 1901
    },
    {
      "epoch": 1.252139565503621,
      "grad_norm": 9.227566719055176,
      "learning_rate": 1.2566541943281348e-05,
      "loss": 1.9972,
      "step": 1902
    },
    {
      "epoch": 1.2527978933508888,
      "grad_norm": 4.266833305358887,
      "learning_rate": 1.255987826684586e-05,
      "loss": 1.6513,
      "step": 1903
    },
    {
      "epoch": 1.2534562211981566,
      "grad_norm": 4.661250591278076,
      "learning_rate": 1.2553213373773472e-05,
      "loss": 1.6853,
      "step": 1904
    },
    {
      "epoch": 1.2541145490454246,
      "grad_norm": 10.543699264526367,
      "learning_rate": 1.2546547267231817e-05,
      "loss": 1.9735,
      "step": 1905
    },
    {
      "epoch": 1.2547728768926927,
      "grad_norm": 2.5807104110717773,
      "learning_rate": 1.2539879950389104e-05,
      "loss": 1.6202,
      "step": 1906
    },
    {
      "epoch": 1.2554312047399605,
      "grad_norm": 13.47191047668457,
      "learning_rate": 1.2533211426414123e-05,
      "loss": 2.0917,
      "step": 1907
    },
    {
      "epoch": 1.2560895325872283,
      "grad_norm": 18.37852668762207,
      "learning_rate": 1.2526541698476223e-05,
      "loss": 2.2057,
      "step": 1908
    },
    {
      "epoch": 1.2567478604344964,
      "grad_norm": 14.797247886657715,
      "learning_rate": 1.2519870769745341e-05,
      "loss": 2.1789,
      "step": 1909
    },
    {
      "epoch": 1.2574061882817644,
      "grad_norm": 1.8850077390670776,
      "learning_rate": 1.251319864339198e-05,
      "loss": 1.6244,
      "step": 1910
    },
    {
      "epoch": 1.2580645161290323,
      "grad_norm": 1.413942575454712,
      "learning_rate": 1.2506525322587207e-05,
      "loss": 1.627,
      "step": 1911
    },
    {
      "epoch": 1.2587228439763,
      "grad_norm": 13.070850372314453,
      "learning_rate": 1.2499850810502663e-05,
      "loss": 2.1638,
      "step": 1912
    },
    {
      "epoch": 1.2593811718235681,
      "grad_norm": 10.56748104095459,
      "learning_rate": 1.249317511031055e-05,
      "loss": 2.0092,
      "step": 1913
    },
    {
      "epoch": 1.2600394996708362,
      "grad_norm": 8.258384704589844,
      "learning_rate": 1.2486498225183639e-05,
      "loss": 2.0301,
      "step": 1914
    },
    {
      "epoch": 1.260697827518104,
      "grad_norm": 12.012789726257324,
      "learning_rate": 1.2479820158295264e-05,
      "loss": 2.1616,
      "step": 1915
    },
    {
      "epoch": 1.2613561553653718,
      "grad_norm": 1.8682501316070557,
      "learning_rate": 1.2473140912819317e-05,
      "loss": 1.6241,
      "step": 1916
    },
    {
      "epoch": 1.2620144832126399,
      "grad_norm": 19.961179733276367,
      "learning_rate": 1.246646049193025e-05,
      "loss": 2.3959,
      "step": 1917
    },
    {
      "epoch": 1.262672811059908,
      "grad_norm": 20.131860733032227,
      "learning_rate": 1.2459778898803083e-05,
      "loss": 2.4776,
      "step": 1918
    },
    {
      "epoch": 1.2633311389071757,
      "grad_norm": 16.290611267089844,
      "learning_rate": 1.2453096136613379e-05,
      "loss": 2.0394,
      "step": 1919
    },
    {
      "epoch": 1.2639894667544438,
      "grad_norm": 5.980796813964844,
      "learning_rate": 1.2446412208537266e-05,
      "loss": 1.915,
      "step": 1920
    },
    {
      "epoch": 1.2646477946017116,
      "grad_norm": 8.972203254699707,
      "learning_rate": 1.2439727117751426e-05,
      "loss": 1.9629,
      "step": 1921
    },
    {
      "epoch": 1.2653061224489797,
      "grad_norm": 5.109227180480957,
      "learning_rate": 1.2433040867433087e-05,
      "loss": 1.9126,
      "step": 1922
    },
    {
      "epoch": 1.2659644502962475,
      "grad_norm": 8.988327026367188,
      "learning_rate": 1.2426353460760036e-05,
      "loss": 1.9592,
      "step": 1923
    },
    {
      "epoch": 1.2666227781435155,
      "grad_norm": 16.182973861694336,
      "learning_rate": 1.2419664900910608e-05,
      "loss": 2.1441,
      "step": 1924
    },
    {
      "epoch": 1.2672811059907834,
      "grad_norm": 7.1990180015563965,
      "learning_rate": 1.2412975191063678e-05,
      "loss": 1.9351,
      "step": 1925
    },
    {
      "epoch": 1.2679394338380514,
      "grad_norm": 10.753005027770996,
      "learning_rate": 1.2406284334398677e-05,
      "loss": 2.1027,
      "step": 1926
    },
    {
      "epoch": 1.2685977616853192,
      "grad_norm": 24.499217987060547,
      "learning_rate": 1.239959233409558e-05,
      "loss": 2.361,
      "step": 1927
    },
    {
      "epoch": 1.2692560895325873,
      "grad_norm": 22.059316635131836,
      "learning_rate": 1.23928991933349e-05,
      "loss": 2.4422,
      "step": 1928
    },
    {
      "epoch": 1.269914417379855,
      "grad_norm": 9.960470199584961,
      "learning_rate": 1.2386204915297695e-05,
      "loss": 2.0531,
      "step": 1929
    },
    {
      "epoch": 1.2705727452271232,
      "grad_norm": 10.953670501708984,
      "learning_rate": 1.2379509503165565e-05,
      "loss": 1.9703,
      "step": 1930
    },
    {
      "epoch": 1.271231073074391,
      "grad_norm": 15.800895690917969,
      "learning_rate": 1.2372812960120649e-05,
      "loss": 3.0186,
      "step": 1931
    },
    {
      "epoch": 1.271889400921659,
      "grad_norm": 18.41704750061035,
      "learning_rate": 1.236611528934562e-05,
      "loss": 2.3459,
      "step": 1932
    },
    {
      "epoch": 1.2725477287689269,
      "grad_norm": 9.611031532287598,
      "learning_rate": 1.2359416494023692e-05,
      "loss": 1.9244,
      "step": 1933
    },
    {
      "epoch": 1.273206056616195,
      "grad_norm": 7.006026268005371,
      "learning_rate": 1.2352716577338609e-05,
      "loss": 1.8073,
      "step": 1934
    },
    {
      "epoch": 1.2738643844634627,
      "grad_norm": 20.727235794067383,
      "learning_rate": 1.234601554247465e-05,
      "loss": 2.4339,
      "step": 1935
    },
    {
      "epoch": 1.2745227123107308,
      "grad_norm": 7.5892229080200195,
      "learning_rate": 1.2339313392616623e-05,
      "loss": 1.8351,
      "step": 1936
    },
    {
      "epoch": 1.2751810401579986,
      "grad_norm": 3.0275280475616455,
      "learning_rate": 1.233261013094987e-05,
      "loss": 1.6774,
      "step": 1937
    },
    {
      "epoch": 1.2758393680052666,
      "grad_norm": 68.87812042236328,
      "learning_rate": 1.2325905760660264e-05,
      "loss": 2.3671,
      "step": 1938
    },
    {
      "epoch": 1.2764976958525347,
      "grad_norm": 17.835254669189453,
      "learning_rate": 1.2319200284934191e-05,
      "loss": 1.8797,
      "step": 1939
    },
    {
      "epoch": 1.2771560236998025,
      "grad_norm": 11.657641410827637,
      "learning_rate": 1.2312493706958579e-05,
      "loss": 2.1207,
      "step": 1940
    },
    {
      "epoch": 1.2778143515470703,
      "grad_norm": 2.0041182041168213,
      "learning_rate": 1.2305786029920871e-05,
      "loss": 1.6295,
      "step": 1941
    },
    {
      "epoch": 1.2784726793943384,
      "grad_norm": 12.85594367980957,
      "learning_rate": 1.2299077257009033e-05,
      "loss": 2.1382,
      "step": 1942
    },
    {
      "epoch": 1.2791310072416064,
      "grad_norm": 16.174222946166992,
      "learning_rate": 1.2292367391411552e-05,
      "loss": 2.2692,
      "step": 1943
    },
    {
      "epoch": 1.2797893350888743,
      "grad_norm": 18.304719924926758,
      "learning_rate": 1.2285656436317442e-05,
      "loss": 2.2929,
      "step": 1944
    },
    {
      "epoch": 1.280447662936142,
      "grad_norm": 11.60567855834961,
      "learning_rate": 1.2278944394916217e-05,
      "loss": 2.0435,
      "step": 1945
    },
    {
      "epoch": 1.2811059907834101,
      "grad_norm": 5.638899326324463,
      "learning_rate": 1.2272231270397925e-05,
      "loss": 1.9818,
      "step": 1946
    },
    {
      "epoch": 1.2817643186306782,
      "grad_norm": 4.598821640014648,
      "learning_rate": 1.2265517065953119e-05,
      "loss": 1.6497,
      "step": 1947
    },
    {
      "epoch": 1.282422646477946,
      "grad_norm": 2.8934004306793213,
      "learning_rate": 1.2258801784772871e-05,
      "loss": 1.6471,
      "step": 1948
    },
    {
      "epoch": 1.2830809743252138,
      "grad_norm": 12.21224594116211,
      "learning_rate": 1.2252085430048761e-05,
      "loss": 2.0714,
      "step": 1949
    },
    {
      "epoch": 1.2837393021724819,
      "grad_norm": 14.4107666015625,
      "learning_rate": 1.224536800497288e-05,
      "loss": 1.866,
      "step": 1950
    },
    {
      "epoch": 1.28439763001975,
      "grad_norm": 2.2552380561828613,
      "learning_rate": 1.2238649512737828e-05,
      "loss": 1.632,
      "step": 1951
    },
    {
      "epoch": 1.2850559578670178,
      "grad_norm": 3.412112236022949,
      "learning_rate": 1.2231929956536713e-05,
      "loss": 1.6505,
      "step": 1952
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 9.382243156433105,
      "learning_rate": 1.2225209339563144e-05,
      "loss": 2.0647,
      "step": 1953
    },
    {
      "epoch": 1.2863726135615536,
      "grad_norm": 9.31067180633545,
      "learning_rate": 1.2218487665011245e-05,
      "loss": 1.7856,
      "step": 1954
    },
    {
      "epoch": 1.2870309414088217,
      "grad_norm": 15.84212589263916,
      "learning_rate": 1.221176493607563e-05,
      "loss": 2.1684,
      "step": 1955
    },
    {
      "epoch": 1.2876892692560895,
      "grad_norm": 2.2687082290649414,
      "learning_rate": 1.220504115595142e-05,
      "loss": 1.6342,
      "step": 1956
    },
    {
      "epoch": 1.2883475971033573,
      "grad_norm": 2.1552722454071045,
      "learning_rate": 1.2198316327834236e-05,
      "loss": 1.6283,
      "step": 1957
    },
    {
      "epoch": 1.2890059249506254,
      "grad_norm": 25.416099548339844,
      "learning_rate": 1.2191590454920201e-05,
      "loss": 2.5588,
      "step": 1958
    },
    {
      "epoch": 1.2896642527978934,
      "grad_norm": 6.445657730102539,
      "learning_rate": 1.2184863540405924e-05,
      "loss": 1.7675,
      "step": 1959
    },
    {
      "epoch": 1.2903225806451613,
      "grad_norm": 1.7079962491989136,
      "learning_rate": 1.2178135587488515e-05,
      "loss": 1.6142,
      "step": 1960
    },
    {
      "epoch": 1.2909809084924293,
      "grad_norm": 7.593596458435059,
      "learning_rate": 1.2171406599365584e-05,
      "loss": 1.7964,
      "step": 1961
    },
    {
      "epoch": 1.2916392363396971,
      "grad_norm": 0.9357966184616089,
      "learning_rate": 1.2164676579235217e-05,
      "loss": 1.6031,
      "step": 1962
    },
    {
      "epoch": 1.2922975641869652,
      "grad_norm": 9.532113075256348,
      "learning_rate": 1.2157945530296007e-05,
      "loss": 1.9997,
      "step": 1963
    },
    {
      "epoch": 1.292955892034233,
      "grad_norm": 21.44019317626953,
      "learning_rate": 1.2151213455747031e-05,
      "loss": 2.0496,
      "step": 1964
    },
    {
      "epoch": 1.293614219881501,
      "grad_norm": 22.164926528930664,
      "learning_rate": 1.2144480358787843e-05,
      "loss": 2.3199,
      "step": 1965
    },
    {
      "epoch": 1.2942725477287689,
      "grad_norm": 2.6095752716064453,
      "learning_rate": 1.2137746242618498e-05,
      "loss": 1.6609,
      "step": 1966
    },
    {
      "epoch": 1.294930875576037,
      "grad_norm": 3.2717103958129883,
      "learning_rate": 1.2131011110439523e-05,
      "loss": 1.6582,
      "step": 1967
    },
    {
      "epoch": 1.2955892034233047,
      "grad_norm": 25.84861183166504,
      "learning_rate": 1.2124274965451938e-05,
      "loss": 2.5182,
      "step": 1968
    },
    {
      "epoch": 1.2962475312705728,
      "grad_norm": 20.41146469116211,
      "learning_rate": 1.2117537810857242e-05,
      "loss": 2.6146,
      "step": 1969
    },
    {
      "epoch": 1.2969058591178406,
      "grad_norm": 10.606066703796387,
      "learning_rate": 1.2110799649857407e-05,
      "loss": 2.0819,
      "step": 1970
    },
    {
      "epoch": 1.2975641869651087,
      "grad_norm": 8.483794212341309,
      "learning_rate": 1.210406048565489e-05,
      "loss": 1.8479,
      "step": 1971
    },
    {
      "epoch": 1.2982225148123765,
      "grad_norm": 17.563974380493164,
      "learning_rate": 1.2097320321452629e-05,
      "loss": 2.3538,
      "step": 1972
    },
    {
      "epoch": 1.2988808426596445,
      "grad_norm": 1.744853138923645,
      "learning_rate": 1.2090579160454017e-05,
      "loss": 1.6119,
      "step": 1973
    },
    {
      "epoch": 1.2995391705069124,
      "grad_norm": 24.208824157714844,
      "learning_rate": 1.2083837005862946e-05,
      "loss": 2.7866,
      "step": 1974
    },
    {
      "epoch": 1.3001974983541804,
      "grad_norm": 38.165367126464844,
      "learning_rate": 1.2077093860883768e-05,
      "loss": 2.7972,
      "step": 1975
    },
    {
      "epoch": 1.3008558262014482,
      "grad_norm": 13.993042945861816,
      "learning_rate": 1.2070349728721307e-05,
      "loss": 2.2622,
      "step": 1976
    },
    {
      "epoch": 1.3015141540487163,
      "grad_norm": 1.0835011005401611,
      "learning_rate": 1.2063604612580848e-05,
      "loss": 1.6116,
      "step": 1977
    },
    {
      "epoch": 1.3021724818959841,
      "grad_norm": 6.261628150939941,
      "learning_rate": 1.2056858515668161e-05,
      "loss": 1.9122,
      "step": 1978
    },
    {
      "epoch": 1.3028308097432522,
      "grad_norm": 2.161830425262451,
      "learning_rate": 1.2050111441189472e-05,
      "loss": 1.6237,
      "step": 1979
    },
    {
      "epoch": 1.3034891375905202,
      "grad_norm": 25.89176368713379,
      "learning_rate": 1.2043363392351468e-05,
      "loss": 2.6881,
      "step": 1980
    },
    {
      "epoch": 1.304147465437788,
      "grad_norm": 0.989247739315033,
      "learning_rate": 1.2036614372361309e-05,
      "loss": 1.6136,
      "step": 1981
    },
    {
      "epoch": 1.3048057932850559,
      "grad_norm": 8.237006187438965,
      "learning_rate": 1.2029864384426604e-05,
      "loss": 1.9167,
      "step": 1982
    },
    {
      "epoch": 1.305464121132324,
      "grad_norm": 4.810333728790283,
      "learning_rate": 1.2023113431755435e-05,
      "loss": 1.7682,
      "step": 1983
    },
    {
      "epoch": 1.306122448979592,
      "grad_norm": 3.063140869140625,
      "learning_rate": 1.2016361517556334e-05,
      "loss": 1.6349,
      "step": 1984
    },
    {
      "epoch": 1.3067807768268598,
      "grad_norm": 4.246967315673828,
      "learning_rate": 1.2009608645038293e-05,
      "loss": 1.6741,
      "step": 1985
    },
    {
      "epoch": 1.3074391046741276,
      "grad_norm": 4.638052940368652,
      "learning_rate": 1.2002854817410762e-05,
      "loss": 1.6735,
      "step": 1986
    },
    {
      "epoch": 1.3080974325213957,
      "grad_norm": 1.6360529661178589,
      "learning_rate": 1.1996100037883639e-05,
      "loss": 1.6057,
      "step": 1987
    },
    {
      "epoch": 1.3087557603686637,
      "grad_norm": 9.542486190795898,
      "learning_rate": 1.1989344309667278e-05,
      "loss": 1.9951,
      "step": 1988
    },
    {
      "epoch": 1.3094140882159315,
      "grad_norm": 24.835918426513672,
      "learning_rate": 1.1982587635972483e-05,
      "loss": 2.5961,
      "step": 1989
    },
    {
      "epoch": 1.3100724160631994,
      "grad_norm": 2.055943727493286,
      "learning_rate": 1.1975830020010508e-05,
      "loss": 1.622,
      "step": 1990
    },
    {
      "epoch": 1.3107307439104674,
      "grad_norm": 16.331823348999023,
      "learning_rate": 1.1969071464993054e-05,
      "loss": 2.7533,
      "step": 1991
    },
    {
      "epoch": 1.3113890717577354,
      "grad_norm": 9.02027702331543,
      "learning_rate": 1.196231197413227e-05,
      "loss": 2.0175,
      "step": 1992
    },
    {
      "epoch": 1.3120473996050033,
      "grad_norm": 14.145129203796387,
      "learning_rate": 1.1955551550640748e-05,
      "loss": 2.3189,
      "step": 1993
    },
    {
      "epoch": 1.312705727452271,
      "grad_norm": 1.0711735486984253,
      "learning_rate": 1.1948790197731523e-05,
      "loss": 1.5979,
      "step": 1994
    },
    {
      "epoch": 1.3133640552995391,
      "grad_norm": 8.987524032592773,
      "learning_rate": 1.1942027918618076e-05,
      "loss": 1.9542,
      "step": 1995
    },
    {
      "epoch": 1.3140223831468072,
      "grad_norm": 11.557028770446777,
      "learning_rate": 1.1935264716514324e-05,
      "loss": 2.0468,
      "step": 1996
    },
    {
      "epoch": 1.314680710994075,
      "grad_norm": 21.380231857299805,
      "learning_rate": 1.1928500594634613e-05,
      "loss": 2.7202,
      "step": 1997
    },
    {
      "epoch": 1.315339038841343,
      "grad_norm": 20.61231231689453,
      "learning_rate": 1.1921735556193755e-05,
      "loss": 2.1075,
      "step": 1998
    },
    {
      "epoch": 1.315997366688611,
      "grad_norm": 20.42952537536621,
      "learning_rate": 1.1914969604406963e-05,
      "loss": 2.6017,
      "step": 1999
    },
    {
      "epoch": 1.316655694535879,
      "grad_norm": 28.22997283935547,
      "learning_rate": 1.1908202742489911e-05,
      "loss": 2.3895,
      "step": 2000
    },
    {
      "epoch": 1.3173140223831468,
      "grad_norm": 4.567468166351318,
      "learning_rate": 1.190143497365869e-05,
      "loss": 1.652,
      "step": 2001
    },
    {
      "epoch": 1.3179723502304148,
      "grad_norm": 3.0602943897247314,
      "learning_rate": 1.1894666301129825e-05,
      "loss": 1.6371,
      "step": 2002
    },
    {
      "epoch": 1.3186306780776826,
      "grad_norm": 1.9374582767486572,
      "learning_rate": 1.1887896728120278e-05,
      "loss": 1.6315,
      "step": 2003
    },
    {
      "epoch": 1.3192890059249507,
      "grad_norm": 17.401365280151367,
      "learning_rate": 1.1881126257847429e-05,
      "loss": 2.2567,
      "step": 2004
    },
    {
      "epoch": 1.3199473337722185,
      "grad_norm": 20.155864715576172,
      "learning_rate": 1.187435489352909e-05,
      "loss": 2.1827,
      "step": 2005
    },
    {
      "epoch": 1.3206056616194866,
      "grad_norm": 7.612912654876709,
      "learning_rate": 1.18675826383835e-05,
      "loss": 1.9507,
      "step": 2006
    },
    {
      "epoch": 1.3212639894667544,
      "grad_norm": 6.532698154449463,
      "learning_rate": 1.1860809495629311e-05,
      "loss": 1.999,
      "step": 2007
    },
    {
      "epoch": 1.3219223173140224,
      "grad_norm": 4.530083656311035,
      "learning_rate": 1.1854035468485608e-05,
      "loss": 1.6544,
      "step": 2008
    },
    {
      "epoch": 1.3225806451612903,
      "grad_norm": 1.598029613494873,
      "learning_rate": 1.1847260560171895e-05,
      "loss": 1.5941,
      "step": 2009
    },
    {
      "epoch": 1.3232389730085583,
      "grad_norm": 11.151507377624512,
      "learning_rate": 1.1840484773908087e-05,
      "loss": 2.0412,
      "step": 2010
    },
    {
      "epoch": 1.3238973008558261,
      "grad_norm": 4.438724040985107,
      "learning_rate": 1.1833708112914523e-05,
      "loss": 1.7635,
      "step": 2011
    },
    {
      "epoch": 1.3245556287030942,
      "grad_norm": 12.91472339630127,
      "learning_rate": 1.1826930580411962e-05,
      "loss": 2.1745,
      "step": 2012
    },
    {
      "epoch": 1.325213956550362,
      "grad_norm": 6.768413543701172,
      "learning_rate": 1.1820152179621564e-05,
      "loss": 1.9387,
      "step": 2013
    },
    {
      "epoch": 1.32587228439763,
      "grad_norm": 8.10771369934082,
      "learning_rate": 1.1813372913764906e-05,
      "loss": 2.0032,
      "step": 2014
    },
    {
      "epoch": 1.3265306122448979,
      "grad_norm": 6.926836013793945,
      "learning_rate": 1.1806592786063991e-05,
      "loss": 1.9539,
      "step": 2015
    },
    {
      "epoch": 1.327188940092166,
      "grad_norm": 5.8828020095825195,
      "learning_rate": 1.1799811799741209e-05,
      "loss": 1.7016,
      "step": 2016
    },
    {
      "epoch": 1.327847267939434,
      "grad_norm": 1.5730794668197632,
      "learning_rate": 1.1793029958019373e-05,
      "loss": 1.6044,
      "step": 2017
    },
    {
      "epoch": 1.3285055957867018,
      "grad_norm": 13.089173316955566,
      "learning_rate": 1.1786247264121701e-05,
      "loss": 1.9953,
      "step": 2018
    },
    {
      "epoch": 1.3291639236339696,
      "grad_norm": 1.724913477897644,
      "learning_rate": 1.1779463721271807e-05,
      "loss": 1.6091,
      "step": 2019
    },
    {
      "epoch": 1.3298222514812377,
      "grad_norm": 2.326843023300171,
      "learning_rate": 1.1772679332693722e-05,
      "loss": 1.6154,
      "step": 2020
    },
    {
      "epoch": 1.3304805793285057,
      "grad_norm": 17.46356773376465,
      "learning_rate": 1.1765894101611864e-05,
      "loss": 2.0584,
      "step": 2021
    },
    {
      "epoch": 1.3311389071757735,
      "grad_norm": 5.556208610534668,
      "learning_rate": 1.1759108031251063e-05,
      "loss": 1.6679,
      "step": 2022
    },
    {
      "epoch": 1.3317972350230414,
      "grad_norm": 12.077079772949219,
      "learning_rate": 1.1752321124836548e-05,
      "loss": 1.8476,
      "step": 2023
    },
    {
      "epoch": 1.3324555628703094,
      "grad_norm": 6.853794574737549,
      "learning_rate": 1.1745533385593934e-05,
      "loss": 1.8102,
      "step": 2024
    },
    {
      "epoch": 1.3331138907175775,
      "grad_norm": 22.329204559326172,
      "learning_rate": 1.1738744816749243e-05,
      "loss": 2.5984,
      "step": 2025
    },
    {
      "epoch": 1.3337722185648453,
      "grad_norm": 16.635658264160156,
      "learning_rate": 1.1731955421528891e-05,
      "loss": 2.1266,
      "step": 2026
    },
    {
      "epoch": 1.3344305464121131,
      "grad_norm": 19.22334861755371,
      "learning_rate": 1.1725165203159678e-05,
      "loss": 2.4449,
      "step": 2027
    },
    {
      "epoch": 1.3350888742593812,
      "grad_norm": 27.752819061279297,
      "learning_rate": 1.1718374164868802e-05,
      "loss": 2.7672,
      "step": 2028
    },
    {
      "epoch": 1.3357472021066492,
      "grad_norm": 36.273773193359375,
      "learning_rate": 1.1711582309883852e-05,
      "loss": 3.0989,
      "step": 2029
    },
    {
      "epoch": 1.336405529953917,
      "grad_norm": 9.147497177124023,
      "learning_rate": 1.1704789641432796e-05,
      "loss": 1.9582,
      "step": 2030
    },
    {
      "epoch": 1.3370638578011849,
      "grad_norm": 24.52450180053711,
      "learning_rate": 1.1697996162743999e-05,
      "loss": 2.507,
      "step": 2031
    },
    {
      "epoch": 1.337722185648453,
      "grad_norm": 2.50044322013855,
      "learning_rate": 1.169120187704621e-05,
      "loss": 1.6152,
      "step": 2032
    },
    {
      "epoch": 1.338380513495721,
      "grad_norm": 4.612846851348877,
      "learning_rate": 1.1684406787568554e-05,
      "loss": 1.6566,
      "step": 2033
    },
    {
      "epoch": 1.3390388413429888,
      "grad_norm": 19.280643463134766,
      "learning_rate": 1.1677610897540542e-05,
      "loss": 2.3964,
      "step": 2034
    },
    {
      "epoch": 1.3396971691902566,
      "grad_norm": 1.3867439031600952,
      "learning_rate": 1.1670814210192071e-05,
      "loss": 1.6015,
      "step": 2035
    },
    {
      "epoch": 1.3403554970375247,
      "grad_norm": 9.90781021118164,
      "learning_rate": 1.166401672875341e-05,
      "loss": 2.0068,
      "step": 2036
    },
    {
      "epoch": 1.3410138248847927,
      "grad_norm": 10.289654731750488,
      "learning_rate": 1.1657218456455207e-05,
      "loss": 2.1049,
      "step": 2037
    },
    {
      "epoch": 1.3416721527320605,
      "grad_norm": 1.2433292865753174,
      "learning_rate": 1.1650419396528482e-05,
      "loss": 1.5817,
      "step": 2038
    },
    {
      "epoch": 1.3423304805793286,
      "grad_norm": 34.02655029296875,
      "learning_rate": 1.164361955220464e-05,
      "loss": 2.599,
      "step": 2039
    },
    {
      "epoch": 1.3429888084265964,
      "grad_norm": 2.551806688308716,
      "learning_rate": 1.1636818926715452e-05,
      "loss": 1.6036,
      "step": 2040
    },
    {
      "epoch": 1.3436471362738645,
      "grad_norm": 14.55557918548584,
      "learning_rate": 1.1630017523293055e-05,
      "loss": 2.0697,
      "step": 2041
    },
    {
      "epoch": 1.3443054641211323,
      "grad_norm": 4.910468101501465,
      "learning_rate": 1.1623215345169967e-05,
      "loss": 1.6924,
      "step": 2042
    },
    {
      "epoch": 1.3449637919684003,
      "grad_norm": 2.0891740322113037,
      "learning_rate": 1.161641239557907e-05,
      "loss": 1.6003,
      "step": 2043
    },
    {
      "epoch": 1.3456221198156681,
      "grad_norm": 19.918046951293945,
      "learning_rate": 1.1609608677753602e-05,
      "loss": 2.069,
      "step": 2044
    },
    {
      "epoch": 1.3462804476629362,
      "grad_norm": 2.0328502655029297,
      "learning_rate": 1.1602804194927187e-05,
      "loss": 1.6137,
      "step": 2045
    },
    {
      "epoch": 1.346938775510204,
      "grad_norm": 3.6087498664855957,
      "learning_rate": 1.1595998950333794e-05,
      "loss": 1.6309,
      "step": 2046
    },
    {
      "epoch": 1.347597103357472,
      "grad_norm": 27.141511917114258,
      "learning_rate": 1.1589192947207763e-05,
      "loss": 2.8413,
      "step": 2047
    },
    {
      "epoch": 1.34825543120474,
      "grad_norm": 4.298201084136963,
      "learning_rate": 1.1582386188783789e-05,
      "loss": 1.6532,
      "step": 2048
    },
    {
      "epoch": 1.348913759052008,
      "grad_norm": 23.03420639038086,
      "learning_rate": 1.157557867829694e-05,
      "loss": 2.6061,
      "step": 2049
    },
    {
      "epoch": 1.3495720868992758,
      "grad_norm": 21.9796085357666,
      "learning_rate": 1.156877041898262e-05,
      "loss": 2.4751,
      "step": 2050
    },
    {
      "epoch": 1.3502304147465438,
      "grad_norm": 28.067893981933594,
      "learning_rate": 1.1561961414076598e-05,
      "loss": 2.2253,
      "step": 2051
    },
    {
      "epoch": 1.3508887425938116,
      "grad_norm": 13.96785831451416,
      "learning_rate": 1.1555151666815014e-05,
      "loss": 2.1407,
      "step": 2052
    },
    {
      "epoch": 1.3515470704410797,
      "grad_norm": 17.26968765258789,
      "learning_rate": 1.1548341180434335e-05,
      "loss": 2.145,
      "step": 2053
    },
    {
      "epoch": 1.3522053982883475,
      "grad_norm": 15.531237602233887,
      "learning_rate": 1.1541529958171389e-05,
      "loss": 2.123,
      "step": 2054
    },
    {
      "epoch": 1.3528637261356156,
      "grad_norm": 17.628780364990234,
      "learning_rate": 1.153471800326336e-05,
      "loss": 2.3129,
      "step": 2055
    },
    {
      "epoch": 1.3535220539828834,
      "grad_norm": 1.638785481452942,
      "learning_rate": 1.1527905318947772e-05,
      "loss": 1.5998,
      "step": 2056
    },
    {
      "epoch": 1.3541803818301514,
      "grad_norm": 21.554014205932617,
      "learning_rate": 1.1521091908462503e-05,
      "loss": 2.7359,
      "step": 2057
    },
    {
      "epoch": 1.3548387096774195,
      "grad_norm": 9.193665504455566,
      "learning_rate": 1.1514277775045768e-05,
      "loss": 2.132,
      "step": 2058
    },
    {
      "epoch": 1.3554970375246873,
      "grad_norm": 26.43217658996582,
      "learning_rate": 1.150746292193613e-05,
      "loss": 2.6261,
      "step": 2059
    },
    {
      "epoch": 1.3561553653719551,
      "grad_norm": 2.7277300357818604,
      "learning_rate": 1.1500647352372497e-05,
      "loss": 1.605,
      "step": 2060
    },
    {
      "epoch": 1.3568136932192232,
      "grad_norm": 10.84356689453125,
      "learning_rate": 1.1493831069594112e-05,
      "loss": 2.027,
      "step": 2061
    },
    {
      "epoch": 1.3574720210664912,
      "grad_norm": 5.9714765548706055,
      "learning_rate": 1.148701407684056e-05,
      "loss": 1.8532,
      "step": 2062
    },
    {
      "epoch": 1.358130348913759,
      "grad_norm": 9.944074630737305,
      "learning_rate": 1.1480196377351761e-05,
      "loss": 2.0775,
      "step": 2063
    },
    {
      "epoch": 1.3587886767610269,
      "grad_norm": 12.845330238342285,
      "learning_rate": 1.1473377974367974e-05,
      "loss": 1.869,
      "step": 2064
    },
    {
      "epoch": 1.359447004608295,
      "grad_norm": 9.308859825134277,
      "learning_rate": 1.146655887112979e-05,
      "loss": 2.1474,
      "step": 2065
    },
    {
      "epoch": 1.360105332455563,
      "grad_norm": 9.807592391967773,
      "learning_rate": 1.1459739070878135e-05,
      "loss": 2.092,
      "step": 2066
    },
    {
      "epoch": 1.3607636603028308,
      "grad_norm": 4.642906188964844,
      "learning_rate": 1.1452918576854265e-05,
      "loss": 1.6581,
      "step": 2067
    },
    {
      "epoch": 1.3614219881500986,
      "grad_norm": 8.883065223693848,
      "learning_rate": 1.144609739229976e-05,
      "loss": 1.966,
      "step": 2068
    },
    {
      "epoch": 1.3620803159973667,
      "grad_norm": 6.127208232879639,
      "learning_rate": 1.1439275520456542e-05,
      "loss": 1.6823,
      "step": 2069
    },
    {
      "epoch": 1.3627386438446347,
      "grad_norm": 12.53448486328125,
      "learning_rate": 1.1432452964566849e-05,
      "loss": 2.1035,
      "step": 2070
    },
    {
      "epoch": 1.3633969716919025,
      "grad_norm": 19.13306999206543,
      "learning_rate": 1.1425629727873242e-05,
      "loss": 2.232,
      "step": 2071
    },
    {
      "epoch": 1.3640552995391704,
      "grad_norm": 17.98362922668457,
      "learning_rate": 1.1418805813618614e-05,
      "loss": 2.1356,
      "step": 2072
    },
    {
      "epoch": 1.3647136273864384,
      "grad_norm": 11.189577102661133,
      "learning_rate": 1.141198122504618e-05,
      "loss": 1.8202,
      "step": 2073
    },
    {
      "epoch": 1.3653719552337065,
      "grad_norm": 2.0806424617767334,
      "learning_rate": 1.1405155965399463e-05,
      "loss": 1.609,
      "step": 2074
    },
    {
      "epoch": 1.3660302830809743,
      "grad_norm": 10.250747680664062,
      "learning_rate": 1.139833003792232e-05,
      "loss": 2.0639,
      "step": 2075
    },
    {
      "epoch": 1.3666886109282423,
      "grad_norm": 3.8053531646728516,
      "learning_rate": 1.1391503445858918e-05,
      "loss": 1.6594,
      "step": 2076
    },
    {
      "epoch": 1.3673469387755102,
      "grad_norm": 11.674434661865234,
      "learning_rate": 1.138467619245374e-05,
      "loss": 2.4572,
      "step": 2077
    },
    {
      "epoch": 1.3680052666227782,
      "grad_norm": 10.907059669494629,
      "learning_rate": 1.1377848280951587e-05,
      "loss": 2.0924,
      "step": 2078
    },
    {
      "epoch": 1.368663594470046,
      "grad_norm": 6.8481574058532715,
      "learning_rate": 1.1371019714597562e-05,
      "loss": 1.9305,
      "step": 2079
    },
    {
      "epoch": 1.369321922317314,
      "grad_norm": 10.392617225646973,
      "learning_rate": 1.1364190496637097e-05,
      "loss": 2.0941,
      "step": 2080
    },
    {
      "epoch": 1.369980250164582,
      "grad_norm": 2.7508273124694824,
      "learning_rate": 1.1357360630315918e-05,
      "loss": 1.6212,
      "step": 2081
    },
    {
      "epoch": 1.37063857801185,
      "grad_norm": 6.779391288757324,
      "learning_rate": 1.1350530118880066e-05,
      "loss": 1.6878,
      "step": 2082
    },
    {
      "epoch": 1.3712969058591178,
      "grad_norm": 2.8867104053497314,
      "learning_rate": 1.1343698965575892e-05,
      "loss": 1.6364,
      "step": 2083
    },
    {
      "epoch": 1.3719552337063858,
      "grad_norm": 9.685952186584473,
      "learning_rate": 1.1336867173650041e-05,
      "loss": 1.794,
      "step": 2084
    },
    {
      "epoch": 1.3726135615536537,
      "grad_norm": 2.5163774490356445,
      "learning_rate": 1.133003474634947e-05,
      "loss": 1.6964,
      "step": 2085
    },
    {
      "epoch": 1.3732718894009217,
      "grad_norm": 3.331732749938965,
      "learning_rate": 1.132320168692144e-05,
      "loss": 1.6528,
      "step": 2086
    },
    {
      "epoch": 1.3739302172481895,
      "grad_norm": 19.344770431518555,
      "learning_rate": 1.1316367998613505e-05,
      "loss": 2.5402,
      "step": 2087
    },
    {
      "epoch": 1.3745885450954576,
      "grad_norm": 1.4527579545974731,
      "learning_rate": 1.1309533684673519e-05,
      "loss": 1.5876,
      "step": 2088
    },
    {
      "epoch": 1.3752468729427254,
      "grad_norm": 1.4269201755523682,
      "learning_rate": 1.1302698748349645e-05,
      "loss": 1.5863,
      "step": 2089
    },
    {
      "epoch": 1.3759052007899935,
      "grad_norm": 1.7008956670761108,
      "learning_rate": 1.1295863192890326e-05,
      "loss": 1.6085,
      "step": 2090
    },
    {
      "epoch": 1.3765635286372613,
      "grad_norm": 2.1911439895629883,
      "learning_rate": 1.1289027021544307e-05,
      "loss": 1.6063,
      "step": 2091
    },
    {
      "epoch": 1.3772218564845293,
      "grad_norm": 20.794677734375,
      "learning_rate": 1.1282190237560622e-05,
      "loss": 2.2142,
      "step": 2092
    },
    {
      "epoch": 1.3778801843317972,
      "grad_norm": 1.0316863059997559,
      "learning_rate": 1.1275352844188605e-05,
      "loss": 1.5943,
      "step": 2093
    },
    {
      "epoch": 1.3785385121790652,
      "grad_norm": 21.129074096679688,
      "learning_rate": 1.126851484467787e-05,
      "loss": 2.7976,
      "step": 2094
    },
    {
      "epoch": 1.3791968400263332,
      "grad_norm": 14.799318313598633,
      "learning_rate": 1.1261676242278318e-05,
      "loss": 1.9437,
      "step": 2095
    },
    {
      "epoch": 1.379855167873601,
      "grad_norm": 16.682268142700195,
      "learning_rate": 1.1254837040240149e-05,
      "loss": 1.7686,
      "step": 2096
    },
    {
      "epoch": 1.380513495720869,
      "grad_norm": 1.0052801370620728,
      "learning_rate": 1.1247997241813839e-05,
      "loss": 1.5949,
      "step": 2097
    },
    {
      "epoch": 1.381171823568137,
      "grad_norm": 11.900443077087402,
      "learning_rate": 1.1241156850250144e-05,
      "loss": 2.0465,
      "step": 2098
    },
    {
      "epoch": 1.381830151415405,
      "grad_norm": 17.103796005249023,
      "learning_rate": 1.1234315868800108e-05,
      "loss": 2.1361,
      "step": 2099
    },
    {
      "epoch": 1.3824884792626728,
      "grad_norm": 17.45086097717285,
      "learning_rate": 1.1227474300715056e-05,
      "loss": 2.0386,
      "step": 2100
    },
    {
      "epoch": 1.3831468071099406,
      "grad_norm": 9.883355140686035,
      "learning_rate": 1.1220632149246588e-05,
      "loss": 2.0367,
      "step": 2101
    },
    {
      "epoch": 1.3838051349572087,
      "grad_norm": 1.448639154434204,
      "learning_rate": 1.1213789417646579e-05,
      "loss": 1.5879,
      "step": 2102
    },
    {
      "epoch": 1.3844634628044767,
      "grad_norm": 1.637243628501892,
      "learning_rate": 1.1206946109167189e-05,
      "loss": 1.6034,
      "step": 2103
    },
    {
      "epoch": 1.3851217906517446,
      "grad_norm": 9.61210823059082,
      "learning_rate": 1.120010222706085e-05,
      "loss": 1.9168,
      "step": 2104
    },
    {
      "epoch": 1.3857801184990124,
      "grad_norm": 20.31421661376953,
      "learning_rate": 1.119325777458025e-05,
      "loss": 2.0664,
      "step": 2105
    },
    {
      "epoch": 1.3864384463462804,
      "grad_norm": 10.655616760253906,
      "learning_rate": 1.1186412754978375e-05,
      "loss": 1.9315,
      "step": 2106
    },
    {
      "epoch": 1.3870967741935485,
      "grad_norm": 23.230976104736328,
      "learning_rate": 1.1179567171508463e-05,
      "loss": 2.1728,
      "step": 2107
    },
    {
      "epoch": 1.3877551020408163,
      "grad_norm": 19.27560806274414,
      "learning_rate": 1.1172721027424021e-05,
      "loss": 2.3872,
      "step": 2108
    },
    {
      "epoch": 1.3884134298880841,
      "grad_norm": 7.554090976715088,
      "learning_rate": 1.1165874325978825e-05,
      "loss": 1.869,
      "step": 2109
    },
    {
      "epoch": 1.3890717577353522,
      "grad_norm": 26.356250762939453,
      "learning_rate": 1.1159027070426922e-05,
      "loss": 2.6867,
      "step": 2110
    },
    {
      "epoch": 1.3897300855826202,
      "grad_norm": 12.171375274658203,
      "learning_rate": 1.1152179264022611e-05,
      "loss": 2.0436,
      "step": 2111
    },
    {
      "epoch": 1.390388413429888,
      "grad_norm": 5.639232635498047,
      "learning_rate": 1.114533091002046e-05,
      "loss": 1.856,
      "step": 2112
    },
    {
      "epoch": 1.391046741277156,
      "grad_norm": 8.34467601776123,
      "learning_rate": 1.1138482011675296e-05,
      "loss": 2.1015,
      "step": 2113
    },
    {
      "epoch": 1.391705069124424,
      "grad_norm": 15.40482234954834,
      "learning_rate": 1.1131632572242208e-05,
      "loss": 2.0898,
      "step": 2114
    },
    {
      "epoch": 1.392363396971692,
      "grad_norm": 4.650865077972412,
      "learning_rate": 1.1124782594976533e-05,
      "loss": 1.6526,
      "step": 2115
    },
    {
      "epoch": 1.3930217248189598,
      "grad_norm": 3.4129862785339355,
      "learning_rate": 1.1117932083133871e-05,
      "loss": 1.7101,
      "step": 2116
    },
    {
      "epoch": 1.3936800526662279,
      "grad_norm": 7.647568225860596,
      "learning_rate": 1.1111081039970078e-05,
      "loss": 1.6627,
      "step": 2117
    },
    {
      "epoch": 1.3943383805134957,
      "grad_norm": 26.553831100463867,
      "learning_rate": 1.1104229468741252e-05,
      "loss": 2.1558,
      "step": 2118
    },
    {
      "epoch": 1.3949967083607637,
      "grad_norm": 2.0418460369110107,
      "learning_rate": 1.1097377372703755e-05,
      "loss": 1.6191,
      "step": 2119
    },
    {
      "epoch": 1.3956550362080316,
      "grad_norm": 10.791646003723145,
      "learning_rate": 1.109052475511419e-05,
      "loss": 1.9053,
      "step": 2120
    },
    {
      "epoch": 1.3963133640552996,
      "grad_norm": 5.795683860778809,
      "learning_rate": 1.1083671619229409e-05,
      "loss": 1.626,
      "step": 2121
    },
    {
      "epoch": 1.3969716919025674,
      "grad_norm": 10.700316429138184,
      "learning_rate": 1.1076817968306509e-05,
      "loss": 1.6982,
      "step": 2122
    },
    {
      "epoch": 1.3976300197498355,
      "grad_norm": 13.03862476348877,
      "learning_rate": 1.1069963805602839e-05,
      "loss": 2.0828,
      "step": 2123
    },
    {
      "epoch": 1.3982883475971033,
      "grad_norm": 16.132177352905273,
      "learning_rate": 1.1063109134375989e-05,
      "loss": 1.9469,
      "step": 2124
    },
    {
      "epoch": 1.3989466754443713,
      "grad_norm": 17.727888107299805,
      "learning_rate": 1.1056253957883777e-05,
      "loss": 2.0964,
      "step": 2125
    },
    {
      "epoch": 1.3996050032916392,
      "grad_norm": 173.8037567138672,
      "learning_rate": 1.1049398279384284e-05,
      "loss": 3.4072,
      "step": 2126
    },
    {
      "epoch": 1.4002633311389072,
      "grad_norm": 26.582059860229492,
      "learning_rate": 1.1042542102135813e-05,
      "loss": 2.4222,
      "step": 2127
    },
    {
      "epoch": 1.400921658986175,
      "grad_norm": 28.59858512878418,
      "learning_rate": 1.1035685429396906e-05,
      "loss": 2.6512,
      "step": 2128
    },
    {
      "epoch": 1.401579986833443,
      "grad_norm": 7.083073616027832,
      "learning_rate": 1.1028828264426347e-05,
      "loss": 1.6931,
      "step": 2129
    },
    {
      "epoch": 1.402238314680711,
      "grad_norm": 16.04559898376465,
      "learning_rate": 1.1021970610483152e-05,
      "loss": 2.0607,
      "step": 2130
    },
    {
      "epoch": 1.402896642527979,
      "grad_norm": 14.687079429626465,
      "learning_rate": 1.101511247082656e-05,
      "loss": 2.0084,
      "step": 2131
    },
    {
      "epoch": 1.4035549703752468,
      "grad_norm": 2.8325140476226807,
      "learning_rate": 1.1008253848716057e-05,
      "loss": 1.6201,
      "step": 2132
    },
    {
      "epoch": 1.4042132982225148,
      "grad_norm": 2.5644760131835938,
      "learning_rate": 1.1001394747411347e-05,
      "loss": 1.5869,
      "step": 2133
    },
    {
      "epoch": 1.4048716260697827,
      "grad_norm": 13.704314231872559,
      "learning_rate": 1.0994535170172367e-05,
      "loss": 2.0332,
      "step": 2134
    },
    {
      "epoch": 1.4055299539170507,
      "grad_norm": 13.177745819091797,
      "learning_rate": 1.0987675120259271e-05,
      "loss": 2.0437,
      "step": 2135
    },
    {
      "epoch": 1.4061882817643188,
      "grad_norm": 15.975018501281738,
      "learning_rate": 1.0980814600932447e-05,
      "loss": 2.293,
      "step": 2136
    },
    {
      "epoch": 1.4068466096115866,
      "grad_norm": 1.2516443729400635,
      "learning_rate": 1.097395361545251e-05,
      "loss": 1.598,
      "step": 2137
    },
    {
      "epoch": 1.4075049374588544,
      "grad_norm": 30.929147720336914,
      "learning_rate": 1.0967092167080285e-05,
      "loss": 2.9355,
      "step": 2138
    },
    {
      "epoch": 1.4081632653061225,
      "grad_norm": 22.340412139892578,
      "learning_rate": 1.0960230259076819e-05,
      "loss": 2.1449,
      "step": 2139
    },
    {
      "epoch": 1.4088215931533905,
      "grad_norm": 22.08444595336914,
      "learning_rate": 1.0953367894703386e-05,
      "loss": 2.2226,
      "step": 2140
    },
    {
      "epoch": 1.4094799210006583,
      "grad_norm": 37.83921432495117,
      "learning_rate": 1.094650507722147e-05,
      "loss": 3.6583,
      "step": 2141
    },
    {
      "epoch": 1.4101382488479262,
      "grad_norm": 2.588097095489502,
      "learning_rate": 1.0939641809892767e-05,
      "loss": 1.6163,
      "step": 2142
    },
    {
      "epoch": 1.4107965766951942,
      "grad_norm": 12.494032859802246,
      "learning_rate": 1.0932778095979197e-05,
      "loss": 1.8982,
      "step": 2143
    },
    {
      "epoch": 1.4114549045424623,
      "grad_norm": 9.345396041870117,
      "learning_rate": 1.0925913938742889e-05,
      "loss": 1.8744,
      "step": 2144
    },
    {
      "epoch": 1.41211323238973,
      "grad_norm": 9.910506248474121,
      "learning_rate": 1.0919049341446175e-05,
      "loss": 2.0507,
      "step": 2145
    },
    {
      "epoch": 1.412771560236998,
      "grad_norm": 13.613606452941895,
      "learning_rate": 1.0912184307351604e-05,
      "loss": 1.8768,
      "step": 2146
    },
    {
      "epoch": 1.413429888084266,
      "grad_norm": 10.053226470947266,
      "learning_rate": 1.090531883972193e-05,
      "loss": 1.95,
      "step": 2147
    },
    {
      "epoch": 1.414088215931534,
      "grad_norm": 16.211252212524414,
      "learning_rate": 1.0898452941820113e-05,
      "loss": 2.1909,
      "step": 2148
    },
    {
      "epoch": 1.4147465437788018,
      "grad_norm": 2.3242199420928955,
      "learning_rate": 1.0891586616909316e-05,
      "loss": 1.6005,
      "step": 2149
    },
    {
      "epoch": 1.4154048716260696,
      "grad_norm": 17.766538619995117,
      "learning_rate": 1.0884719868252913e-05,
      "loss": 2.0391,
      "step": 2150
    },
    {
      "epoch": 1.4160631994733377,
      "grad_norm": 8.256030082702637,
      "learning_rate": 1.0877852699114466e-05,
      "loss": 1.7022,
      "step": 2151
    },
    {
      "epoch": 1.4167215273206057,
      "grad_norm": 8.863424301147461,
      "learning_rate": 1.0870985112757748e-05,
      "loss": 2.0466,
      "step": 2152
    },
    {
      "epoch": 1.4173798551678736,
      "grad_norm": 2.5175836086273193,
      "learning_rate": 1.0864117112446725e-05,
      "loss": 1.6003,
      "step": 2153
    },
    {
      "epoch": 1.4180381830151416,
      "grad_norm": 40.711341857910156,
      "learning_rate": 1.0857248701445566e-05,
      "loss": 3.4029,
      "step": 2154
    },
    {
      "epoch": 1.4186965108624094,
      "grad_norm": 10.487722396850586,
      "learning_rate": 1.0850379883018623e-05,
      "loss": 2.1319,
      "step": 2155
    },
    {
      "epoch": 1.4193548387096775,
      "grad_norm": 23.76055908203125,
      "learning_rate": 1.0843510660430447e-05,
      "loss": 2.2513,
      "step": 2156
    },
    {
      "epoch": 1.4200131665569453,
      "grad_norm": 7.433319568634033,
      "learning_rate": 1.0836641036945794e-05,
      "loss": 1.9057,
      "step": 2157
    },
    {
      "epoch": 1.4206714944042134,
      "grad_norm": 1.3300834894180298,
      "learning_rate": 1.0829771015829594e-05,
      "loss": 1.5923,
      "step": 2158
    },
    {
      "epoch": 1.4213298222514812,
      "grad_norm": 12.980661392211914,
      "learning_rate": 1.0822900600346964e-05,
      "loss": 1.7485,
      "step": 2159
    },
    {
      "epoch": 1.4219881500987492,
      "grad_norm": 12.860316276550293,
      "learning_rate": 1.0816029793763227e-05,
      "loss": 1.9054,
      "step": 2160
    },
    {
      "epoch": 1.422646477946017,
      "grad_norm": 11.688307762145996,
      "learning_rate": 1.0809158599343876e-05,
      "loss": 2.0447,
      "step": 2161
    },
    {
      "epoch": 1.423304805793285,
      "grad_norm": 20.49762725830078,
      "learning_rate": 1.0802287020354594e-05,
      "loss": 2.3007,
      "step": 2162
    },
    {
      "epoch": 1.423963133640553,
      "grad_norm": 23.320049285888672,
      "learning_rate": 1.0795415060061242e-05,
      "loss": 1.9971,
      "step": 2163
    },
    {
      "epoch": 1.424621461487821,
      "grad_norm": 1.0995454788208008,
      "learning_rate": 1.0788542721729873e-05,
      "loss": 1.583,
      "step": 2164
    },
    {
      "epoch": 1.4252797893350888,
      "grad_norm": 7.891900062561035,
      "learning_rate": 1.0781670008626707e-05,
      "loss": 1.6683,
      "step": 2165
    },
    {
      "epoch": 1.4259381171823569,
      "grad_norm": 25.524213790893555,
      "learning_rate": 1.0774796924018152e-05,
      "loss": 2.0657,
      "step": 2166
    },
    {
      "epoch": 1.4265964450296247,
      "grad_norm": 18.089601516723633,
      "learning_rate": 1.0767923471170786e-05,
      "loss": 2.1909,
      "step": 2167
    },
    {
      "epoch": 1.4272547728768927,
      "grad_norm": 15.944912910461426,
      "learning_rate": 1.0761049653351362e-05,
      "loss": 2.0132,
      "step": 2168
    },
    {
      "epoch": 1.4279131007241606,
      "grad_norm": 3.195679187774658,
      "learning_rate": 1.0754175473826815e-05,
      "loss": 1.6963,
      "step": 2169
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 1.2204447984695435,
      "learning_rate": 1.0747300935864245e-05,
      "loss": 1.594,
      "step": 2170
    },
    {
      "epoch": 1.4292297564186964,
      "grad_norm": 11.501195907592773,
      "learning_rate": 1.0740426042730917e-05,
      "loss": 1.7606,
      "step": 2171
    },
    {
      "epoch": 1.4298880842659645,
      "grad_norm": 8.288370132446289,
      "learning_rate": 1.073355079769428e-05,
      "loss": 1.8624,
      "step": 2172
    },
    {
      "epoch": 1.4305464121132325,
      "grad_norm": 3.0833802223205566,
      "learning_rate": 1.0726675204021928e-05,
      "loss": 1.6274,
      "step": 2173
    },
    {
      "epoch": 1.4312047399605003,
      "grad_norm": 28.29083824157715,
      "learning_rate": 1.071979926498165e-05,
      "loss": 2.8089,
      "step": 2174
    },
    {
      "epoch": 1.4318630678077682,
      "grad_norm": 22.704668045043945,
      "learning_rate": 1.0712922983841374e-05,
      "loss": 2.1909,
      "step": 2175
    },
    {
      "epoch": 1.4325213956550362,
      "grad_norm": 8.438177108764648,
      "learning_rate": 1.0706046363869196e-05,
      "loss": 1.8445,
      "step": 2176
    },
    {
      "epoch": 1.4331797235023043,
      "grad_norm": 5.712281227111816,
      "learning_rate": 1.0699169408333387e-05,
      "loss": 1.7527,
      "step": 2177
    },
    {
      "epoch": 1.433838051349572,
      "grad_norm": 8.224143028259277,
      "learning_rate": 1.0692292120502364e-05,
      "loss": 1.8156,
      "step": 2178
    },
    {
      "epoch": 1.43449637919684,
      "grad_norm": 11.511683464050293,
      "learning_rate": 1.0685414503644697e-05,
      "loss": 1.9885,
      "step": 2179
    },
    {
      "epoch": 1.435154707044108,
      "grad_norm": 10.958858489990234,
      "learning_rate": 1.067853656102913e-05,
      "loss": 2.0331,
      "step": 2180
    },
    {
      "epoch": 1.435813034891376,
      "grad_norm": 10.670624732971191,
      "learning_rate": 1.067165829592455e-05,
      "loss": 2.0043,
      "step": 2181
    },
    {
      "epoch": 1.4364713627386438,
      "grad_norm": 17.0616512298584,
      "learning_rate": 1.0664779711599995e-05,
      "loss": 2.1249,
      "step": 2182
    },
    {
      "epoch": 1.4371296905859117,
      "grad_norm": 38.47516632080078,
      "learning_rate": 1.0657900811324666e-05,
      "loss": 2.2101,
      "step": 2183
    },
    {
      "epoch": 1.4377880184331797,
      "grad_norm": 11.278708457946777,
      "learning_rate": 1.0651021598367905e-05,
      "loss": 2.2664,
      "step": 2184
    },
    {
      "epoch": 1.4384463462804478,
      "grad_norm": 20.28569793701172,
      "learning_rate": 1.0644142075999201e-05,
      "loss": 2.096,
      "step": 2185
    },
    {
      "epoch": 1.4391046741277156,
      "grad_norm": 21.28508186340332,
      "learning_rate": 1.06372622474882e-05,
      "loss": 2.5257,
      "step": 2186
    },
    {
      "epoch": 1.4397630019749834,
      "grad_norm": 1.4138832092285156,
      "learning_rate": 1.0630382116104688e-05,
      "loss": 1.5809,
      "step": 2187
    },
    {
      "epoch": 1.4404213298222515,
      "grad_norm": 10.725486755371094,
      "learning_rate": 1.0623501685118586e-05,
      "loss": 2.2444,
      "step": 2188
    },
    {
      "epoch": 1.4410796576695195,
      "grad_norm": 4.866662979125977,
      "learning_rate": 1.0616620957799977e-05,
      "loss": 1.6283,
      "step": 2189
    },
    {
      "epoch": 1.4417379855167873,
      "grad_norm": 6.842565536499023,
      "learning_rate": 1.0609739937419068e-05,
      "loss": 1.7164,
      "step": 2190
    },
    {
      "epoch": 1.4423963133640554,
      "grad_norm": 23.082260131835938,
      "learning_rate": 1.0602858627246208e-05,
      "loss": 2.3779,
      "step": 2191
    },
    {
      "epoch": 1.4430546412113232,
      "grad_norm": 9.527081489562988,
      "learning_rate": 1.0595977030551896e-05,
      "loss": 2.0122,
      "step": 2192
    },
    {
      "epoch": 1.4437129690585913,
      "grad_norm": 7.053195953369141,
      "learning_rate": 1.0589095150606747e-05,
      "loss": 1.7574,
      "step": 2193
    },
    {
      "epoch": 1.444371296905859,
      "grad_norm": 3.085174083709717,
      "learning_rate": 1.0582212990681534e-05,
      "loss": 1.6645,
      "step": 2194
    },
    {
      "epoch": 1.4450296247531271,
      "grad_norm": 17.970735549926758,
      "learning_rate": 1.0575330554047143e-05,
      "loss": 2.1003,
      "step": 2195
    },
    {
      "epoch": 1.445687952600395,
      "grad_norm": 19.540557861328125,
      "learning_rate": 1.0568447843974596e-05,
      "loss": 2.1116,
      "step": 2196
    },
    {
      "epoch": 1.446346280447663,
      "grad_norm": 14.051051139831543,
      "learning_rate": 1.0561564863735055e-05,
      "loss": 2.366,
      "step": 2197
    },
    {
      "epoch": 1.4470046082949308,
      "grad_norm": 7.058327674865723,
      "learning_rate": 1.0554681616599803e-05,
      "loss": 1.9602,
      "step": 2198
    },
    {
      "epoch": 1.4476629361421989,
      "grad_norm": 8.35659122467041,
      "learning_rate": 1.0547798105840245e-05,
      "loss": 1.8244,
      "step": 2199
    },
    {
      "epoch": 1.4483212639894667,
      "grad_norm": 6.751095294952393,
      "learning_rate": 1.0540914334727921e-05,
      "loss": 1.6887,
      "step": 2200
    },
    {
      "epoch": 1.4489795918367347,
      "grad_norm": 1.6699093580245972,
      "learning_rate": 1.0534030306534491e-05,
      "loss": 1.5824,
      "step": 2201
    },
    {
      "epoch": 1.4496379196840026,
      "grad_norm": 27.63909339904785,
      "learning_rate": 1.0527146024531733e-05,
      "loss": 2.3303,
      "step": 2202
    },
    {
      "epoch": 1.4502962475312706,
      "grad_norm": 20.191267013549805,
      "learning_rate": 1.0520261491991555e-05,
      "loss": 2.0801,
      "step": 2203
    },
    {
      "epoch": 1.4509545753785384,
      "grad_norm": 8.743863105773926,
      "learning_rate": 1.0513376712185974e-05,
      "loss": 1.886,
      "step": 2204
    },
    {
      "epoch": 1.4516129032258065,
      "grad_norm": 1.051529884338379,
      "learning_rate": 1.0506491688387128e-05,
      "loss": 1.5771,
      "step": 2205
    },
    {
      "epoch": 1.4522712310730743,
      "grad_norm": 2.4062860012054443,
      "learning_rate": 1.0499606423867274e-05,
      "loss": 1.5921,
      "step": 2206
    },
    {
      "epoch": 1.4529295589203424,
      "grad_norm": 23.886308670043945,
      "learning_rate": 1.0492720921898784e-05,
      "loss": 2.2751,
      "step": 2207
    },
    {
      "epoch": 1.4535878867676102,
      "grad_norm": 13.501264572143555,
      "learning_rate": 1.0485835185754133e-05,
      "loss": 1.9982,
      "step": 2208
    },
    {
      "epoch": 1.4542462146148782,
      "grad_norm": 26.723295211791992,
      "learning_rate": 1.0478949218705923e-05,
      "loss": 2.5921,
      "step": 2209
    },
    {
      "epoch": 1.454904542462146,
      "grad_norm": 22.6666202545166,
      "learning_rate": 1.0472063024026848e-05,
      "loss": 2.3209,
      "step": 2210
    },
    {
      "epoch": 1.4555628703094141,
      "grad_norm": 12.252516746520996,
      "learning_rate": 1.0465176604989724e-05,
      "loss": 1.987,
      "step": 2211
    },
    {
      "epoch": 1.456221198156682,
      "grad_norm": 8.196998596191406,
      "learning_rate": 1.0458289964867473e-05,
      "loss": 1.9473,
      "step": 2212
    },
    {
      "epoch": 1.45687952600395,
      "grad_norm": 4.939297199249268,
      "learning_rate": 1.045140310693311e-05,
      "loss": 1.6465,
      "step": 2213
    },
    {
      "epoch": 1.457537853851218,
      "grad_norm": 3.4125869274139404,
      "learning_rate": 1.0444516034459764e-05,
      "loss": 1.6182,
      "step": 2214
    },
    {
      "epoch": 1.4581961816984859,
      "grad_norm": 4.945363998413086,
      "learning_rate": 1.043762875072067e-05,
      "loss": 1.817,
      "step": 2215
    },
    {
      "epoch": 1.4588545095457537,
      "grad_norm": 2.661208391189575,
      "learning_rate": 1.0430741258989148e-05,
      "loss": 1.6121,
      "step": 2216
    },
    {
      "epoch": 1.4595128373930217,
      "grad_norm": 1.3654955625534058,
      "learning_rate": 1.0423853562538628e-05,
      "loss": 1.5947,
      "step": 2217
    },
    {
      "epoch": 1.4601711652402898,
      "grad_norm": 12.69777774810791,
      "learning_rate": 1.041696566464264e-05,
      "loss": 1.8749,
      "step": 2218
    },
    {
      "epoch": 1.4608294930875576,
      "grad_norm": 19.739543914794922,
      "learning_rate": 1.0410077568574799e-05,
      "loss": 2.3402,
      "step": 2219
    },
    {
      "epoch": 1.4614878209348254,
      "grad_norm": 24.97065544128418,
      "learning_rate": 1.0403189277608823e-05,
      "loss": 2.4809,
      "step": 2220
    },
    {
      "epoch": 1.4621461487820935,
      "grad_norm": 8.506436347961426,
      "learning_rate": 1.0396300795018522e-05,
      "loss": 1.9014,
      "step": 2221
    },
    {
      "epoch": 1.4628044766293615,
      "grad_norm": 1.5170528888702393,
      "learning_rate": 1.0389412124077786e-05,
      "loss": 1.5798,
      "step": 2222
    },
    {
      "epoch": 1.4634628044766294,
      "grad_norm": 7.519004821777344,
      "learning_rate": 1.0382523268060612e-05,
      "loss": 1.9048,
      "step": 2223
    },
    {
      "epoch": 1.4641211323238972,
      "grad_norm": 16.786497116088867,
      "learning_rate": 1.0375634230241074e-05,
      "loss": 2.0316,
      "step": 2224
    },
    {
      "epoch": 1.4647794601711652,
      "grad_norm": 16.001575469970703,
      "learning_rate": 1.0368745013893332e-05,
      "loss": 2.187,
      "step": 2225
    },
    {
      "epoch": 1.4654377880184333,
      "grad_norm": 2.336735486984253,
      "learning_rate": 1.0361855622291637e-05,
      "loss": 1.5979,
      "step": 2226
    },
    {
      "epoch": 1.466096115865701,
      "grad_norm": 5.569124221801758,
      "learning_rate": 1.0354966058710314e-05,
      "loss": 1.7997,
      "step": 2227
    },
    {
      "epoch": 1.466754443712969,
      "grad_norm": 1.76809561252594,
      "learning_rate": 1.034807632642378e-05,
      "loss": 1.5992,
      "step": 2228
    },
    {
      "epoch": 1.467412771560237,
      "grad_norm": 4.464003562927246,
      "learning_rate": 1.034118642870653e-05,
      "loss": 1.7408,
      "step": 2229
    },
    {
      "epoch": 1.468071099407505,
      "grad_norm": 2.4043314456939697,
      "learning_rate": 1.0334296368833125e-05,
      "loss": 1.6015,
      "step": 2230
    },
    {
      "epoch": 1.4687294272547728,
      "grad_norm": 22.88658332824707,
      "learning_rate": 1.0327406150078228e-05,
      "loss": 2.1136,
      "step": 2231
    },
    {
      "epoch": 1.469387755102041,
      "grad_norm": 1.3436702489852905,
      "learning_rate": 1.0320515775716556e-05,
      "loss": 1.5777,
      "step": 2232
    },
    {
      "epoch": 1.4700460829493087,
      "grad_norm": 16.7060489654541,
      "learning_rate": 1.03136252490229e-05,
      "loss": 2.2103,
      "step": 2233
    },
    {
      "epoch": 1.4707044107965768,
      "grad_norm": 27.4038143157959,
      "learning_rate": 1.0306734573272136e-05,
      "loss": 2.5593,
      "step": 2234
    },
    {
      "epoch": 1.4713627386438446,
      "grad_norm": 2.07810115814209,
      "learning_rate": 1.0299843751739212e-05,
      "loss": 1.5907,
      "step": 2235
    },
    {
      "epoch": 1.4720210664911126,
      "grad_norm": 34.73283767700195,
      "learning_rate": 1.0292952787699126e-05,
      "loss": 2.7317,
      "step": 2236
    },
    {
      "epoch": 1.4726793943383805,
      "grad_norm": 23.1137638092041,
      "learning_rate": 1.0286061684426963e-05,
      "loss": 2.3356,
      "step": 2237
    },
    {
      "epoch": 1.4733377221856485,
      "grad_norm": 1.0580651760101318,
      "learning_rate": 1.0279170445197869e-05,
      "loss": 1.5672,
      "step": 2238
    },
    {
      "epoch": 1.4739960500329163,
      "grad_norm": 7.936003684997559,
      "learning_rate": 1.0272279073287043e-05,
      "loss": 1.8856,
      "step": 2239
    },
    {
      "epoch": 1.4746543778801844,
      "grad_norm": 10.927790641784668,
      "learning_rate": 1.0265387571969768e-05,
      "loss": 2.3228,
      "step": 2240
    },
    {
      "epoch": 1.4753127057274522,
      "grad_norm": 16.330835342407227,
      "learning_rate": 1.0258495944521374e-05,
      "loss": 2.124,
      "step": 2241
    },
    {
      "epoch": 1.4759710335747203,
      "grad_norm": 20.067537307739258,
      "learning_rate": 1.0251604194217251e-05,
      "loss": 2.3511,
      "step": 2242
    },
    {
      "epoch": 1.476629361421988,
      "grad_norm": 5.268579959869385,
      "learning_rate": 1.0244712324332856e-05,
      "loss": 1.6213,
      "step": 2243
    },
    {
      "epoch": 1.4772876892692561,
      "grad_norm": 7.375617504119873,
      "learning_rate": 1.0237820338143691e-05,
      "loss": 1.7483,
      "step": 2244
    },
    {
      "epoch": 1.477946017116524,
      "grad_norm": 25.098752975463867,
      "learning_rate": 1.0230928238925323e-05,
      "loss": 2.3717,
      "step": 2245
    },
    {
      "epoch": 1.478604344963792,
      "grad_norm": 2.3997397422790527,
      "learning_rate": 1.0224036029953375e-05,
      "loss": 1.5896,
      "step": 2246
    },
    {
      "epoch": 1.4792626728110598,
      "grad_norm": 20.057252883911133,
      "learning_rate": 1.0217143714503508e-05,
      "loss": 2.3184,
      "step": 2247
    },
    {
      "epoch": 1.4799210006583279,
      "grad_norm": 10.043161392211914,
      "learning_rate": 1.0210251295851445e-05,
      "loss": 1.9384,
      "step": 2248
    },
    {
      "epoch": 1.4805793285055957,
      "grad_norm": 9.364724159240723,
      "learning_rate": 1.020335877727296e-05,
      "loss": 1.7951,
      "step": 2249
    },
    {
      "epoch": 1.4812376563528638,
      "grad_norm": 26.54902458190918,
      "learning_rate": 1.0196466162043866e-05,
      "loss": 2.4669,
      "step": 2250
    },
    {
      "epoch": 1.4818959842001318,
      "grad_norm": 4.697421550750732,
      "learning_rate": 1.0189573453440024e-05,
      "loss": 1.7241,
      "step": 2251
    },
    {
      "epoch": 1.4825543120473996,
      "grad_norm": 1.7195079326629639,
      "learning_rate": 1.0182680654737348e-05,
      "loss": 1.5789,
      "step": 2252
    },
    {
      "epoch": 1.4832126398946675,
      "grad_norm": 12.185050010681152,
      "learning_rate": 1.0175787769211778e-05,
      "loss": 2.2157,
      "step": 2253
    },
    {
      "epoch": 1.4838709677419355,
      "grad_norm": 19.4929256439209,
      "learning_rate": 1.0168894800139311e-05,
      "loss": 2.003,
      "step": 2254
    },
    {
      "epoch": 1.4845292955892035,
      "grad_norm": 10.210500717163086,
      "learning_rate": 1.0162001750795982e-05,
      "loss": 1.6809,
      "step": 2255
    },
    {
      "epoch": 1.4851876234364714,
      "grad_norm": 7.008435249328613,
      "learning_rate": 1.0155108624457853e-05,
      "loss": 2.0697,
      "step": 2256
    },
    {
      "epoch": 1.4858459512837392,
      "grad_norm": 11.008559226989746,
      "learning_rate": 1.0148215424401032e-05,
      "loss": 1.8248,
      "step": 2257
    },
    {
      "epoch": 1.4865042791310072,
      "grad_norm": 14.444803237915039,
      "learning_rate": 1.0141322153901662e-05,
      "loss": 2.4042,
      "step": 2258
    },
    {
      "epoch": 1.4871626069782753,
      "grad_norm": 1.8790258169174194,
      "learning_rate": 1.0134428816235914e-05,
      "loss": 1.59,
      "step": 2259
    },
    {
      "epoch": 1.4878209348255431,
      "grad_norm": 8.63924503326416,
      "learning_rate": 1.0127535414679997e-05,
      "loss": 2.0832,
      "step": 2260
    },
    {
      "epoch": 1.488479262672811,
      "grad_norm": 5.321118354797363,
      "learning_rate": 1.0120641952510147e-05,
      "loss": 1.7442,
      "step": 2261
    },
    {
      "epoch": 1.489137590520079,
      "grad_norm": 1.1567738056182861,
      "learning_rate": 1.0113748433002628e-05,
      "loss": 1.5787,
      "step": 2262
    },
    {
      "epoch": 1.489795918367347,
      "grad_norm": 5.320582389831543,
      "learning_rate": 1.0106854859433734e-05,
      "loss": 1.6307,
      "step": 2263
    },
    {
      "epoch": 1.4904542462146149,
      "grad_norm": 20.13910675048828,
      "learning_rate": 1.0099961235079781e-05,
      "loss": 2.2127,
      "step": 2264
    },
    {
      "epoch": 1.4911125740618827,
      "grad_norm": 10.554838180541992,
      "learning_rate": 1.009306756321711e-05,
      "loss": 1.9909,
      "step": 2265
    },
    {
      "epoch": 1.4917709019091507,
      "grad_norm": 21.713077545166016,
      "learning_rate": 1.0086173847122095e-05,
      "loss": 2.2404,
      "step": 2266
    },
    {
      "epoch": 1.4924292297564188,
      "grad_norm": 2.329566240310669,
      "learning_rate": 1.0079280090071112e-05,
      "loss": 1.595,
      "step": 2267
    },
    {
      "epoch": 1.4930875576036866,
      "grad_norm": 2.374196767807007,
      "learning_rate": 1.0072386295340572e-05,
      "loss": 1.5931,
      "step": 2268
    },
    {
      "epoch": 1.4937458854509547,
      "grad_norm": 6.393774032592773,
      "learning_rate": 1.0065492466206899e-05,
      "loss": 1.7884,
      "step": 2269
    },
    {
      "epoch": 1.4944042132982225,
      "grad_norm": 10.976576805114746,
      "learning_rate": 1.0058598605946525e-05,
      "loss": 1.868,
      "step": 2270
    },
    {
      "epoch": 1.4950625411454905,
      "grad_norm": 10.495136260986328,
      "learning_rate": 1.0051704717835915e-05,
      "loss": 1.9707,
      "step": 2271
    },
    {
      "epoch": 1.4957208689927584,
      "grad_norm": 17.579864501953125,
      "learning_rate": 1.004481080515153e-05,
      "loss": 2.1213,
      "step": 2272
    },
    {
      "epoch": 1.4963791968400264,
      "grad_norm": 1.5932841300964355,
      "learning_rate": 1.003791687116985e-05,
      "loss": 1.5839,
      "step": 2273
    },
    {
      "epoch": 1.4970375246872942,
      "grad_norm": 1.2757588624954224,
      "learning_rate": 1.0031022919167369e-05,
      "loss": 1.5812,
      "step": 2274
    },
    {
      "epoch": 1.4976958525345623,
      "grad_norm": 30.61459732055664,
      "learning_rate": 1.0024128952420583e-05,
      "loss": 2.7009,
      "step": 2275
    },
    {
      "epoch": 1.49835418038183,
      "grad_norm": 3.149038553237915,
      "learning_rate": 1.0017234974205992e-05,
      "loss": 1.622,
      "step": 2276
    },
    {
      "epoch": 1.4990125082290982,
      "grad_norm": 9.769227027893066,
      "learning_rate": 1.0010340987800112e-05,
      "loss": 2.0351,
      "step": 2277
    },
    {
      "epoch": 1.499670836076366,
      "grad_norm": 2.636880874633789,
      "learning_rate": 1.0003446996479458e-05,
      "loss": 1.5958,
      "step": 2278
    },
    {
      "epoch": 1.500329163923634,
      "grad_norm": 6.256493091583252,
      "learning_rate": 9.996553003520543e-06,
      "loss": 1.6324,
      "step": 2279
    },
    {
      "epoch": 1.500987491770902,
      "grad_norm": 10.7005615234375,
      "learning_rate": 9.989659012199888e-06,
      "loss": 1.964,
      "step": 2280
    },
    {
      "epoch": 1.50164581961817,
      "grad_norm": 2.0694050788879395,
      "learning_rate": 9.982765025794011e-06,
      "loss": 1.5895,
      "step": 2281
    },
    {
      "epoch": 1.5023041474654377,
      "grad_norm": 20.617530822753906,
      "learning_rate": 9.97587104757942e-06,
      "loss": 2.0655,
      "step": 2282
    },
    {
      "epoch": 1.5029624753127058,
      "grad_norm": 18.342105865478516,
      "learning_rate": 9.968977080832633e-06,
      "loss": 2.0161,
      "step": 2283
    },
    {
      "epoch": 1.5036208031599738,
      "grad_norm": 7.334378719329834,
      "learning_rate": 9.962083128830151e-06,
      "loss": 1.876,
      "step": 2284
    },
    {
      "epoch": 1.5042791310072416,
      "grad_norm": 19.522939682006836,
      "learning_rate": 9.955189194848473e-06,
      "loss": 2.0816,
      "step": 2285
    },
    {
      "epoch": 1.5049374588545095,
      "grad_norm": 31.137250900268555,
      "learning_rate": 9.948295282164087e-06,
      "loss": 1.9398,
      "step": 2286
    },
    {
      "epoch": 1.5055957867017775,
      "grad_norm": 3.569131374359131,
      "learning_rate": 9.941401394053477e-06,
      "loss": 1.6557,
      "step": 2287
    },
    {
      "epoch": 1.5062541145490456,
      "grad_norm": 7.805257320404053,
      "learning_rate": 9.934507533793105e-06,
      "loss": 1.8098,
      "step": 2288
    },
    {
      "epoch": 1.5069124423963134,
      "grad_norm": 25.094961166381836,
      "learning_rate": 9.92761370465943e-06,
      "loss": 2.4929,
      "step": 2289
    },
    {
      "epoch": 1.5075707702435812,
      "grad_norm": 3.20727801322937,
      "learning_rate": 9.92071990992889e-06,
      "loss": 1.6055,
      "step": 2290
    },
    {
      "epoch": 1.5082290980908493,
      "grad_norm": 9.634259223937988,
      "learning_rate": 9.913826152877908e-06,
      "loss": 1.9055,
      "step": 2291
    },
    {
      "epoch": 1.5088874259381173,
      "grad_norm": 2.6556396484375,
      "learning_rate": 9.906932436782889e-06,
      "loss": 1.5845,
      "step": 2292
    },
    {
      "epoch": 1.5095457537853851,
      "grad_norm": 1.8435487747192383,
      "learning_rate": 9.900038764920224e-06,
      "loss": 1.5816,
      "step": 2293
    },
    {
      "epoch": 1.510204081632653,
      "grad_norm": 10.490877151489258,
      "learning_rate": 9.89314514056627e-06,
      "loss": 1.7548,
      "step": 2294
    },
    {
      "epoch": 1.510862409479921,
      "grad_norm": 2.418722152709961,
      "learning_rate": 9.886251566997373e-06,
      "loss": 1.5985,
      "step": 2295
    },
    {
      "epoch": 1.511520737327189,
      "grad_norm": 20.185440063476562,
      "learning_rate": 9.879358047489858e-06,
      "loss": 1.9991,
      "step": 2296
    },
    {
      "epoch": 1.5121790651744569,
      "grad_norm": 18.013059616088867,
      "learning_rate": 9.872464585320006e-06,
      "loss": 2.039,
      "step": 2297
    },
    {
      "epoch": 1.5128373930217247,
      "grad_norm": 11.661652565002441,
      "learning_rate": 9.865571183764086e-06,
      "loss": 2.096,
      "step": 2298
    },
    {
      "epoch": 1.5134957208689928,
      "grad_norm": 7.565800189971924,
      "learning_rate": 9.858677846098343e-06,
      "loss": 1.7791,
      "step": 2299
    },
    {
      "epoch": 1.5141540487162608,
      "grad_norm": 1.860181450843811,
      "learning_rate": 9.851784575598972e-06,
      "loss": 1.5742,
      "step": 2300
    },
    {
      "epoch": 1.5148123765635286,
      "grad_norm": 2.431985378265381,
      "learning_rate": 9.844891375542149e-06,
      "loss": 1.5749,
      "step": 2301
    },
    {
      "epoch": 1.5154707044107965,
      "grad_norm": 2.9347598552703857,
      "learning_rate": 9.837998249204024e-06,
      "loss": 1.5926,
      "step": 2302
    },
    {
      "epoch": 1.5161290322580645,
      "grad_norm": 1.2308759689331055,
      "learning_rate": 9.83110519986069e-06,
      "loss": 1.5636,
      "step": 2303
    },
    {
      "epoch": 1.5167873601053325,
      "grad_norm": 12.099336624145508,
      "learning_rate": 9.824212230788226e-06,
      "loss": 2.1523,
      "step": 2304
    },
    {
      "epoch": 1.5174456879526004,
      "grad_norm": 3.4111828804016113,
      "learning_rate": 9.81731934526266e-06,
      "loss": 1.6185,
      "step": 2305
    },
    {
      "epoch": 1.5181040157998682,
      "grad_norm": 2.6053102016448975,
      "learning_rate": 9.810426546559979e-06,
      "loss": 1.5782,
      "step": 2306
    },
    {
      "epoch": 1.5187623436471362,
      "grad_norm": 30.686620712280273,
      "learning_rate": 9.803533837956138e-06,
      "loss": 2.6632,
      "step": 2307
    },
    {
      "epoch": 1.5194206714944043,
      "grad_norm": 13.551676750183105,
      "learning_rate": 9.796641222727043e-06,
      "loss": 2.0199,
      "step": 2308
    },
    {
      "epoch": 1.5200789993416721,
      "grad_norm": 1.1792136430740356,
      "learning_rate": 9.789748704148557e-06,
      "loss": 1.574,
      "step": 2309
    },
    {
      "epoch": 1.52073732718894,
      "grad_norm": 12.38329029083252,
      "learning_rate": 9.782856285496494e-06,
      "loss": 2.0974,
      "step": 2310
    },
    {
      "epoch": 1.521395655036208,
      "grad_norm": 19.926950454711914,
      "learning_rate": 9.77596397004663e-06,
      "loss": 2.1017,
      "step": 2311
    },
    {
      "epoch": 1.522053982883476,
      "grad_norm": 20.46682357788086,
      "learning_rate": 9.769071761074678e-06,
      "loss": 2.0498,
      "step": 2312
    },
    {
      "epoch": 1.5227123107307439,
      "grad_norm": 5.8707404136657715,
      "learning_rate": 9.762179661856312e-06,
      "loss": 1.6201,
      "step": 2313
    },
    {
      "epoch": 1.5233706385780117,
      "grad_norm": 6.86793327331543,
      "learning_rate": 9.75528767566715e-06,
      "loss": 1.7381,
      "step": 2314
    },
    {
      "epoch": 1.5240289664252797,
      "grad_norm": 19.135622024536133,
      "learning_rate": 9.748395805782752e-06,
      "loss": 1.8543,
      "step": 2315
    },
    {
      "epoch": 1.5246872942725478,
      "grad_norm": 19.209857940673828,
      "learning_rate": 9.74150405547863e-06,
      "loss": 2.1261,
      "step": 2316
    },
    {
      "epoch": 1.5253456221198156,
      "grad_norm": 22.664133071899414,
      "learning_rate": 9.734612428030234e-06,
      "loss": 2.0383,
      "step": 2317
    },
    {
      "epoch": 1.5260039499670837,
      "grad_norm": 29.925006866455078,
      "learning_rate": 9.727720926712959e-06,
      "loss": 3.1162,
      "step": 2318
    },
    {
      "epoch": 1.5266622778143515,
      "grad_norm": 13.055654525756836,
      "learning_rate": 9.720829554802136e-06,
      "loss": 1.8788,
      "step": 2319
    },
    {
      "epoch": 1.5273206056616195,
      "grad_norm": 2.0216145515441895,
      "learning_rate": 9.713938315573039e-06,
      "loss": 1.6095,
      "step": 2320
    },
    {
      "epoch": 1.5279789335088876,
      "grad_norm": 12.729220390319824,
      "learning_rate": 9.707047212300876e-06,
      "loss": 2.2159,
      "step": 2321
    },
    {
      "epoch": 1.5286372613561554,
      "grad_norm": 27.101266860961914,
      "learning_rate": 9.700156248260792e-06,
      "loss": 2.1891,
      "step": 2322
    },
    {
      "epoch": 1.5292955892034232,
      "grad_norm": 1.3708654642105103,
      "learning_rate": 9.693265426727862e-06,
      "loss": 1.5756,
      "step": 2323
    },
    {
      "epoch": 1.5299539170506913,
      "grad_norm": 21.71125602722168,
      "learning_rate": 9.686374750977104e-06,
      "loss": 2.2846,
      "step": 2324
    },
    {
      "epoch": 1.5306122448979593,
      "grad_norm": 24.999189376831055,
      "learning_rate": 9.67948422428345e-06,
      "loss": 2.2927,
      "step": 2325
    },
    {
      "epoch": 1.5312705727452272,
      "grad_norm": 11.031952857971191,
      "learning_rate": 9.672593849921772e-06,
      "loss": 2.3099,
      "step": 2326
    },
    {
      "epoch": 1.531928900592495,
      "grad_norm": 20.256317138671875,
      "learning_rate": 9.665703631166876e-06,
      "loss": 2.3498,
      "step": 2327
    },
    {
      "epoch": 1.532587228439763,
      "grad_norm": 16.89726448059082,
      "learning_rate": 9.658813571293474e-06,
      "loss": 2.1107,
      "step": 2328
    },
    {
      "epoch": 1.533245556287031,
      "grad_norm": 31.626617431640625,
      "learning_rate": 9.65192367357622e-06,
      "loss": 3.2295,
      "step": 2329
    },
    {
      "epoch": 1.533903884134299,
      "grad_norm": 9.120450973510742,
      "learning_rate": 9.645033941289691e-06,
      "loss": 1.9727,
      "step": 2330
    },
    {
      "epoch": 1.5345622119815667,
      "grad_norm": 4.523958206176758,
      "learning_rate": 9.638144377708366e-06,
      "loss": 1.6766,
      "step": 2331
    },
    {
      "epoch": 1.5352205398288348,
      "grad_norm": 1.7179033756256104,
      "learning_rate": 9.63125498610667e-06,
      "loss": 1.5807,
      "step": 2332
    },
    {
      "epoch": 1.5358788676761028,
      "grad_norm": 32.93995666503906,
      "learning_rate": 9.624365769758933e-06,
      "loss": 3.8583,
      "step": 2333
    },
    {
      "epoch": 1.5365371955233706,
      "grad_norm": 3.0421948432922363,
      "learning_rate": 9.617476731939391e-06,
      "loss": 1.5833,
      "step": 2334
    },
    {
      "epoch": 1.5371955233706385,
      "grad_norm": 5.265535354614258,
      "learning_rate": 9.610587875922214e-06,
      "loss": 1.7051,
      "step": 2335
    },
    {
      "epoch": 1.5378538512179065,
      "grad_norm": 10.49859619140625,
      "learning_rate": 9.603699204981485e-06,
      "loss": 2.0066,
      "step": 2336
    },
    {
      "epoch": 1.5385121790651746,
      "grad_norm": 6.789299488067627,
      "learning_rate": 9.59681072239118e-06,
      "loss": 1.8378,
      "step": 2337
    },
    {
      "epoch": 1.5391705069124424,
      "grad_norm": 21.102855682373047,
      "learning_rate": 9.589922431425203e-06,
      "loss": 2.2932,
      "step": 2338
    },
    {
      "epoch": 1.5398288347597102,
      "grad_norm": 25.28767967224121,
      "learning_rate": 9.583034335357365e-06,
      "loss": 2.3202,
      "step": 2339
    },
    {
      "epoch": 1.5404871626069783,
      "grad_norm": 1.2603427171707153,
      "learning_rate": 9.576146437461374e-06,
      "loss": 1.5692,
      "step": 2340
    },
    {
      "epoch": 1.5411454904542463,
      "grad_norm": 5.657652854919434,
      "learning_rate": 9.569258741010857e-06,
      "loss": 1.6367,
      "step": 2341
    },
    {
      "epoch": 1.5418038183015141,
      "grad_norm": 1.9106040000915527,
      "learning_rate": 9.562371249279336e-06,
      "loss": 1.5816,
      "step": 2342
    },
    {
      "epoch": 1.542462146148782,
      "grad_norm": 10.21791934967041,
      "learning_rate": 9.555483965540238e-06,
      "loss": 1.8105,
      "step": 2343
    },
    {
      "epoch": 1.54312047399605,
      "grad_norm": 8.80219841003418,
      "learning_rate": 9.548596893066894e-06,
      "loss": 1.9107,
      "step": 2344
    },
    {
      "epoch": 1.543778801843318,
      "grad_norm": 1.6521490812301636,
      "learning_rate": 9.541710035132532e-06,
      "loss": 1.5839,
      "step": 2345
    },
    {
      "epoch": 1.5444371296905859,
      "grad_norm": 3.1552369594573975,
      "learning_rate": 9.534823395010278e-06,
      "loss": 1.6101,
      "step": 2346
    },
    {
      "epoch": 1.5450954575378537,
      "grad_norm": 9.01061725616455,
      "learning_rate": 9.527936975973155e-06,
      "loss": 1.9183,
      "step": 2347
    },
    {
      "epoch": 1.5457537853851218,
      "grad_norm": 16.580089569091797,
      "learning_rate": 9.521050781294082e-06,
      "loss": 2.0679,
      "step": 2348
    },
    {
      "epoch": 1.5464121132323898,
      "grad_norm": 25.21286392211914,
      "learning_rate": 9.514164814245868e-06,
      "loss": 2.6508,
      "step": 2349
    },
    {
      "epoch": 1.5470704410796576,
      "grad_norm": 11.9378662109375,
      "learning_rate": 9.507279078101219e-06,
      "loss": 1.7754,
      "step": 2350
    },
    {
      "epoch": 1.5477287689269255,
      "grad_norm": 4.858908653259277,
      "learning_rate": 9.500393576132728e-06,
      "loss": 1.6205,
      "step": 2351
    },
    {
      "epoch": 1.5483870967741935,
      "grad_norm": 7.7163214683532715,
      "learning_rate": 9.493508311612874e-06,
      "loss": 1.7621,
      "step": 2352
    },
    {
      "epoch": 1.5490454246214616,
      "grad_norm": 14.534443855285645,
      "learning_rate": 9.486623287814028e-06,
      "loss": 1.8734,
      "step": 2353
    },
    {
      "epoch": 1.5497037524687294,
      "grad_norm": 27.506919860839844,
      "learning_rate": 9.47973850800845e-06,
      "loss": 2.1519,
      "step": 2354
    },
    {
      "epoch": 1.5503620803159972,
      "grad_norm": 9.827217102050781,
      "learning_rate": 9.472853975468269e-06,
      "loss": 1.9212,
      "step": 2355
    },
    {
      "epoch": 1.5510204081632653,
      "grad_norm": 1.6895790100097656,
      "learning_rate": 9.46596969346551e-06,
      "loss": 1.5858,
      "step": 2356
    },
    {
      "epoch": 1.5516787360105333,
      "grad_norm": 16.920927047729492,
      "learning_rate": 9.45908566527208e-06,
      "loss": 2.1276,
      "step": 2357
    },
    {
      "epoch": 1.5523370638578013,
      "grad_norm": 6.187242031097412,
      "learning_rate": 9.452201894159758e-06,
      "loss": 1.8922,
      "step": 2358
    },
    {
      "epoch": 1.5529953917050692,
      "grad_norm": 13.370992660522461,
      "learning_rate": 9.4453183834002e-06,
      "loss": 1.8082,
      "step": 2359
    },
    {
      "epoch": 1.553653719552337,
      "grad_norm": 20.819456100463867,
      "learning_rate": 9.438435136264947e-06,
      "loss": 2.6959,
      "step": 2360
    },
    {
      "epoch": 1.554312047399605,
      "grad_norm": 1.7750730514526367,
      "learning_rate": 9.431552156025407e-06,
      "loss": 1.5709,
      "step": 2361
    },
    {
      "epoch": 1.554970375246873,
      "grad_norm": 7.734354019165039,
      "learning_rate": 9.42466944595286e-06,
      "loss": 1.7275,
      "step": 2362
    },
    {
      "epoch": 1.555628703094141,
      "grad_norm": 6.931989669799805,
      "learning_rate": 9.417787009318468e-06,
      "loss": 1.8092,
      "step": 2363
    },
    {
      "epoch": 1.5562870309414087,
      "grad_norm": 5.824033260345459,
      "learning_rate": 9.410904849393255e-06,
      "loss": 1.7802,
      "step": 2364
    },
    {
      "epoch": 1.5569453587886768,
      "grad_norm": 1.63314688205719,
      "learning_rate": 9.404022969448106e-06,
      "loss": 1.5656,
      "step": 2365
    },
    {
      "epoch": 1.5576036866359448,
      "grad_norm": 10.857194900512695,
      "learning_rate": 9.397141372753791e-06,
      "loss": 1.7955,
      "step": 2366
    },
    {
      "epoch": 1.5582620144832127,
      "grad_norm": 23.00733184814453,
      "learning_rate": 9.390260062580938e-06,
      "loss": 2.4753,
      "step": 2367
    },
    {
      "epoch": 1.5589203423304805,
      "grad_norm": 13.031045913696289,
      "learning_rate": 9.383379042200027e-06,
      "loss": 2.1137,
      "step": 2368
    },
    {
      "epoch": 1.5595786701777485,
      "grad_norm": 18.756328582763672,
      "learning_rate": 9.376498314881412e-06,
      "loss": 1.828,
      "step": 2369
    },
    {
      "epoch": 1.5602369980250166,
      "grad_norm": 4.034592151641846,
      "learning_rate": 9.369617883895317e-06,
      "loss": 1.6035,
      "step": 2370
    },
    {
      "epoch": 1.5608953258722844,
      "grad_norm": 25.123798370361328,
      "learning_rate": 9.362737752511803e-06,
      "loss": 2.6114,
      "step": 2371
    },
    {
      "epoch": 1.5615536537195522,
      "grad_norm": 2.1450798511505127,
      "learning_rate": 9.355857924000799e-06,
      "loss": 1.5759,
      "step": 2372
    },
    {
      "epoch": 1.5622119815668203,
      "grad_norm": 10.252403259277344,
      "learning_rate": 9.348978401632101e-06,
      "loss": 1.7179,
      "step": 2373
    },
    {
      "epoch": 1.5628703094140883,
      "grad_norm": 3.3264074325561523,
      "learning_rate": 9.342099188675338e-06,
      "loss": 1.6257,
      "step": 2374
    },
    {
      "epoch": 1.5635286372613562,
      "grad_norm": 14.390135765075684,
      "learning_rate": 9.335220288400007e-06,
      "loss": 1.7517,
      "step": 2375
    },
    {
      "epoch": 1.564186965108624,
      "grad_norm": 4.233493804931641,
      "learning_rate": 9.328341704075455e-06,
      "loss": 1.5999,
      "step": 2376
    },
    {
      "epoch": 1.564845292955892,
      "grad_norm": 2.3528738021850586,
      "learning_rate": 9.321463438970874e-06,
      "loss": 1.5922,
      "step": 2377
    },
    {
      "epoch": 1.56550362080316,
      "grad_norm": 11.104569435119629,
      "learning_rate": 9.314585496355305e-06,
      "loss": 1.7695,
      "step": 2378
    },
    {
      "epoch": 1.566161948650428,
      "grad_norm": 22.704242706298828,
      "learning_rate": 9.307707879497643e-06,
      "loss": 2.2539,
      "step": 2379
    },
    {
      "epoch": 1.5668202764976957,
      "grad_norm": 7.059818267822266,
      "learning_rate": 9.300830591666615e-06,
      "loss": 1.9103,
      "step": 2380
    },
    {
      "epoch": 1.5674786043449638,
      "grad_norm": 1.0063972473144531,
      "learning_rate": 9.293953636130804e-06,
      "loss": 1.5645,
      "step": 2381
    },
    {
      "epoch": 1.5681369321922318,
      "grad_norm": 2.2397966384887695,
      "learning_rate": 9.28707701615863e-06,
      "loss": 1.5718,
      "step": 2382
    },
    {
      "epoch": 1.5687952600394997,
      "grad_norm": 14.629030227661133,
      "learning_rate": 9.280200735018354e-06,
      "loss": 2.1598,
      "step": 2383
    },
    {
      "epoch": 1.5694535878867675,
      "grad_norm": 11.106968879699707,
      "learning_rate": 9.27332479597807e-06,
      "loss": 1.7879,
      "step": 2384
    },
    {
      "epoch": 1.5701119157340355,
      "grad_norm": 2.567772626876831,
      "learning_rate": 9.266449202305726e-06,
      "loss": 1.5922,
      "step": 2385
    },
    {
      "epoch": 1.5707702435813036,
      "grad_norm": 20.67805290222168,
      "learning_rate": 9.259573957269086e-06,
      "loss": 2.4426,
      "step": 2386
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 1.5236899852752686,
      "learning_rate": 9.252699064135759e-06,
      "loss": 1.5745,
      "step": 2387
    },
    {
      "epoch": 1.5720868992758392,
      "grad_norm": 10.815689086914062,
      "learning_rate": 9.245824526173187e-06,
      "loss": 1.9235,
      "step": 2388
    },
    {
      "epoch": 1.5727452271231073,
      "grad_norm": 10.2808837890625,
      "learning_rate": 9.23895034664864e-06,
      "loss": 1.8131,
      "step": 2389
    },
    {
      "epoch": 1.5734035549703753,
      "grad_norm": 13.868293762207031,
      "learning_rate": 9.232076528829217e-06,
      "loss": 2.1971,
      "step": 2390
    },
    {
      "epoch": 1.5740618828176431,
      "grad_norm": 21.423852920532227,
      "learning_rate": 9.225203075981853e-06,
      "loss": 2.3399,
      "step": 2391
    },
    {
      "epoch": 1.574720210664911,
      "grad_norm": 36.927734375,
      "learning_rate": 9.218329991373295e-06,
      "loss": 2.8909,
      "step": 2392
    },
    {
      "epoch": 1.575378538512179,
      "grad_norm": 7.622129440307617,
      "learning_rate": 9.211457278270129e-06,
      "loss": 1.8583,
      "step": 2393
    },
    {
      "epoch": 1.576036866359447,
      "grad_norm": 6.229084014892578,
      "learning_rate": 9.204584939938763e-06,
      "loss": 1.6446,
      "step": 2394
    },
    {
      "epoch": 1.576695194206715,
      "grad_norm": 6.047793865203857,
      "learning_rate": 9.19771297964541e-06,
      "loss": 1.6375,
      "step": 2395
    },
    {
      "epoch": 1.577353522053983,
      "grad_norm": 13.939196586608887,
      "learning_rate": 9.190841400656126e-06,
      "loss": 1.8627,
      "step": 2396
    },
    {
      "epoch": 1.5780118499012508,
      "grad_norm": 14.241209983825684,
      "learning_rate": 9.183970206236773e-06,
      "loss": 1.8536,
      "step": 2397
    },
    {
      "epoch": 1.5786701777485188,
      "grad_norm": 5.055757999420166,
      "learning_rate": 9.17709939965304e-06,
      "loss": 1.6318,
      "step": 2398
    },
    {
      "epoch": 1.5793285055957869,
      "grad_norm": 2.705479383468628,
      "learning_rate": 9.17022898417041e-06,
      "loss": 1.5846,
      "step": 2399
    },
    {
      "epoch": 1.5799868334430547,
      "grad_norm": 8.28245735168457,
      "learning_rate": 9.163358963054206e-06,
      "loss": 1.6544,
      "step": 2400
    },
    {
      "epoch": 1.5806451612903225,
      "grad_norm": 2.0354690551757812,
      "learning_rate": 9.156489339569555e-06,
      "loss": 1.5866,
      "step": 2401
    },
    {
      "epoch": 1.5813034891375906,
      "grad_norm": 1.3223788738250732,
      "learning_rate": 9.149620116981378e-06,
      "loss": 1.5673,
      "step": 2402
    },
    {
      "epoch": 1.5819618169848586,
      "grad_norm": 2.1122512817382812,
      "learning_rate": 9.142751298554436e-06,
      "loss": 1.5846,
      "step": 2403
    },
    {
      "epoch": 1.5826201448321264,
      "grad_norm": 4.071165084838867,
      "learning_rate": 9.135882887553276e-06,
      "loss": 1.5916,
      "step": 2404
    },
    {
      "epoch": 1.5832784726793943,
      "grad_norm": 14.78763198852539,
      "learning_rate": 9.129014887242255e-06,
      "loss": 2.0128,
      "step": 2405
    },
    {
      "epoch": 1.5839368005266623,
      "grad_norm": 12.959000587463379,
      "learning_rate": 9.122147300885534e-06,
      "loss": 2.1086,
      "step": 2406
    },
    {
      "epoch": 1.5845951283739304,
      "grad_norm": 39.13937759399414,
      "learning_rate": 9.115280131747092e-06,
      "loss": 3.6754,
      "step": 2407
    },
    {
      "epoch": 1.5852534562211982,
      "grad_norm": 12.070223808288574,
      "learning_rate": 9.108413383090686e-06,
      "loss": 1.9581,
      "step": 2408
    },
    {
      "epoch": 1.585911784068466,
      "grad_norm": 13.620814323425293,
      "learning_rate": 9.101547058179892e-06,
      "loss": 2.1667,
      "step": 2409
    },
    {
      "epoch": 1.586570111915734,
      "grad_norm": 1.2900338172912598,
      "learning_rate": 9.094681160278076e-06,
      "loss": 1.5682,
      "step": 2410
    },
    {
      "epoch": 1.587228439763002,
      "grad_norm": 11.48133373260498,
      "learning_rate": 9.087815692648401e-06,
      "loss": 1.9239,
      "step": 2411
    },
    {
      "epoch": 1.58788676761027,
      "grad_norm": 1.5100126266479492,
      "learning_rate": 9.080950658553829e-06,
      "loss": 1.5655,
      "step": 2412
    },
    {
      "epoch": 1.5885450954575377,
      "grad_norm": 11.431654930114746,
      "learning_rate": 9.074086061257116e-06,
      "loss": 2.04,
      "step": 2413
    },
    {
      "epoch": 1.5892034233048058,
      "grad_norm": 1.54572594165802,
      "learning_rate": 9.067221904020805e-06,
      "loss": 1.5598,
      "step": 2414
    },
    {
      "epoch": 1.5898617511520738,
      "grad_norm": 24.16768455505371,
      "learning_rate": 9.060358190107235e-06,
      "loss": 2.2062,
      "step": 2415
    },
    {
      "epoch": 1.5905200789993417,
      "grad_norm": 10.227916717529297,
      "learning_rate": 9.053494922778536e-06,
      "loss": 1.7266,
      "step": 2416
    },
    {
      "epoch": 1.5911784068466095,
      "grad_norm": 1.1594160795211792,
      "learning_rate": 9.046632105296617e-06,
      "loss": 1.5728,
      "step": 2417
    },
    {
      "epoch": 1.5918367346938775,
      "grad_norm": 17.56338119506836,
      "learning_rate": 9.039769740923183e-06,
      "loss": 2.6298,
      "step": 2418
    },
    {
      "epoch": 1.5924950625411456,
      "grad_norm": 8.65854549407959,
      "learning_rate": 9.03290783291972e-06,
      "loss": 2.0316,
      "step": 2419
    },
    {
      "epoch": 1.5931533903884134,
      "grad_norm": 8.081019401550293,
      "learning_rate": 9.026046384547492e-06,
      "loss": 1.8713,
      "step": 2420
    },
    {
      "epoch": 1.5938117182356812,
      "grad_norm": 2.8661768436431885,
      "learning_rate": 9.019185399067553e-06,
      "loss": 1.5896,
      "step": 2421
    },
    {
      "epoch": 1.5944700460829493,
      "grad_norm": 6.615875244140625,
      "learning_rate": 9.012324879740732e-06,
      "loss": 1.7347,
      "step": 2422
    },
    {
      "epoch": 1.5951283739302173,
      "grad_norm": 24.83548927307129,
      "learning_rate": 9.005464829827637e-06,
      "loss": 3.4505,
      "step": 2423
    },
    {
      "epoch": 1.5957867017774852,
      "grad_norm": 8.053031921386719,
      "learning_rate": 8.998605252588653e-06,
      "loss": 1.8835,
      "step": 2424
    },
    {
      "epoch": 1.596445029624753,
      "grad_norm": 2.082392930984497,
      "learning_rate": 8.991746151283945e-06,
      "loss": 1.5879,
      "step": 2425
    },
    {
      "epoch": 1.597103357472021,
      "grad_norm": 1.3975286483764648,
      "learning_rate": 8.984887529173441e-06,
      "loss": 1.5795,
      "step": 2426
    },
    {
      "epoch": 1.597761685319289,
      "grad_norm": 5.086730480194092,
      "learning_rate": 8.978029389516851e-06,
      "loss": 1.6634,
      "step": 2427
    },
    {
      "epoch": 1.598420013166557,
      "grad_norm": 10.611896514892578,
      "learning_rate": 8.971171735573658e-06,
      "loss": 1.9469,
      "step": 2428
    },
    {
      "epoch": 1.5990783410138247,
      "grad_norm": 16.65001106262207,
      "learning_rate": 8.964314570603096e-06,
      "loss": 2.2651,
      "step": 2429
    },
    {
      "epoch": 1.5997366688610928,
      "grad_norm": 29.015338897705078,
      "learning_rate": 8.95745789786419e-06,
      "loss": 2.7977,
      "step": 2430
    },
    {
      "epoch": 1.6003949967083608,
      "grad_norm": 10.06486988067627,
      "learning_rate": 8.95060172061572e-06,
      "loss": 1.8762,
      "step": 2431
    },
    {
      "epoch": 1.6010533245556287,
      "grad_norm": 16.795543670654297,
      "learning_rate": 8.943746042116224e-06,
      "loss": 2.0319,
      "step": 2432
    },
    {
      "epoch": 1.6017116524028965,
      "grad_norm": 23.722320556640625,
      "learning_rate": 8.936890865624014e-06,
      "loss": 2.4394,
      "step": 2433
    },
    {
      "epoch": 1.6023699802501645,
      "grad_norm": 2.6074206829071045,
      "learning_rate": 8.930036194397164e-06,
      "loss": 1.584,
      "step": 2434
    },
    {
      "epoch": 1.6030283080974326,
      "grad_norm": 1.1502512693405151,
      "learning_rate": 8.923182031693495e-06,
      "loss": 1.5557,
      "step": 2435
    },
    {
      "epoch": 1.6036866359447006,
      "grad_norm": 9.821525573730469,
      "learning_rate": 8.916328380770595e-06,
      "loss": 1.6028,
      "step": 2436
    },
    {
      "epoch": 1.6043449637919684,
      "grad_norm": 29.531217575073242,
      "learning_rate": 8.909475244885812e-06,
      "loss": 3.5927,
      "step": 2437
    },
    {
      "epoch": 1.6050032916392363,
      "grad_norm": 1.2367939949035645,
      "learning_rate": 8.902622627296248e-06,
      "loss": 1.5741,
      "step": 2438
    },
    {
      "epoch": 1.6056616194865043,
      "grad_norm": 4.6805877685546875,
      "learning_rate": 8.895770531258751e-06,
      "loss": 1.5962,
      "step": 2439
    },
    {
      "epoch": 1.6063199473337724,
      "grad_norm": 13.308212280273438,
      "learning_rate": 8.888918960029924e-06,
      "loss": 1.9014,
      "step": 2440
    },
    {
      "epoch": 1.6069782751810402,
      "grad_norm": 1.9887276887893677,
      "learning_rate": 8.882067916866132e-06,
      "loss": 1.5899,
      "step": 2441
    },
    {
      "epoch": 1.607636603028308,
      "grad_norm": 11.522814750671387,
      "learning_rate": 8.87521740502347e-06,
      "loss": 1.8185,
      "step": 2442
    },
    {
      "epoch": 1.608294930875576,
      "grad_norm": 11.442852973937988,
      "learning_rate": 8.868367427757792e-06,
      "loss": 2.079,
      "step": 2443
    },
    {
      "epoch": 1.6089532587228441,
      "grad_norm": 21.759103775024414,
      "learning_rate": 8.861517988324705e-06,
      "loss": 2.0565,
      "step": 2444
    },
    {
      "epoch": 1.609611586570112,
      "grad_norm": 9.001713752746582,
      "learning_rate": 8.854669089979543e-06,
      "loss": 1.9059,
      "step": 2445
    },
    {
      "epoch": 1.6102699144173798,
      "grad_norm": 2.5531415939331055,
      "learning_rate": 8.847820735977392e-06,
      "loss": 1.5924,
      "step": 2446
    },
    {
      "epoch": 1.6109282422646478,
      "grad_norm": 7.591753959655762,
      "learning_rate": 8.840972929573085e-06,
      "loss": 1.8587,
      "step": 2447
    },
    {
      "epoch": 1.6115865701119159,
      "grad_norm": 18.72543716430664,
      "learning_rate": 8.834125674021178e-06,
      "loss": 2.1439,
      "step": 2448
    },
    {
      "epoch": 1.6122448979591837,
      "grad_norm": 1.4006727933883667,
      "learning_rate": 8.827278972575984e-06,
      "loss": 1.5736,
      "step": 2449
    },
    {
      "epoch": 1.6129032258064515,
      "grad_norm": 14.855216979980469,
      "learning_rate": 8.820432828491542e-06,
      "loss": 1.938,
      "step": 2450
    },
    {
      "epoch": 1.6135615536537196,
      "grad_norm": 1.3817142248153687,
      "learning_rate": 8.813587245021628e-06,
      "loss": 1.5647,
      "step": 2451
    },
    {
      "epoch": 1.6142198815009876,
      "grad_norm": 13.064002990722656,
      "learning_rate": 8.806742225419751e-06,
      "loss": 2.1065,
      "step": 2452
    },
    {
      "epoch": 1.6148782093482554,
      "grad_norm": 0.8590745329856873,
      "learning_rate": 8.799897772939157e-06,
      "loss": 1.5544,
      "step": 2453
    },
    {
      "epoch": 1.6155365371955233,
      "grad_norm": 8.56078052520752,
      "learning_rate": 8.793053890832813e-06,
      "loss": 1.666,
      "step": 2454
    },
    {
      "epoch": 1.6161948650427913,
      "grad_norm": 3.413335084915161,
      "learning_rate": 8.786210582353423e-06,
      "loss": 1.5747,
      "step": 2455
    },
    {
      "epoch": 1.6168531928900594,
      "grad_norm": 19.939022064208984,
      "learning_rate": 8.779367850753418e-06,
      "loss": 2.2632,
      "step": 2456
    },
    {
      "epoch": 1.6175115207373272,
      "grad_norm": 22.5186710357666,
      "learning_rate": 8.772525699284947e-06,
      "loss": 2.41,
      "step": 2457
    },
    {
      "epoch": 1.618169848584595,
      "grad_norm": 0.7978677153587341,
      "learning_rate": 8.765684131199894e-06,
      "loss": 1.5497,
      "step": 2458
    },
    {
      "epoch": 1.618828176431863,
      "grad_norm": 15.153861999511719,
      "learning_rate": 8.75884314974986e-06,
      "loss": 1.9519,
      "step": 2459
    },
    {
      "epoch": 1.619486504279131,
      "grad_norm": 7.827334403991699,
      "learning_rate": 8.752002758186163e-06,
      "loss": 1.8927,
      "step": 2460
    },
    {
      "epoch": 1.620144832126399,
      "grad_norm": 25.05408477783203,
      "learning_rate": 8.745162959759851e-06,
      "loss": 2.3485,
      "step": 2461
    },
    {
      "epoch": 1.6208031599736668,
      "grad_norm": 27.29457664489746,
      "learning_rate": 8.738323757721683e-06,
      "loss": 2.1509,
      "step": 2462
    },
    {
      "epoch": 1.6214614878209348,
      "grad_norm": 17.98583221435547,
      "learning_rate": 8.731485155322134e-06,
      "loss": 2.1877,
      "step": 2463
    },
    {
      "epoch": 1.6221198156682028,
      "grad_norm": 7.2516398429870605,
      "learning_rate": 8.724647155811397e-06,
      "loss": 1.6911,
      "step": 2464
    },
    {
      "epoch": 1.6227781435154707,
      "grad_norm": 16.961641311645508,
      "learning_rate": 8.717809762439382e-06,
      "loss": 2.1605,
      "step": 2465
    },
    {
      "epoch": 1.6234364713627385,
      "grad_norm": 26.87907600402832,
      "learning_rate": 8.710972978455696e-06,
      "loss": 2.4027,
      "step": 2466
    },
    {
      "epoch": 1.6240947992100065,
      "grad_norm": 10.727408409118652,
      "learning_rate": 8.704136807109676e-06,
      "loss": 1.9575,
      "step": 2467
    },
    {
      "epoch": 1.6247531270572746,
      "grad_norm": 10.136686325073242,
      "learning_rate": 8.69730125165036e-06,
      "loss": 1.9028,
      "step": 2468
    },
    {
      "epoch": 1.6254114549045424,
      "grad_norm": 1.1018913984298706,
      "learning_rate": 8.690466315326483e-06,
      "loss": 1.5602,
      "step": 2469
    },
    {
      "epoch": 1.6260697827518102,
      "grad_norm": 1.9566326141357422,
      "learning_rate": 8.683632001386497e-06,
      "loss": 1.6276,
      "step": 2470
    },
    {
      "epoch": 1.6267281105990783,
      "grad_norm": 10.69311237335205,
      "learning_rate": 8.676798313078565e-06,
      "loss": 1.9538,
      "step": 2471
    },
    {
      "epoch": 1.6273864384463463,
      "grad_norm": 12.474327087402344,
      "learning_rate": 8.669965253650534e-06,
      "loss": 1.9474,
      "step": 2472
    },
    {
      "epoch": 1.6280447662936142,
      "grad_norm": 11.88668441772461,
      "learning_rate": 8.663132826349962e-06,
      "loss": 1.905,
      "step": 2473
    },
    {
      "epoch": 1.6287030941408822,
      "grad_norm": 17.06386375427246,
      "learning_rate": 8.65630103442411e-06,
      "loss": 2.4291,
      "step": 2474
    },
    {
      "epoch": 1.62936142198815,
      "grad_norm": 9.700840950012207,
      "learning_rate": 8.649469881119936e-06,
      "loss": 1.703,
      "step": 2475
    },
    {
      "epoch": 1.630019749835418,
      "grad_norm": 1.1682770252227783,
      "learning_rate": 8.642639369684085e-06,
      "loss": 1.5602,
      "step": 2476
    },
    {
      "epoch": 1.6306780776826861,
      "grad_norm": 2.404909610748291,
      "learning_rate": 8.635809503362903e-06,
      "loss": 1.5779,
      "step": 2477
    },
    {
      "epoch": 1.631336405529954,
      "grad_norm": 12.744278907775879,
      "learning_rate": 8.62898028540244e-06,
      "loss": 1.8913,
      "step": 2478
    },
    {
      "epoch": 1.6319947333772218,
      "grad_norm": 8.384578704833984,
      "learning_rate": 8.622151719048417e-06,
      "loss": 1.672,
      "step": 2479
    },
    {
      "epoch": 1.6326530612244898,
      "grad_norm": 15.633485794067383,
      "learning_rate": 8.615323807546258e-06,
      "loss": 2.0956,
      "step": 2480
    },
    {
      "epoch": 1.6333113890717579,
      "grad_norm": 17.180307388305664,
      "learning_rate": 8.608496554141085e-06,
      "loss": 2.0647,
      "step": 2481
    },
    {
      "epoch": 1.6339697169190257,
      "grad_norm": 1.4318718910217285,
      "learning_rate": 8.601669962077682e-06,
      "loss": 1.5654,
      "step": 2482
    },
    {
      "epoch": 1.6346280447662935,
      "grad_norm": 5.098809242248535,
      "learning_rate": 8.594844034600538e-06,
      "loss": 1.6793,
      "step": 2483
    },
    {
      "epoch": 1.6352863726135616,
      "grad_norm": 24.773014068603516,
      "learning_rate": 8.588018774953824e-06,
      "loss": 2.6427,
      "step": 2484
    },
    {
      "epoch": 1.6359447004608296,
      "grad_norm": 2.2869532108306885,
      "learning_rate": 8.581194186381388e-06,
      "loss": 1.5702,
      "step": 2485
    },
    {
      "epoch": 1.6366030283080975,
      "grad_norm": 21.82651710510254,
      "learning_rate": 8.574370272126761e-06,
      "loss": 2.4753,
      "step": 2486
    },
    {
      "epoch": 1.6372613561553653,
      "grad_norm": 2.0188465118408203,
      "learning_rate": 8.567547035433158e-06,
      "loss": 1.5698,
      "step": 2487
    },
    {
      "epoch": 1.6379196840026333,
      "grad_norm": 4.694016933441162,
      "learning_rate": 8.560724479543461e-06,
      "loss": 1.6097,
      "step": 2488
    },
    {
      "epoch": 1.6385780118499014,
      "grad_norm": 27.26450538635254,
      "learning_rate": 8.553902607700242e-06,
      "loss": 2.1892,
      "step": 2489
    },
    {
      "epoch": 1.6392363396971692,
      "grad_norm": 0.9503549933433533,
      "learning_rate": 8.54708142314574e-06,
      "loss": 1.5628,
      "step": 2490
    },
    {
      "epoch": 1.639894667544437,
      "grad_norm": 2.328996419906616,
      "learning_rate": 8.540260929121867e-06,
      "loss": 1.5705,
      "step": 2491
    },
    {
      "epoch": 1.640552995391705,
      "grad_norm": 2.7639033794403076,
      "learning_rate": 8.53344112887021e-06,
      "loss": 1.5781,
      "step": 2492
    },
    {
      "epoch": 1.6412113232389731,
      "grad_norm": 9.179645538330078,
      "learning_rate": 8.52662202563203e-06,
      "loss": 1.865,
      "step": 2493
    },
    {
      "epoch": 1.641869651086241,
      "grad_norm": 12.270637512207031,
      "learning_rate": 8.51980362264824e-06,
      "loss": 1.7729,
      "step": 2494
    },
    {
      "epoch": 1.6425279789335088,
      "grad_norm": 6.843016624450684,
      "learning_rate": 8.51298592315944e-06,
      "loss": 1.8287,
      "step": 2495
    },
    {
      "epoch": 1.6431863067807768,
      "grad_norm": 3.9947476387023926,
      "learning_rate": 8.50616893040589e-06,
      "loss": 1.6022,
      "step": 2496
    },
    {
      "epoch": 1.6438446346280449,
      "grad_norm": 0.642562985420227,
      "learning_rate": 8.499352647627504e-06,
      "loss": 1.5438,
      "step": 2497
    },
    {
      "epoch": 1.6445029624753127,
      "grad_norm": 14.065369606018066,
      "learning_rate": 8.49253707806387e-06,
      "loss": 1.9587,
      "step": 2498
    },
    {
      "epoch": 1.6451612903225805,
      "grad_norm": 1.6171315908432007,
      "learning_rate": 8.485722224954237e-06,
      "loss": 1.5697,
      "step": 2499
    },
    {
      "epoch": 1.6458196181698486,
      "grad_norm": 1.3502596616744995,
      "learning_rate": 8.4789080915375e-06,
      "loss": 1.5573,
      "step": 2500
    },
    {
      "epoch": 1.6464779460171166,
      "grad_norm": 12.963980674743652,
      "learning_rate": 8.472094681052228e-06,
      "loss": 2.0272,
      "step": 2501
    },
    {
      "epoch": 1.6471362738643844,
      "grad_norm": 5.619535446166992,
      "learning_rate": 8.465281996736645e-06,
      "loss": 1.6903,
      "step": 2502
    },
    {
      "epoch": 1.6477946017116523,
      "grad_norm": 15.323017120361328,
      "learning_rate": 8.458470041828613e-06,
      "loss": 2.255,
      "step": 2503
    },
    {
      "epoch": 1.6484529295589203,
      "grad_norm": 5.346390247344971,
      "learning_rate": 8.451658819565669e-06,
      "loss": 1.7866,
      "step": 2504
    },
    {
      "epoch": 1.6491112574061884,
      "grad_norm": 8.912522315979004,
      "learning_rate": 8.444848333184991e-06,
      "loss": 1.7366,
      "step": 2505
    },
    {
      "epoch": 1.6497695852534562,
      "grad_norm": 1.3739312887191772,
      "learning_rate": 8.438038585923403e-06,
      "loss": 1.5613,
      "step": 2506
    },
    {
      "epoch": 1.650427913100724,
      "grad_norm": 1.089361548423767,
      "learning_rate": 8.431229581017382e-06,
      "loss": 1.5646,
      "step": 2507
    },
    {
      "epoch": 1.651086240947992,
      "grad_norm": 39.90925979614258,
      "learning_rate": 8.424421321703067e-06,
      "loss": 2.2703,
      "step": 2508
    },
    {
      "epoch": 1.65174456879526,
      "grad_norm": 31.699827194213867,
      "learning_rate": 8.417613811216213e-06,
      "loss": 3.2878,
      "step": 2509
    },
    {
      "epoch": 1.652402896642528,
      "grad_norm": 13.88883113861084,
      "learning_rate": 8.410807052792238e-06,
      "loss": 1.8257,
      "step": 2510
    },
    {
      "epoch": 1.6530612244897958,
      "grad_norm": 11.004709243774414,
      "learning_rate": 8.404001049666211e-06,
      "loss": 1.7521,
      "step": 2511
    },
    {
      "epoch": 1.6537195523370638,
      "grad_norm": 11.481544494628906,
      "learning_rate": 8.397195805072818e-06,
      "loss": 2.0281,
      "step": 2512
    },
    {
      "epoch": 1.6543778801843319,
      "grad_norm": 6.970668792724609,
      "learning_rate": 8.3903913222464e-06,
      "loss": 1.807,
      "step": 2513
    },
    {
      "epoch": 1.6550362080316,
      "grad_norm": 2.1778361797332764,
      "learning_rate": 8.383587604420932e-06,
      "loss": 1.5756,
      "step": 2514
    },
    {
      "epoch": 1.6556945358788677,
      "grad_norm": 19.210460662841797,
      "learning_rate": 8.376784654830037e-06,
      "loss": 2.3015,
      "step": 2515
    },
    {
      "epoch": 1.6563528637261355,
      "grad_norm": 9.894875526428223,
      "learning_rate": 8.369982476706948e-06,
      "loss": 1.9078,
      "step": 2516
    },
    {
      "epoch": 1.6570111915734036,
      "grad_norm": 6.681386947631836,
      "learning_rate": 8.363181073284551e-06,
      "loss": 1.9605,
      "step": 2517
    },
    {
      "epoch": 1.6576695194206716,
      "grad_norm": 14.481765747070312,
      "learning_rate": 8.356380447795364e-06,
      "loss": 1.9905,
      "step": 2518
    },
    {
      "epoch": 1.6583278472679395,
      "grad_norm": 1.2207365036010742,
      "learning_rate": 8.349580603471521e-06,
      "loss": 1.5676,
      "step": 2519
    },
    {
      "epoch": 1.6589861751152073,
      "grad_norm": 10.463823318481445,
      "learning_rate": 8.342781543544799e-06,
      "loss": 1.8978,
      "step": 2520
    },
    {
      "epoch": 1.6596445029624753,
      "grad_norm": 1.3096399307250977,
      "learning_rate": 8.335983271246594e-06,
      "loss": 1.5615,
      "step": 2521
    },
    {
      "epoch": 1.6603028308097434,
      "grad_norm": 13.824811935424805,
      "learning_rate": 8.32918578980793e-06,
      "loss": 1.79,
      "step": 2522
    },
    {
      "epoch": 1.6609611586570112,
      "grad_norm": 1.004926323890686,
      "learning_rate": 8.322389102459458e-06,
      "loss": 1.5473,
      "step": 2523
    },
    {
      "epoch": 1.661619486504279,
      "grad_norm": 16.707542419433594,
      "learning_rate": 8.31559321243145e-06,
      "loss": 2.2243,
      "step": 2524
    },
    {
      "epoch": 1.662277814351547,
      "grad_norm": 18.660982131958008,
      "learning_rate": 8.308798122953794e-06,
      "loss": 2.1575,
      "step": 2525
    },
    {
      "epoch": 1.6629361421988151,
      "grad_norm": 17.183401107788086,
      "learning_rate": 8.302003837256003e-06,
      "loss": 2.181,
      "step": 2526
    },
    {
      "epoch": 1.663594470046083,
      "grad_norm": 14.812411308288574,
      "learning_rate": 8.295210358567207e-06,
      "loss": 1.9411,
      "step": 2527
    },
    {
      "epoch": 1.6642527978933508,
      "grad_norm": 24.90526008605957,
      "learning_rate": 8.288417690116153e-06,
      "loss": 2.3269,
      "step": 2528
    },
    {
      "epoch": 1.6649111257406188,
      "grad_norm": 10.311063766479492,
      "learning_rate": 8.2816258351312e-06,
      "loss": 1.9186,
      "step": 2529
    },
    {
      "epoch": 1.6655694535878869,
      "grad_norm": 9.337321281433105,
      "learning_rate": 8.274834796840325e-06,
      "loss": 1.8779,
      "step": 2530
    },
    {
      "epoch": 1.6662277814351547,
      "grad_norm": 33.08439636230469,
      "learning_rate": 8.268044578471112e-06,
      "loss": 2.6991,
      "step": 2531
    },
    {
      "epoch": 1.6668861092824225,
      "grad_norm": 22.784305572509766,
      "learning_rate": 8.261255183250755e-06,
      "loss": 2.3447,
      "step": 2532
    },
    {
      "epoch": 1.6675444371296906,
      "grad_norm": 7.147454261779785,
      "learning_rate": 8.254466614406067e-06,
      "loss": 1.8341,
      "step": 2533
    },
    {
      "epoch": 1.6682027649769586,
      "grad_norm": 21.53377914428711,
      "learning_rate": 8.247678875163456e-06,
      "loss": 2.5619,
      "step": 2534
    },
    {
      "epoch": 1.6688610928242265,
      "grad_norm": 14.892061233520508,
      "learning_rate": 8.240891968748937e-06,
      "loss": 2.0033,
      "step": 2535
    },
    {
      "epoch": 1.6695194206714943,
      "grad_norm": 20.19422149658203,
      "learning_rate": 8.234105898388141e-06,
      "loss": 1.9056,
      "step": 2536
    },
    {
      "epoch": 1.6701777485187623,
      "grad_norm": 14.953591346740723,
      "learning_rate": 8.227320667306283e-06,
      "loss": 2.0763,
      "step": 2537
    },
    {
      "epoch": 1.6708360763660304,
      "grad_norm": 12.672167778015137,
      "learning_rate": 8.220536278728194e-06,
      "loss": 2.1052,
      "step": 2538
    },
    {
      "epoch": 1.6714944042132982,
      "grad_norm": 5.292051792144775,
      "learning_rate": 8.213752735878304e-06,
      "loss": 1.8114,
      "step": 2539
    },
    {
      "epoch": 1.672152732060566,
      "grad_norm": 2.2366604804992676,
      "learning_rate": 8.206970041980629e-06,
      "loss": 1.5622,
      "step": 2540
    },
    {
      "epoch": 1.672811059907834,
      "grad_norm": 11.859838485717773,
      "learning_rate": 8.200188200258793e-06,
      "loss": 1.8776,
      "step": 2541
    },
    {
      "epoch": 1.6734693877551021,
      "grad_norm": 7.211766719818115,
      "learning_rate": 8.193407213936014e-06,
      "loss": 1.9989,
      "step": 2542
    },
    {
      "epoch": 1.67412771560237,
      "grad_norm": 17.904512405395508,
      "learning_rate": 8.186627086235096e-06,
      "loss": 1.9636,
      "step": 2543
    },
    {
      "epoch": 1.6747860434496378,
      "grad_norm": 10.888585090637207,
      "learning_rate": 8.179847820378438e-06,
      "loss": 1.9868,
      "step": 2544
    },
    {
      "epoch": 1.6754443712969058,
      "grad_norm": 2.1841862201690674,
      "learning_rate": 8.173069419588041e-06,
      "loss": 1.5867,
      "step": 2545
    },
    {
      "epoch": 1.6761026991441739,
      "grad_norm": 10.841814041137695,
      "learning_rate": 8.166291887085478e-06,
      "loss": 1.9723,
      "step": 2546
    },
    {
      "epoch": 1.6767610269914417,
      "grad_norm": 3.583401679992676,
      "learning_rate": 8.159515226091914e-06,
      "loss": 1.5945,
      "step": 2547
    },
    {
      "epoch": 1.6774193548387095,
      "grad_norm": 13.138132095336914,
      "learning_rate": 8.15273943982811e-06,
      "loss": 1.9861,
      "step": 2548
    },
    {
      "epoch": 1.6780776826859776,
      "grad_norm": 11.073062896728516,
      "learning_rate": 8.145964531514394e-06,
      "loss": 1.9099,
      "step": 2549
    },
    {
      "epoch": 1.6787360105332456,
      "grad_norm": 2.395960569381714,
      "learning_rate": 8.139190504370692e-06,
      "loss": 1.5745,
      "step": 2550
    },
    {
      "epoch": 1.6793943383805134,
      "grad_norm": 5.753378391265869,
      "learning_rate": 8.132417361616507e-06,
      "loss": 1.7867,
      "step": 2551
    },
    {
      "epoch": 1.6800526662277815,
      "grad_norm": 15.190503120422363,
      "learning_rate": 8.125645106470914e-06,
      "loss": 2.1001,
      "step": 2552
    },
    {
      "epoch": 1.6807109940750493,
      "grad_norm": 2.049422025680542,
      "learning_rate": 8.118873742152574e-06,
      "loss": 1.5546,
      "step": 2553
    },
    {
      "epoch": 1.6813693219223174,
      "grad_norm": 10.56663703918457,
      "learning_rate": 8.112103271879724e-06,
      "loss": 2.0755,
      "step": 2554
    },
    {
      "epoch": 1.6820276497695854,
      "grad_norm": 1.0854394435882568,
      "learning_rate": 8.105333698870178e-06,
      "loss": 1.5474,
      "step": 2555
    },
    {
      "epoch": 1.6826859776168532,
      "grad_norm": 16.76810646057129,
      "learning_rate": 8.098565026341313e-06,
      "loss": 2.0069,
      "step": 2556
    },
    {
      "epoch": 1.683344305464121,
      "grad_norm": 16.57796859741211,
      "learning_rate": 8.09179725751009e-06,
      "loss": 2.3044,
      "step": 2557
    },
    {
      "epoch": 1.684002633311389,
      "grad_norm": 21.420886993408203,
      "learning_rate": 8.085030395593038e-06,
      "loss": 2.2236,
      "step": 2558
    },
    {
      "epoch": 1.6846609611586572,
      "grad_norm": 9.922325134277344,
      "learning_rate": 8.07826444380625e-06,
      "loss": 1.8279,
      "step": 2559
    },
    {
      "epoch": 1.685319289005925,
      "grad_norm": 27.845569610595703,
      "learning_rate": 8.071499405365385e-06,
      "loss": 2.1895,
      "step": 2560
    },
    {
      "epoch": 1.6859776168531928,
      "grad_norm": 2.8122880458831787,
      "learning_rate": 8.064735283485683e-06,
      "loss": 1.6122,
      "step": 2561
    },
    {
      "epoch": 1.6866359447004609,
      "grad_norm": 15.733349800109863,
      "learning_rate": 8.057972081381926e-06,
      "loss": 2.1213,
      "step": 2562
    },
    {
      "epoch": 1.687294272547729,
      "grad_norm": 10.201196670532227,
      "learning_rate": 8.051209802268477e-06,
      "loss": 1.9912,
      "step": 2563
    },
    {
      "epoch": 1.6879526003949967,
      "grad_norm": 6.367223739624023,
      "learning_rate": 8.044448449359254e-06,
      "loss": 1.8358,
      "step": 2564
    },
    {
      "epoch": 1.6886109282422646,
      "grad_norm": 1.62654709815979,
      "learning_rate": 8.037688025867732e-06,
      "loss": 1.5606,
      "step": 2565
    },
    {
      "epoch": 1.6892692560895326,
      "grad_norm": 13.33708667755127,
      "learning_rate": 8.030928535006948e-06,
      "loss": 2.018,
      "step": 2566
    },
    {
      "epoch": 1.6899275839368006,
      "grad_norm": 22.557270050048828,
      "learning_rate": 8.024169979989495e-06,
      "loss": 2.3098,
      "step": 2567
    },
    {
      "epoch": 1.6905859117840685,
      "grad_norm": 10.62355899810791,
      "learning_rate": 8.017412364027518e-06,
      "loss": 2.062,
      "step": 2568
    },
    {
      "epoch": 1.6912442396313363,
      "grad_norm": 16.040111541748047,
      "learning_rate": 8.010655690332724e-06,
      "loss": 2.1303,
      "step": 2569
    },
    {
      "epoch": 1.6919025674786043,
      "grad_norm": 22.24139404296875,
      "learning_rate": 8.003899962116366e-06,
      "loss": 2.25,
      "step": 2570
    },
    {
      "epoch": 1.6925608953258724,
      "grad_norm": 16.071863174438477,
      "learning_rate": 7.99714518258924e-06,
      "loss": 2.1716,
      "step": 2571
    },
    {
      "epoch": 1.6932192231731402,
      "grad_norm": 1.4481885433197021,
      "learning_rate": 7.990391354961705e-06,
      "loss": 1.5526,
      "step": 2572
    },
    {
      "epoch": 1.693877551020408,
      "grad_norm": 22.364200592041016,
      "learning_rate": 7.983638482443671e-06,
      "loss": 2.2,
      "step": 2573
    },
    {
      "epoch": 1.694535878867676,
      "grad_norm": 2.2287633419036865,
      "learning_rate": 7.976886568244567e-06,
      "loss": 1.6071,
      "step": 2574
    },
    {
      "epoch": 1.6951942067149441,
      "grad_norm": 9.22990894317627,
      "learning_rate": 7.970135615573398e-06,
      "loss": 2.0892,
      "step": 2575
    },
    {
      "epoch": 1.695852534562212,
      "grad_norm": 13.484087944030762,
      "learning_rate": 7.963385627638698e-06,
      "loss": 1.9606,
      "step": 2576
    },
    {
      "epoch": 1.6965108624094798,
      "grad_norm": 9.8057279586792,
      "learning_rate": 7.956636607648536e-06,
      "loss": 2.0806,
      "step": 2577
    },
    {
      "epoch": 1.6971691902567478,
      "grad_norm": 7.147702217102051,
      "learning_rate": 7.94988855881053e-06,
      "loss": 1.6183,
      "step": 2578
    },
    {
      "epoch": 1.6978275181040159,
      "grad_norm": 7.472629547119141,
      "learning_rate": 7.943141484331842e-06,
      "loss": 1.7954,
      "step": 2579
    },
    {
      "epoch": 1.6984858459512837,
      "grad_norm": 11.108895301818848,
      "learning_rate": 7.936395387419155e-06,
      "loss": 2.0067,
      "step": 2580
    },
    {
      "epoch": 1.6991441737985515,
      "grad_norm": 1.0692211389541626,
      "learning_rate": 7.929650271278698e-06,
      "loss": 1.5588,
      "step": 2581
    },
    {
      "epoch": 1.6998025016458196,
      "grad_norm": 25.87289047241211,
      "learning_rate": 7.922906139116237e-06,
      "loss": 2.3986,
      "step": 2582
    },
    {
      "epoch": 1.7004608294930876,
      "grad_norm": 13.538250923156738,
      "learning_rate": 7.916162994137056e-06,
      "loss": 2.0502,
      "step": 2583
    },
    {
      "epoch": 1.7011191573403555,
      "grad_norm": 20.015838623046875,
      "learning_rate": 7.909420839545986e-06,
      "loss": 1.8777,
      "step": 2584
    },
    {
      "epoch": 1.7017774851876233,
      "grad_norm": 8.807910919189453,
      "learning_rate": 7.902679678547378e-06,
      "loss": 1.8887,
      "step": 2585
    },
    {
      "epoch": 1.7024358130348913,
      "grad_norm": 13.643820762634277,
      "learning_rate": 7.895939514345111e-06,
      "loss": 2.0783,
      "step": 2586
    },
    {
      "epoch": 1.7030941408821594,
      "grad_norm": 25.25885009765625,
      "learning_rate": 7.889200350142594e-06,
      "loss": 2.5741,
      "step": 2587
    },
    {
      "epoch": 1.7037524687294272,
      "grad_norm": 8.73566722869873,
      "learning_rate": 7.882462189142761e-06,
      "loss": 1.9694,
      "step": 2588
    },
    {
      "epoch": 1.704410796576695,
      "grad_norm": 10.338213920593262,
      "learning_rate": 7.875725034548063e-06,
      "loss": 1.9309,
      "step": 2589
    },
    {
      "epoch": 1.705069124423963,
      "grad_norm": 21.00031852722168,
      "learning_rate": 7.868988889560479e-06,
      "loss": 2.2683,
      "step": 2590
    },
    {
      "epoch": 1.7057274522712311,
      "grad_norm": 3.4703915119171143,
      "learning_rate": 7.862253757381505e-06,
      "loss": 1.5837,
      "step": 2591
    },
    {
      "epoch": 1.7063857801184992,
      "grad_norm": 2.588268995285034,
      "learning_rate": 7.855519641212162e-06,
      "loss": 1.6321,
      "step": 2592
    },
    {
      "epoch": 1.707044107965767,
      "grad_norm": 8.574957847595215,
      "learning_rate": 7.848786544252974e-06,
      "loss": 1.9354,
      "step": 2593
    },
    {
      "epoch": 1.7077024358130348,
      "grad_norm": 3.6959011554718018,
      "learning_rate": 7.842054469703993e-06,
      "loss": 1.604,
      "step": 2594
    },
    {
      "epoch": 1.7083607636603029,
      "grad_norm": 1.944260597229004,
      "learning_rate": 7.835323420764784e-06,
      "loss": 1.5813,
      "step": 2595
    },
    {
      "epoch": 1.709019091507571,
      "grad_norm": 34.85675811767578,
      "learning_rate": 7.828593400634418e-06,
      "loss": 2.4297,
      "step": 2596
    },
    {
      "epoch": 1.7096774193548387,
      "grad_norm": 5.250584602355957,
      "learning_rate": 7.821864412511485e-06,
      "loss": 1.7684,
      "step": 2597
    },
    {
      "epoch": 1.7103357472021066,
      "grad_norm": 5.144174098968506,
      "learning_rate": 7.81513645959408e-06,
      "loss": 1.6875,
      "step": 2598
    },
    {
      "epoch": 1.7109940750493746,
      "grad_norm": 7.362793445587158,
      "learning_rate": 7.8084095450798e-06,
      "loss": 1.8083,
      "step": 2599
    },
    {
      "epoch": 1.7116524028966427,
      "grad_norm": 9.065020561218262,
      "learning_rate": 7.801683672165764e-06,
      "loss": 1.6401,
      "step": 2600
    },
    {
      "epoch": 1.7123107307439105,
      "grad_norm": 9.914477348327637,
      "learning_rate": 7.794958844048582e-06,
      "loss": 1.7827,
      "step": 2601
    },
    {
      "epoch": 1.7129690585911783,
      "grad_norm": 3.9983785152435303,
      "learning_rate": 7.788235063924372e-06,
      "loss": 1.6635,
      "step": 2602
    },
    {
      "epoch": 1.7136273864384464,
      "grad_norm": 9.468786239624023,
      "learning_rate": 7.781512334988757e-06,
      "loss": 1.8879,
      "step": 2603
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 1.601731300354004,
      "learning_rate": 7.774790660436857e-06,
      "loss": 1.5712,
      "step": 2604
    },
    {
      "epoch": 1.7149440421329822,
      "grad_norm": 12.510459899902344,
      "learning_rate": 7.768070043463289e-06,
      "loss": 2.1748,
      "step": 2605
    },
    {
      "epoch": 1.71560236998025,
      "grad_norm": 32.59128952026367,
      "learning_rate": 7.761350487262174e-06,
      "loss": 2.5048,
      "step": 2606
    },
    {
      "epoch": 1.7162606978275181,
      "grad_norm": 15.87074089050293,
      "learning_rate": 7.754631995027125e-06,
      "loss": 2.366,
      "step": 2607
    },
    {
      "epoch": 1.7169190256747862,
      "grad_norm": 6.521390438079834,
      "learning_rate": 7.74791456995124e-06,
      "loss": 1.6067,
      "step": 2608
    },
    {
      "epoch": 1.717577353522054,
      "grad_norm": 8.417160034179688,
      "learning_rate": 7.74119821522713e-06,
      "loss": 1.7988,
      "step": 2609
    },
    {
      "epoch": 1.7182356813693218,
      "grad_norm": 22.61762237548828,
      "learning_rate": 7.734482934046884e-06,
      "loss": 2.1574,
      "step": 2610
    },
    {
      "epoch": 1.7188940092165899,
      "grad_norm": 1.1678054332733154,
      "learning_rate": 7.727768729602078e-06,
      "loss": 1.5584,
      "step": 2611
    },
    {
      "epoch": 1.719552337063858,
      "grad_norm": 1.9507241249084473,
      "learning_rate": 7.721055605083785e-06,
      "loss": 1.5605,
      "step": 2612
    },
    {
      "epoch": 1.7202106649111257,
      "grad_norm": 14.425836563110352,
      "learning_rate": 7.714343563682565e-06,
      "loss": 2.2854,
      "step": 2613
    },
    {
      "epoch": 1.7208689927583936,
      "grad_norm": 22.376728057861328,
      "learning_rate": 7.70763260858845e-06,
      "loss": 2.3124,
      "step": 2614
    },
    {
      "epoch": 1.7215273206056616,
      "grad_norm": 21.654842376708984,
      "learning_rate": 7.700922742990967e-06,
      "loss": 2.2622,
      "step": 2615
    },
    {
      "epoch": 1.7221856484529297,
      "grad_norm": 23.69441795349121,
      "learning_rate": 7.694213970079132e-06,
      "loss": 2.5951,
      "step": 2616
    },
    {
      "epoch": 1.7228439763001975,
      "grad_norm": 3.8649094104766846,
      "learning_rate": 7.687506293041423e-06,
      "loss": 1.5818,
      "step": 2617
    },
    {
      "epoch": 1.7235023041474653,
      "grad_norm": 1.176734209060669,
      "learning_rate": 7.68079971506581e-06,
      "loss": 1.548,
      "step": 2618
    },
    {
      "epoch": 1.7241606319947334,
      "grad_norm": 2.4397225379943848,
      "learning_rate": 7.67409423933974e-06,
      "loss": 1.5655,
      "step": 2619
    },
    {
      "epoch": 1.7248189598420014,
      "grad_norm": 11.869657516479492,
      "learning_rate": 7.667389869050131e-06,
      "loss": 1.9443,
      "step": 2620
    },
    {
      "epoch": 1.7254772876892692,
      "grad_norm": 16.713510513305664,
      "learning_rate": 7.66068660738338e-06,
      "loss": 2.0141,
      "step": 2621
    },
    {
      "epoch": 1.726135615536537,
      "grad_norm": 0.9820581078529358,
      "learning_rate": 7.653984457525356e-06,
      "loss": 1.5501,
      "step": 2622
    },
    {
      "epoch": 1.726793943383805,
      "grad_norm": 3.505063533782959,
      "learning_rate": 7.647283422661396e-06,
      "loss": 1.5895,
      "step": 2623
    },
    {
      "epoch": 1.7274522712310731,
      "grad_norm": 9.725358963012695,
      "learning_rate": 7.640583505976311e-06,
      "loss": 1.8697,
      "step": 2624
    },
    {
      "epoch": 1.728110599078341,
      "grad_norm": 29.542945861816406,
      "learning_rate": 7.633884710654383e-06,
      "loss": 2.1582,
      "step": 2625
    },
    {
      "epoch": 1.7287689269256088,
      "grad_norm": 49.72483444213867,
      "learning_rate": 7.6271870398793546e-06,
      "loss": 2.6891,
      "step": 2626
    },
    {
      "epoch": 1.7294272547728768,
      "grad_norm": 9.633712768554688,
      "learning_rate": 7.620490496834437e-06,
      "loss": 1.928,
      "step": 2627
    },
    {
      "epoch": 1.730085582620145,
      "grad_norm": 5.741957664489746,
      "learning_rate": 7.613795084702309e-06,
      "loss": 1.7554,
      "step": 2628
    },
    {
      "epoch": 1.7307439104674127,
      "grad_norm": 31.596115112304688,
      "learning_rate": 7.607100806665104e-06,
      "loss": 2.9675,
      "step": 2629
    },
    {
      "epoch": 1.7314022383146808,
      "grad_norm": 1.2381112575531006,
      "learning_rate": 7.600407665904422e-06,
      "loss": 1.5505,
      "step": 2630
    },
    {
      "epoch": 1.7320605661619486,
      "grad_norm": 14.872258186340332,
      "learning_rate": 7.593715665601323e-06,
      "loss": 2.1786,
      "step": 2631
    },
    {
      "epoch": 1.7327188940092166,
      "grad_norm": 6.287702560424805,
      "learning_rate": 7.587024808936324e-06,
      "loss": 1.7924,
      "step": 2632
    },
    {
      "epoch": 1.7333772218564847,
      "grad_norm": 24.93880271911621,
      "learning_rate": 7.580335099089395e-06,
      "loss": 2.6173,
      "step": 2633
    },
    {
      "epoch": 1.7340355497037525,
      "grad_norm": 9.276853561401367,
      "learning_rate": 7.573646539239964e-06,
      "loss": 1.705,
      "step": 2634
    },
    {
      "epoch": 1.7346938775510203,
      "grad_norm": 29.2106876373291,
      "learning_rate": 7.566959132566914e-06,
      "loss": 2.3232,
      "step": 2635
    },
    {
      "epoch": 1.7353522053982884,
      "grad_norm": 7.48026180267334,
      "learning_rate": 7.560272882248576e-06,
      "loss": 1.8247,
      "step": 2636
    },
    {
      "epoch": 1.7360105332455564,
      "grad_norm": 1.702236294746399,
      "learning_rate": 7.553587791462735e-06,
      "loss": 1.5531,
      "step": 2637
    },
    {
      "epoch": 1.7366688610928243,
      "grad_norm": 5.2191481590271,
      "learning_rate": 7.546903863386625e-06,
      "loss": 1.7407,
      "step": 2638
    },
    {
      "epoch": 1.737327188940092,
      "grad_norm": 1.4053339958190918,
      "learning_rate": 7.5402211011969195e-06,
      "loss": 1.5524,
      "step": 2639
    },
    {
      "epoch": 1.7379855167873601,
      "grad_norm": 23.643325805664062,
      "learning_rate": 7.5335395080697495e-06,
      "loss": 2.1184,
      "step": 2640
    },
    {
      "epoch": 1.7386438446346282,
      "grad_norm": 32.12313461303711,
      "learning_rate": 7.526859087180687e-06,
      "loss": 1.9,
      "step": 2641
    },
    {
      "epoch": 1.739302172481896,
      "grad_norm": 10.887393951416016,
      "learning_rate": 7.520179841704738e-06,
      "loss": 1.9286,
      "step": 2642
    },
    {
      "epoch": 1.7399605003291638,
      "grad_norm": 11.981268882751465,
      "learning_rate": 7.513501774816362e-06,
      "loss": 1.9743,
      "step": 2643
    },
    {
      "epoch": 1.7406188281764319,
      "grad_norm": 10.447107315063477,
      "learning_rate": 7.506824889689455e-06,
      "loss": 2.02,
      "step": 2644
    },
    {
      "epoch": 1.7412771560237,
      "grad_norm": 18.293922424316406,
      "learning_rate": 7.50014918949734e-06,
      "loss": 2.1298,
      "step": 2645
    },
    {
      "epoch": 1.7419354838709677,
      "grad_norm": 24.18984603881836,
      "learning_rate": 7.493474677412795e-06,
      "loss": 2.5567,
      "step": 2646
    },
    {
      "epoch": 1.7425938117182356,
      "grad_norm": 17.019914627075195,
      "learning_rate": 7.486801356608024e-06,
      "loss": 2.2453,
      "step": 2647
    },
    {
      "epoch": 1.7432521395655036,
      "grad_norm": 1.7302467823028564,
      "learning_rate": 7.480129230254662e-06,
      "loss": 1.5507,
      "step": 2648
    },
    {
      "epoch": 1.7439104674127717,
      "grad_norm": 17.35906219482422,
      "learning_rate": 7.473458301523778e-06,
      "loss": 2.3054,
      "step": 2649
    },
    {
      "epoch": 1.7445687952600395,
      "grad_norm": 10.057038307189941,
      "learning_rate": 7.4667885735858836e-06,
      "loss": 2.0094,
      "step": 2650
    },
    {
      "epoch": 1.7452271231073073,
      "grad_norm": 13.406569480895996,
      "learning_rate": 7.4601200496109e-06,
      "loss": 1.9079,
      "step": 2651
    },
    {
      "epoch": 1.7458854509545754,
      "grad_norm": 14.429738998413086,
      "learning_rate": 7.4534527327681834e-06,
      "loss": 2.1628,
      "step": 2652
    },
    {
      "epoch": 1.7465437788018434,
      "grad_norm": 20.04559326171875,
      "learning_rate": 7.446786626226532e-06,
      "loss": 2.3447,
      "step": 2653
    },
    {
      "epoch": 1.7472021066491112,
      "grad_norm": 21.219524383544922,
      "learning_rate": 7.440121733154144e-06,
      "loss": 2.2276,
      "step": 2654
    },
    {
      "epoch": 1.747860434496379,
      "grad_norm": 18.145294189453125,
      "learning_rate": 7.433458056718656e-06,
      "loss": 2.2211,
      "step": 2655
    },
    {
      "epoch": 1.7485187623436471,
      "grad_norm": 17.677797317504883,
      "learning_rate": 7.426795600087124e-06,
      "loss": 2.1892,
      "step": 2656
    },
    {
      "epoch": 1.7491770901909152,
      "grad_norm": 8.841791152954102,
      "learning_rate": 7.420134366426021e-06,
      "loss": 2.0329,
      "step": 2657
    },
    {
      "epoch": 1.749835418038183,
      "grad_norm": 19.462757110595703,
      "learning_rate": 7.41347435890124e-06,
      "loss": 2.4593,
      "step": 2658
    },
    {
      "epoch": 1.7504937458854508,
      "grad_norm": 5.215124607086182,
      "learning_rate": 7.4068155806780975e-06,
      "loss": 1.6048,
      "step": 2659
    },
    {
      "epoch": 1.7511520737327189,
      "grad_norm": 8.888190269470215,
      "learning_rate": 7.400158034921312e-06,
      "loss": 1.8694,
      "step": 2660
    },
    {
      "epoch": 1.751810401579987,
      "grad_norm": 2.080911159515381,
      "learning_rate": 7.3935017247950306e-06,
      "loss": 1.5684,
      "step": 2661
    },
    {
      "epoch": 1.7524687294272547,
      "grad_norm": 22.060163497924805,
      "learning_rate": 7.386846653462807e-06,
      "loss": 2.0107,
      "step": 2662
    },
    {
      "epoch": 1.7531270572745226,
      "grad_norm": 13.988718032836914,
      "learning_rate": 7.380192824087602e-06,
      "loss": 2.1774,
      "step": 2663
    },
    {
      "epoch": 1.7537853851217906,
      "grad_norm": 13.65775203704834,
      "learning_rate": 7.373540239831794e-06,
      "loss": 1.9738,
      "step": 2664
    },
    {
      "epoch": 1.7544437129690587,
      "grad_norm": 1.6815282106399536,
      "learning_rate": 7.366888903857166e-06,
      "loss": 1.5595,
      "step": 2665
    },
    {
      "epoch": 1.7551020408163265,
      "grad_norm": 7.938213348388672,
      "learning_rate": 7.360238819324903e-06,
      "loss": 1.9277,
      "step": 2666
    },
    {
      "epoch": 1.7557603686635943,
      "grad_norm": 8.876556396484375,
      "learning_rate": 7.353589989395605e-06,
      "loss": 1.9115,
      "step": 2667
    },
    {
      "epoch": 1.7564186965108624,
      "grad_norm": 30.127941131591797,
      "learning_rate": 7.346942417229271e-06,
      "loss": 3.1801,
      "step": 2668
    },
    {
      "epoch": 1.7570770243581304,
      "grad_norm": 8.983050346374512,
      "learning_rate": 7.340296105985299e-06,
      "loss": 1.806,
      "step": 2669
    },
    {
      "epoch": 1.7577353522053984,
      "grad_norm": 19.619110107421875,
      "learning_rate": 7.333651058822491e-06,
      "loss": 2.2126,
      "step": 2670
    },
    {
      "epoch": 1.7583936800526663,
      "grad_norm": 8.35432243347168,
      "learning_rate": 7.3270072788990465e-06,
      "loss": 1.9644,
      "step": 2671
    },
    {
      "epoch": 1.759052007899934,
      "grad_norm": 1.2237330675125122,
      "learning_rate": 7.32036476937257e-06,
      "loss": 1.5499,
      "step": 2672
    },
    {
      "epoch": 1.7597103357472021,
      "grad_norm": 17.079431533813477,
      "learning_rate": 7.313723533400047e-06,
      "loss": 2.1843,
      "step": 2673
    },
    {
      "epoch": 1.7603686635944702,
      "grad_norm": 1.8171207904815674,
      "learning_rate": 7.307083574137876e-06,
      "loss": 1.5557,
      "step": 2674
    },
    {
      "epoch": 1.761026991441738,
      "grad_norm": 8.326783180236816,
      "learning_rate": 7.300444894741836e-06,
      "loss": 1.8809,
      "step": 2675
    },
    {
      "epoch": 1.7616853192890058,
      "grad_norm": 1.7263429164886475,
      "learning_rate": 7.293807498367098e-06,
      "loss": 1.5666,
      "step": 2676
    },
    {
      "epoch": 1.762343647136274,
      "grad_norm": 13.305931091308594,
      "learning_rate": 7.28717138816823e-06,
      "loss": 1.9172,
      "step": 2677
    },
    {
      "epoch": 1.763001974983542,
      "grad_norm": 39.7049560546875,
      "learning_rate": 7.28053656729919e-06,
      "loss": 3.6653,
      "step": 2678
    },
    {
      "epoch": 1.7636603028308098,
      "grad_norm": 2.5461480617523193,
      "learning_rate": 7.273903038913307e-06,
      "loss": 1.5672,
      "step": 2679
    },
    {
      "epoch": 1.7643186306780776,
      "grad_norm": 11.102473258972168,
      "learning_rate": 7.267270806163316e-06,
      "loss": 1.7109,
      "step": 2680
    },
    {
      "epoch": 1.7649769585253456,
      "grad_norm": 18.892107009887695,
      "learning_rate": 7.260639872201328e-06,
      "loss": 1.9712,
      "step": 2681
    },
    {
      "epoch": 1.7656352863726137,
      "grad_norm": 16.730327606201172,
      "learning_rate": 7.254010240178828e-06,
      "loss": 2.2989,
      "step": 2682
    },
    {
      "epoch": 1.7662936142198815,
      "grad_norm": 2.1550216674804688,
      "learning_rate": 7.2473819132466985e-06,
      "loss": 1.56,
      "step": 2683
    },
    {
      "epoch": 1.7669519420671493,
      "grad_norm": 2.2160804271698,
      "learning_rate": 7.240754894555194e-06,
      "loss": 1.5801,
      "step": 2684
    },
    {
      "epoch": 1.7676102699144174,
      "grad_norm": 16.4132137298584,
      "learning_rate": 7.23412918725394e-06,
      "loss": 1.8507,
      "step": 2685
    },
    {
      "epoch": 1.7682685977616854,
      "grad_norm": 6.865994930267334,
      "learning_rate": 7.227504794491948e-06,
      "loss": 1.8552,
      "step": 2686
    },
    {
      "epoch": 1.7689269256089533,
      "grad_norm": 8.85147762298584,
      "learning_rate": 7.220881719417609e-06,
      "loss": 1.6573,
      "step": 2687
    },
    {
      "epoch": 1.769585253456221,
      "grad_norm": 11.397518157958984,
      "learning_rate": 7.214259965178674e-06,
      "loss": 1.982,
      "step": 2688
    },
    {
      "epoch": 1.7702435813034891,
      "grad_norm": 11.396657943725586,
      "learning_rate": 7.207639534922273e-06,
      "loss": 2.0787,
      "step": 2689
    },
    {
      "epoch": 1.7709019091507572,
      "grad_norm": 1.6169284582138062,
      "learning_rate": 7.201020431794912e-06,
      "loss": 1.5576,
      "step": 2690
    },
    {
      "epoch": 1.771560236998025,
      "grad_norm": 0.9292909502983093,
      "learning_rate": 7.1944026589424565e-06,
      "loss": 1.5484,
      "step": 2691
    },
    {
      "epoch": 1.7722185648452928,
      "grad_norm": 7.845619201660156,
      "learning_rate": 7.187786219510147e-06,
      "loss": 2.0413,
      "step": 2692
    },
    {
      "epoch": 1.7728768926925609,
      "grad_norm": 27.559465408325195,
      "learning_rate": 7.181171116642589e-06,
      "loss": 2.3241,
      "step": 2693
    },
    {
      "epoch": 1.773535220539829,
      "grad_norm": 4.722631454467773,
      "learning_rate": 7.174557353483749e-06,
      "loss": 1.8131,
      "step": 2694
    },
    {
      "epoch": 1.7741935483870968,
      "grad_norm": 3.9048378467559814,
      "learning_rate": 7.16794493317696e-06,
      "loss": 1.6152,
      "step": 2695
    },
    {
      "epoch": 1.7748518762343646,
      "grad_norm": 4.5231499671936035,
      "learning_rate": 7.16133385886492e-06,
      "loss": 1.6618,
      "step": 2696
    },
    {
      "epoch": 1.7755102040816326,
      "grad_norm": 0.9065935611724854,
      "learning_rate": 7.154724133689677e-06,
      "loss": 1.5611,
      "step": 2697
    },
    {
      "epoch": 1.7761685319289007,
      "grad_norm": 1.682939052581787,
      "learning_rate": 7.148115760792649e-06,
      "loss": 1.5685,
      "step": 2698
    },
    {
      "epoch": 1.7768268597761685,
      "grad_norm": 20.11478042602539,
      "learning_rate": 7.141508743314606e-06,
      "loss": 2.0549,
      "step": 2699
    },
    {
      "epoch": 1.7774851876234363,
      "grad_norm": 16.231706619262695,
      "learning_rate": 7.134903084395671e-06,
      "loss": 2.0479,
      "step": 2700
    },
    {
      "epoch": 1.7781435154707044,
      "grad_norm": 16.49264144897461,
      "learning_rate": 7.128298787175327e-06,
      "loss": 2.0055,
      "step": 2701
    },
    {
      "epoch": 1.7788018433179724,
      "grad_norm": 15.119377136230469,
      "learning_rate": 7.121695854792412e-06,
      "loss": 1.8144,
      "step": 2702
    },
    {
      "epoch": 1.7794601711652402,
      "grad_norm": 9.001219749450684,
      "learning_rate": 7.115094290385103e-06,
      "loss": 1.8541,
      "step": 2703
    },
    {
      "epoch": 1.780118499012508,
      "grad_norm": 4.035343647003174,
      "learning_rate": 7.108494097090939e-06,
      "loss": 1.6006,
      "step": 2704
    },
    {
      "epoch": 1.7807768268597761,
      "grad_norm": 12.450706481933594,
      "learning_rate": 7.101895278046803e-06,
      "loss": 1.8922,
      "step": 2705
    },
    {
      "epoch": 1.7814351547070442,
      "grad_norm": 20.06954002380371,
      "learning_rate": 7.0952978363889235e-06,
      "loss": 2.0261,
      "step": 2706
    },
    {
      "epoch": 1.7820934825543122,
      "grad_norm": 15.13178539276123,
      "learning_rate": 7.0887017752528755e-06,
      "loss": 1.9921,
      "step": 2707
    },
    {
      "epoch": 1.78275181040158,
      "grad_norm": 9.810636520385742,
      "learning_rate": 7.082107097773579e-06,
      "loss": 1.9629,
      "step": 2708
    },
    {
      "epoch": 1.7834101382488479,
      "grad_norm": 6.195952415466309,
      "learning_rate": 7.075513807085299e-06,
      "loss": 1.6306,
      "step": 2709
    },
    {
      "epoch": 1.784068466096116,
      "grad_norm": 0.9446104764938354,
      "learning_rate": 7.068921906321631e-06,
      "loss": 1.5414,
      "step": 2710
    },
    {
      "epoch": 1.784726793943384,
      "grad_norm": 13.820030212402344,
      "learning_rate": 7.062331398615521e-06,
      "loss": 2.0064,
      "step": 2711
    },
    {
      "epoch": 1.7853851217906518,
      "grad_norm": 15.10848617553711,
      "learning_rate": 7.05574228709925e-06,
      "loss": 1.9334,
      "step": 2712
    },
    {
      "epoch": 1.7860434496379196,
      "grad_norm": 1.6657005548477173,
      "learning_rate": 7.049154574904431e-06,
      "loss": 1.5599,
      "step": 2713
    },
    {
      "epoch": 1.7867017774851877,
      "grad_norm": 7.7547688484191895,
      "learning_rate": 7.042568265162015e-06,
      "loss": 1.7994,
      "step": 2714
    },
    {
      "epoch": 1.7873601053324557,
      "grad_norm": 7.370528697967529,
      "learning_rate": 7.035983361002293e-06,
      "loss": 1.6768,
      "step": 2715
    },
    {
      "epoch": 1.7880184331797235,
      "grad_norm": 7.484477519989014,
      "learning_rate": 7.02939986555487e-06,
      "loss": 1.7793,
      "step": 2716
    },
    {
      "epoch": 1.7886767610269914,
      "grad_norm": 14.852550506591797,
      "learning_rate": 7.022817781948702e-06,
      "loss": 1.9539,
      "step": 2717
    },
    {
      "epoch": 1.7893350888742594,
      "grad_norm": 1.1340512037277222,
      "learning_rate": 7.0162371133120665e-06,
      "loss": 1.5472,
      "step": 2718
    },
    {
      "epoch": 1.7899934167215275,
      "grad_norm": 8.033105850219727,
      "learning_rate": 7.009657862772559e-06,
      "loss": 1.6292,
      "step": 2719
    },
    {
      "epoch": 1.7906517445687953,
      "grad_norm": 1.1274079084396362,
      "learning_rate": 7.003080033457111e-06,
      "loss": 1.5578,
      "step": 2720
    },
    {
      "epoch": 1.791310072416063,
      "grad_norm": 6.443711757659912,
      "learning_rate": 6.996503628491986e-06,
      "loss": 1.6896,
      "step": 2721
    },
    {
      "epoch": 1.7919684002633312,
      "grad_norm": 21.523839950561523,
      "learning_rate": 6.989928651002748e-06,
      "loss": 2.6363,
      "step": 2722
    },
    {
      "epoch": 1.7926267281105992,
      "grad_norm": 2.43356990814209,
      "learning_rate": 6.983355104114301e-06,
      "loss": 1.5668,
      "step": 2723
    },
    {
      "epoch": 1.793285055957867,
      "grad_norm": 1.2543994188308716,
      "learning_rate": 6.97678299095087e-06,
      "loss": 1.5425,
      "step": 2724
    },
    {
      "epoch": 1.7939433838051349,
      "grad_norm": 1.5173077583312988,
      "learning_rate": 6.970212314635982e-06,
      "loss": 1.5487,
      "step": 2725
    },
    {
      "epoch": 1.794601711652403,
      "grad_norm": 6.416282653808594,
      "learning_rate": 6.963643078292497e-06,
      "loss": 1.85,
      "step": 2726
    },
    {
      "epoch": 1.795260039499671,
      "grad_norm": 0.8366973996162415,
      "learning_rate": 6.957075285042586e-06,
      "loss": 1.5419,
      "step": 2727
    },
    {
      "epoch": 1.7959183673469388,
      "grad_norm": 0.9058383703231812,
      "learning_rate": 6.95050893800773e-06,
      "loss": 1.5381,
      "step": 2728
    },
    {
      "epoch": 1.7965766951942066,
      "grad_norm": 19.20816993713379,
      "learning_rate": 6.943944040308725e-06,
      "loss": 2.6331,
      "step": 2729
    },
    {
      "epoch": 1.7972350230414746,
      "grad_norm": 17.82113265991211,
      "learning_rate": 6.937380595065686e-06,
      "loss": 1.9467,
      "step": 2730
    },
    {
      "epoch": 1.7978933508887427,
      "grad_norm": 22.702993392944336,
      "learning_rate": 6.930818605398023e-06,
      "loss": 2.8393,
      "step": 2731
    },
    {
      "epoch": 1.7985516787360105,
      "grad_norm": 1.1848136186599731,
      "learning_rate": 6.924258074424464e-06,
      "loss": 1.5382,
      "step": 2732
    },
    {
      "epoch": 1.7992100065832783,
      "grad_norm": 3.2805490493774414,
      "learning_rate": 6.9176990052630465e-06,
      "loss": 1.5629,
      "step": 2733
    },
    {
      "epoch": 1.7998683344305464,
      "grad_norm": 1.4038801193237305,
      "learning_rate": 6.9111414010311026e-06,
      "loss": 1.547,
      "step": 2734
    },
    {
      "epoch": 1.8005266622778144,
      "grad_norm": 16.965076446533203,
      "learning_rate": 6.904585264845275e-06,
      "loss": 2.2954,
      "step": 2735
    },
    {
      "epoch": 1.8011849901250823,
      "grad_norm": 17.5009708404541,
      "learning_rate": 6.8980305998215106e-06,
      "loss": 2.2444,
      "step": 2736
    },
    {
      "epoch": 1.80184331797235,
      "grad_norm": 21.21161651611328,
      "learning_rate": 6.891477409075051e-06,
      "loss": 2.5281,
      "step": 2737
    },
    {
      "epoch": 1.8025016458196181,
      "grad_norm": 1.001302719116211,
      "learning_rate": 6.884925695720441e-06,
      "loss": 1.5401,
      "step": 2738
    },
    {
      "epoch": 1.8031599736668862,
      "grad_norm": 30.91489028930664,
      "learning_rate": 6.878375462871525e-06,
      "loss": 2.8363,
      "step": 2739
    },
    {
      "epoch": 1.803818301514154,
      "grad_norm": 14.504359245300293,
      "learning_rate": 6.8718267136414375e-06,
      "loss": 2.3445,
      "step": 2740
    },
    {
      "epoch": 1.8044766293614218,
      "grad_norm": 10.572935104370117,
      "learning_rate": 6.865279451142613e-06,
      "loss": 1.7026,
      "step": 2741
    },
    {
      "epoch": 1.8051349572086899,
      "grad_norm": 0.9943259954452515,
      "learning_rate": 6.85873367848678e-06,
      "loss": 1.5387,
      "step": 2742
    },
    {
      "epoch": 1.805793285055958,
      "grad_norm": 1.0119692087173462,
      "learning_rate": 6.852189398784954e-06,
      "loss": 1.5469,
      "step": 2743
    },
    {
      "epoch": 1.8064516129032258,
      "grad_norm": 2.4059107303619385,
      "learning_rate": 6.845646615147445e-06,
      "loss": 1.5888,
      "step": 2744
    },
    {
      "epoch": 1.8071099407504936,
      "grad_norm": 20.154294967651367,
      "learning_rate": 6.839105330683856e-06,
      "loss": 2.2215,
      "step": 2745
    },
    {
      "epoch": 1.8077682685977616,
      "grad_norm": 21.7694149017334,
      "learning_rate": 6.832565548503061e-06,
      "loss": 2.372,
      "step": 2746
    },
    {
      "epoch": 1.8084265964450297,
      "grad_norm": 9.278570175170898,
      "learning_rate": 6.826027271713241e-06,
      "loss": 1.7211,
      "step": 2747
    },
    {
      "epoch": 1.8090849242922977,
      "grad_norm": 9.482497215270996,
      "learning_rate": 6.819490503421846e-06,
      "loss": 1.9747,
      "step": 2748
    },
    {
      "epoch": 1.8097432521395656,
      "grad_norm": 15.700102806091309,
      "learning_rate": 6.812955246735624e-06,
      "loss": 2.1598,
      "step": 2749
    },
    {
      "epoch": 1.8104015799868334,
      "grad_norm": 1.5049365758895874,
      "learning_rate": 6.806421504760583e-06,
      "loss": 1.5618,
      "step": 2750
    },
    {
      "epoch": 1.8110599078341014,
      "grad_norm": 1.9789422750473022,
      "learning_rate": 6.799889280602031e-06,
      "loss": 1.5891,
      "step": 2751
    },
    {
      "epoch": 1.8117182356813695,
      "grad_norm": 2.839484691619873,
      "learning_rate": 6.79335857736455e-06,
      "loss": 1.5637,
      "step": 2752
    },
    {
      "epoch": 1.8123765635286373,
      "grad_norm": 11.017098426818848,
      "learning_rate": 6.786829398151988e-06,
      "loss": 1.9957,
      "step": 2753
    },
    {
      "epoch": 1.8130348913759051,
      "grad_norm": 1.812897801399231,
      "learning_rate": 6.780301746067483e-06,
      "loss": 1.5618,
      "step": 2754
    },
    {
      "epoch": 1.8136932192231732,
      "grad_norm": 21.059022903442383,
      "learning_rate": 6.773775624213444e-06,
      "loss": 2.229,
      "step": 2755
    },
    {
      "epoch": 1.8143515470704412,
      "grad_norm": 19.161449432373047,
      "learning_rate": 6.76725103569154e-06,
      "loss": 1.9993,
      "step": 2756
    },
    {
      "epoch": 1.815009874917709,
      "grad_norm": 14.25429630279541,
      "learning_rate": 6.760727983602727e-06,
      "loss": 2.1868,
      "step": 2757
    },
    {
      "epoch": 1.8156682027649769,
      "grad_norm": 17.427732467651367,
      "learning_rate": 6.754206471047231e-06,
      "loss": 2.1449,
      "step": 2758
    },
    {
      "epoch": 1.816326530612245,
      "grad_norm": 1.5150599479675293,
      "learning_rate": 6.747686501124531e-06,
      "loss": 1.5553,
      "step": 2759
    },
    {
      "epoch": 1.816984858459513,
      "grad_norm": 12.759461402893066,
      "learning_rate": 6.741168076933384e-06,
      "loss": 1.9182,
      "step": 2760
    },
    {
      "epoch": 1.8176431863067808,
      "grad_norm": 11.56478500366211,
      "learning_rate": 6.734651201571815e-06,
      "loss": 2.1918,
      "step": 2761
    },
    {
      "epoch": 1.8183015141540486,
      "grad_norm": 7.8981451988220215,
      "learning_rate": 6.728135878137103e-06,
      "loss": 1.7774,
      "step": 2762
    },
    {
      "epoch": 1.8189598420013167,
      "grad_norm": 24.836166381835938,
      "learning_rate": 6.721622109725799e-06,
      "loss": 1.8457,
      "step": 2763
    },
    {
      "epoch": 1.8196181698485847,
      "grad_norm": 1.8778250217437744,
      "learning_rate": 6.7151098994337096e-06,
      "loss": 1.5573,
      "step": 2764
    },
    {
      "epoch": 1.8202764976958525,
      "grad_norm": 18.158369064331055,
      "learning_rate": 6.7085992503559004e-06,
      "loss": 1.9214,
      "step": 2765
    },
    {
      "epoch": 1.8209348255431204,
      "grad_norm": 11.312434196472168,
      "learning_rate": 6.702090165586698e-06,
      "loss": 1.915,
      "step": 2766
    },
    {
      "epoch": 1.8215931533903884,
      "grad_norm": 11.014607429504395,
      "learning_rate": 6.695582648219685e-06,
      "loss": 1.8477,
      "step": 2767
    },
    {
      "epoch": 1.8222514812376565,
      "grad_norm": 18.051748275756836,
      "learning_rate": 6.689076701347696e-06,
      "loss": 2.2286,
      "step": 2768
    },
    {
      "epoch": 1.8229098090849243,
      "grad_norm": 18.090364456176758,
      "learning_rate": 6.682572328062822e-06,
      "loss": 1.8829,
      "step": 2769
    },
    {
      "epoch": 1.823568136932192,
      "grad_norm": 22.7751522064209,
      "learning_rate": 6.6760695314564085e-06,
      "loss": 2.2904,
      "step": 2770
    },
    {
      "epoch": 1.8242264647794602,
      "grad_norm": 47.16299819946289,
      "learning_rate": 6.6695683146190435e-06,
      "loss": 3.1362,
      "step": 2771
    },
    {
      "epoch": 1.8248847926267282,
      "grad_norm": 14.014546394348145,
      "learning_rate": 6.663068680640574e-06,
      "loss": 2.0539,
      "step": 2772
    },
    {
      "epoch": 1.825543120473996,
      "grad_norm": 22.117935180664062,
      "learning_rate": 6.656570632610089e-06,
      "loss": 2.6998,
      "step": 2773
    },
    {
      "epoch": 1.8262014483212639,
      "grad_norm": 31.1732120513916,
      "learning_rate": 6.650074173615923e-06,
      "loss": 3.2748,
      "step": 2774
    },
    {
      "epoch": 1.826859776168532,
      "grad_norm": 23.99325942993164,
      "learning_rate": 6.643579306745657e-06,
      "loss": 2.063,
      "step": 2775
    },
    {
      "epoch": 1.8275181040158,
      "grad_norm": 15.466495513916016,
      "learning_rate": 6.6370860350861185e-06,
      "loss": 1.893,
      "step": 2776
    },
    {
      "epoch": 1.8281764318630678,
      "grad_norm": 7.8827595710754395,
      "learning_rate": 6.63059436172337e-06,
      "loss": 1.8827,
      "step": 2777
    },
    {
      "epoch": 1.8288347597103356,
      "grad_norm": 1.5300172567367554,
      "learning_rate": 6.624104289742721e-06,
      "loss": 1.5548,
      "step": 2778
    },
    {
      "epoch": 1.8294930875576036,
      "grad_norm": 10.557639122009277,
      "learning_rate": 6.6176158222287175e-06,
      "loss": 2.0547,
      "step": 2779
    },
    {
      "epoch": 1.8301514154048717,
      "grad_norm": 1.1503742933273315,
      "learning_rate": 6.611128962265137e-06,
      "loss": 1.5496,
      "step": 2780
    },
    {
      "epoch": 1.8308097432521395,
      "grad_norm": 2.4282381534576416,
      "learning_rate": 6.604643712935004e-06,
      "loss": 1.5697,
      "step": 2781
    },
    {
      "epoch": 1.8314680710994073,
      "grad_norm": 27.842239379882812,
      "learning_rate": 6.5981600773205746e-06,
      "loss": 2.7245,
      "step": 2782
    },
    {
      "epoch": 1.8321263989466754,
      "grad_norm": 17.981014251708984,
      "learning_rate": 6.591678058503324e-06,
      "loss": 2.229,
      "step": 2783
    },
    {
      "epoch": 1.8327847267939434,
      "grad_norm": 12.575650215148926,
      "learning_rate": 6.58519765956398e-06,
      "loss": 2.0229,
      "step": 2784
    },
    {
      "epoch": 1.8334430546412115,
      "grad_norm": 10.621957778930664,
      "learning_rate": 6.578718883582488e-06,
      "loss": 2.0373,
      "step": 2785
    },
    {
      "epoch": 1.8341013824884793,
      "grad_norm": 1.56666100025177,
      "learning_rate": 6.5722417336380284e-06,
      "loss": 1.5495,
      "step": 2786
    },
    {
      "epoch": 1.8347597103357471,
      "grad_norm": 2.0708374977111816,
      "learning_rate": 6.565766212808996e-06,
      "loss": 1.6073,
      "step": 2787
    },
    {
      "epoch": 1.8354180381830152,
      "grad_norm": 9.558507919311523,
      "learning_rate": 6.559292324173029e-06,
      "loss": 1.8783,
      "step": 2788
    },
    {
      "epoch": 1.8360763660302832,
      "grad_norm": 22.29330062866211,
      "learning_rate": 6.552820070806982e-06,
      "loss": 2.5652,
      "step": 2789
    },
    {
      "epoch": 1.836734693877551,
      "grad_norm": 3.4405672550201416,
      "learning_rate": 6.546349455786926e-06,
      "loss": 1.5831,
      "step": 2790
    },
    {
      "epoch": 1.8373930217248189,
      "grad_norm": 25.771116256713867,
      "learning_rate": 6.539880482188159e-06,
      "loss": 2.6104,
      "step": 2791
    },
    {
      "epoch": 1.838051349572087,
      "grad_norm": 4.562071800231934,
      "learning_rate": 6.533413153085207e-06,
      "loss": 1.6043,
      "step": 2792
    },
    {
      "epoch": 1.838709677419355,
      "grad_norm": 16.746063232421875,
      "learning_rate": 6.526947471551799e-06,
      "loss": 1.934,
      "step": 2793
    },
    {
      "epoch": 1.8393680052666228,
      "grad_norm": 6.842924118041992,
      "learning_rate": 6.520483440660888e-06,
      "loss": 1.7512,
      "step": 2794
    },
    {
      "epoch": 1.8400263331138906,
      "grad_norm": 7.7534499168396,
      "learning_rate": 6.5140210634846524e-06,
      "loss": 1.6047,
      "step": 2795
    },
    {
      "epoch": 1.8406846609611587,
      "grad_norm": 2.9662415981292725,
      "learning_rate": 6.507560343094464e-06,
      "loss": 1.5585,
      "step": 2796
    },
    {
      "epoch": 1.8413429888084267,
      "grad_norm": 5.365765571594238,
      "learning_rate": 6.5011012825609235e-06,
      "loss": 1.6477,
      "step": 2797
    },
    {
      "epoch": 1.8420013166556946,
      "grad_norm": 13.953723907470703,
      "learning_rate": 6.494643884953839e-06,
      "loss": 2.0225,
      "step": 2798
    },
    {
      "epoch": 1.8426596445029624,
      "grad_norm": 12.561821937561035,
      "learning_rate": 6.488188153342222e-06,
      "loss": 2.1987,
      "step": 2799
    },
    {
      "epoch": 1.8433179723502304,
      "grad_norm": 7.96677303314209,
      "learning_rate": 6.481734090794301e-06,
      "loss": 1.6502,
      "step": 2800
    },
    {
      "epoch": 1.8439763001974985,
      "grad_norm": 25.71743392944336,
      "learning_rate": 6.475281700377507e-06,
      "loss": 2.7071,
      "step": 2801
    },
    {
      "epoch": 1.8446346280447663,
      "grad_norm": 2.661113739013672,
      "learning_rate": 6.468830985158474e-06,
      "loss": 1.58,
      "step": 2802
    },
    {
      "epoch": 1.8452929558920341,
      "grad_norm": 14.74122428894043,
      "learning_rate": 6.462381948203045e-06,
      "loss": 2.1559,
      "step": 2803
    },
    {
      "epoch": 1.8459512837393022,
      "grad_norm": 4.587562084197998,
      "learning_rate": 6.455934592576263e-06,
      "loss": 1.7,
      "step": 2804
    },
    {
      "epoch": 1.8466096115865702,
      "grad_norm": 13.010078430175781,
      "learning_rate": 6.449488921342369e-06,
      "loss": 2.058,
      "step": 2805
    },
    {
      "epoch": 1.847267939433838,
      "grad_norm": 7.902049541473389,
      "learning_rate": 6.443044937564807e-06,
      "loss": 1.9207,
      "step": 2806
    },
    {
      "epoch": 1.8479262672811059,
      "grad_norm": 15.435589790344238,
      "learning_rate": 6.436602644306222e-06,
      "loss": 2.2688,
      "step": 2807
    },
    {
      "epoch": 1.848584595128374,
      "grad_norm": 3.3982491493225098,
      "learning_rate": 6.430162044628445e-06,
      "loss": 1.629,
      "step": 2808
    },
    {
      "epoch": 1.849242922975642,
      "grad_norm": 21.390993118286133,
      "learning_rate": 6.423723141592513e-06,
      "loss": 2.6018,
      "step": 2809
    },
    {
      "epoch": 1.8499012508229098,
      "grad_norm": 4.030797958374023,
      "learning_rate": 6.4172859382586525e-06,
      "loss": 1.5848,
      "step": 2810
    },
    {
      "epoch": 1.8505595786701776,
      "grad_norm": 9.673174858093262,
      "learning_rate": 6.410850437686281e-06,
      "loss": 1.9454,
      "step": 2811
    },
    {
      "epoch": 1.8512179065174457,
      "grad_norm": 6.062202453613281,
      "learning_rate": 6.404416642934007e-06,
      "loss": 1.792,
      "step": 2812
    },
    {
      "epoch": 1.8518762343647137,
      "grad_norm": 0.9369637966156006,
      "learning_rate": 6.397984557059632e-06,
      "loss": 1.544,
      "step": 2813
    },
    {
      "epoch": 1.8525345622119815,
      "grad_norm": 1.8105719089508057,
      "learning_rate": 6.391554183120139e-06,
      "loss": 1.5668,
      "step": 2814
    },
    {
      "epoch": 1.8531928900592494,
      "grad_norm": 13.147303581237793,
      "learning_rate": 6.3851255241717e-06,
      "loss": 1.995,
      "step": 2815
    },
    {
      "epoch": 1.8538512179065174,
      "grad_norm": 26.22813606262207,
      "learning_rate": 6.378698583269681e-06,
      "loss": 2.0157,
      "step": 2816
    },
    {
      "epoch": 1.8545095457537855,
      "grad_norm": 26.98009490966797,
      "learning_rate": 6.372273363468611e-06,
      "loss": 2.667,
      "step": 2817
    },
    {
      "epoch": 1.8551678736010533,
      "grad_norm": 2.6892499923706055,
      "learning_rate": 6.365849867822221e-06,
      "loss": 1.5797,
      "step": 2818
    },
    {
      "epoch": 1.8558262014483211,
      "grad_norm": 6.031164169311523,
      "learning_rate": 6.359428099383415e-06,
      "loss": 1.7947,
      "step": 2819
    },
    {
      "epoch": 1.8564845292955892,
      "grad_norm": 9.588826179504395,
      "learning_rate": 6.353008061204271e-06,
      "loss": 1.9427,
      "step": 2820
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 20.790781021118164,
      "learning_rate": 6.34658975633605e-06,
      "loss": 1.9846,
      "step": 2821
    },
    {
      "epoch": 1.857801184990125,
      "grad_norm": 10.092577934265137,
      "learning_rate": 6.340173187829197e-06,
      "loss": 1.7268,
      "step": 2822
    },
    {
      "epoch": 1.8584595128373929,
      "grad_norm": 2.0317578315734863,
      "learning_rate": 6.333758358733313e-06,
      "loss": 1.5585,
      "step": 2823
    },
    {
      "epoch": 1.859117840684661,
      "grad_norm": 33.38673782348633,
      "learning_rate": 6.327345272097183e-06,
      "loss": 3.8819,
      "step": 2824
    },
    {
      "epoch": 1.859776168531929,
      "grad_norm": 15.612563133239746,
      "learning_rate": 6.320933930968771e-06,
      "loss": 2.0739,
      "step": 2825
    },
    {
      "epoch": 1.860434496379197,
      "grad_norm": 4.070261478424072,
      "learning_rate": 6.314524338395205e-06,
      "loss": 1.5849,
      "step": 2826
    },
    {
      "epoch": 1.8610928242264648,
      "grad_norm": 2.675405979156494,
      "learning_rate": 6.308116497422773e-06,
      "loss": 1.5666,
      "step": 2827
    },
    {
      "epoch": 1.8617511520737327,
      "grad_norm": 11.328452110290527,
      "learning_rate": 6.301710411096939e-06,
      "loss": 1.9529,
      "step": 2828
    },
    {
      "epoch": 1.8624094799210007,
      "grad_norm": 11.693103790283203,
      "learning_rate": 6.2953060824623414e-06,
      "loss": 1.7921,
      "step": 2829
    },
    {
      "epoch": 1.8630678077682687,
      "grad_norm": 2.3102965354919434,
      "learning_rate": 6.288903514562764e-06,
      "loss": 1.5606,
      "step": 2830
    },
    {
      "epoch": 1.8637261356155366,
      "grad_norm": 1.922019600868225,
      "learning_rate": 6.28250271044117e-06,
      "loss": 1.5582,
      "step": 2831
    },
    {
      "epoch": 1.8643844634628044,
      "grad_norm": 13.122669219970703,
      "learning_rate": 6.2761036731396774e-06,
      "loss": 1.9071,
      "step": 2832
    },
    {
      "epoch": 1.8650427913100724,
      "grad_norm": 1.2546045780181885,
      "learning_rate": 6.269706405699565e-06,
      "loss": 1.5409,
      "step": 2833
    },
    {
      "epoch": 1.8657011191573405,
      "grad_norm": 10.078812599182129,
      "learning_rate": 6.263310911161269e-06,
      "loss": 1.7034,
      "step": 2834
    },
    {
      "epoch": 1.8663594470046083,
      "grad_norm": 10.046436309814453,
      "learning_rate": 6.2569171925643915e-06,
      "loss": 1.9794,
      "step": 2835
    },
    {
      "epoch": 1.8670177748518761,
      "grad_norm": 1.7002533674240112,
      "learning_rate": 6.250525252947677e-06,
      "loss": 1.5479,
      "step": 2836
    },
    {
      "epoch": 1.8676761026991442,
      "grad_norm": 0.928288459777832,
      "learning_rate": 6.2441350953490335e-06,
      "loss": 1.5389,
      "step": 2837
    },
    {
      "epoch": 1.8683344305464122,
      "grad_norm": 20.113264083862305,
      "learning_rate": 6.237746722805522e-06,
      "loss": 2.3285,
      "step": 2838
    },
    {
      "epoch": 1.86899275839368,
      "grad_norm": 15.795276641845703,
      "learning_rate": 6.231360138353352e-06,
      "loss": 2.3712,
      "step": 2839
    },
    {
      "epoch": 1.869651086240948,
      "grad_norm": 15.025589942932129,
      "learning_rate": 6.224975345027884e-06,
      "loss": 1.7848,
      "step": 2840
    },
    {
      "epoch": 1.870309414088216,
      "grad_norm": 16.78862762451172,
      "learning_rate": 6.2185923458636286e-06,
      "loss": 1.9861,
      "step": 2841
    },
    {
      "epoch": 1.870967741935484,
      "grad_norm": 2.2229206562042236,
      "learning_rate": 6.21221114389424e-06,
      "loss": 1.5517,
      "step": 2842
    },
    {
      "epoch": 1.8716260697827518,
      "grad_norm": 24.653127670288086,
      "learning_rate": 6.205831742152524e-06,
      "loss": 2.608,
      "step": 2843
    },
    {
      "epoch": 1.8722843976300196,
      "grad_norm": 26.220932006835938,
      "learning_rate": 6.199454143670429e-06,
      "loss": 2.2165,
      "step": 2844
    },
    {
      "epoch": 1.8729427254772877,
      "grad_norm": 1.7819663286209106,
      "learning_rate": 6.19307835147904e-06,
      "loss": 1.5512,
      "step": 2845
    },
    {
      "epoch": 1.8736010533245557,
      "grad_norm": 1.2721887826919556,
      "learning_rate": 6.186704368608592e-06,
      "loss": 1.5489,
      "step": 2846
    },
    {
      "epoch": 1.8742593811718236,
      "grad_norm": 5.573164939880371,
      "learning_rate": 6.1803321980884566e-06,
      "loss": 1.761,
      "step": 2847
    },
    {
      "epoch": 1.8749177090190914,
      "grad_norm": 4.693484306335449,
      "learning_rate": 6.173961842947143e-06,
      "loss": 1.5944,
      "step": 2848
    },
    {
      "epoch": 1.8755760368663594,
      "grad_norm": 9.187545776367188,
      "learning_rate": 6.1675933062123e-06,
      "loss": 1.8378,
      "step": 2849
    },
    {
      "epoch": 1.8762343647136275,
      "grad_norm": 13.096282958984375,
      "learning_rate": 6.1612265909107125e-06,
      "loss": 1.8855,
      "step": 2850
    },
    {
      "epoch": 1.8768926925608953,
      "grad_norm": 12.172908782958984,
      "learning_rate": 6.154861700068291e-06,
      "loss": 2.2396,
      "step": 2851
    },
    {
      "epoch": 1.8775510204081631,
      "grad_norm": 15.234270095825195,
      "learning_rate": 6.148498636710092e-06,
      "loss": 1.9957,
      "step": 2852
    },
    {
      "epoch": 1.8782093482554312,
      "grad_norm": 9.084290504455566,
      "learning_rate": 6.142137403860301e-06,
      "loss": 1.6579,
      "step": 2853
    },
    {
      "epoch": 1.8788676761026992,
      "grad_norm": 1.6887221336364746,
      "learning_rate": 6.135778004542219e-06,
      "loss": 1.5879,
      "step": 2854
    },
    {
      "epoch": 1.879526003949967,
      "grad_norm": 3.310616970062256,
      "learning_rate": 6.1294204417782954e-06,
      "loss": 1.6504,
      "step": 2855
    },
    {
      "epoch": 1.8801843317972349,
      "grad_norm": 16.919424057006836,
      "learning_rate": 6.123064718590099e-06,
      "loss": 2.0289,
      "step": 2856
    },
    {
      "epoch": 1.880842659644503,
      "grad_norm": 1.0646295547485352,
      "learning_rate": 6.116710837998314e-06,
      "loss": 1.5436,
      "step": 2857
    },
    {
      "epoch": 1.881500987491771,
      "grad_norm": 1.9148045778274536,
      "learning_rate": 6.1103588030227646e-06,
      "loss": 1.5642,
      "step": 2858
    },
    {
      "epoch": 1.8821593153390388,
      "grad_norm": 0.8367083072662354,
      "learning_rate": 6.1040086166823946e-06,
      "loss": 1.5431,
      "step": 2859
    },
    {
      "epoch": 1.8828176431863066,
      "grad_norm": 8.014071464538574,
      "learning_rate": 6.097660281995257e-06,
      "loss": 1.7025,
      "step": 2860
    },
    {
      "epoch": 1.8834759710335747,
      "grad_norm": 1.0493168830871582,
      "learning_rate": 6.09131380197854e-06,
      "loss": 1.5478,
      "step": 2861
    },
    {
      "epoch": 1.8841342988808427,
      "grad_norm": 16.07305335998535,
      "learning_rate": 6.084969179648542e-06,
      "loss": 1.9758,
      "step": 2862
    },
    {
      "epoch": 1.8847926267281108,
      "grad_norm": 28.133102416992188,
      "learning_rate": 6.07862641802068e-06,
      "loss": 2.194,
      "step": 2863
    },
    {
      "epoch": 1.8854509545753786,
      "grad_norm": 38.996952056884766,
      "learning_rate": 6.072285520109488e-06,
      "loss": 2.5552,
      "step": 2864
    },
    {
      "epoch": 1.8861092824226464,
      "grad_norm": 1.2749459743499756,
      "learning_rate": 6.0659464889286106e-06,
      "loss": 1.5448,
      "step": 2865
    },
    {
      "epoch": 1.8867676102699145,
      "grad_norm": 6.266534328460693,
      "learning_rate": 6.059609327490817e-06,
      "loss": 1.5693,
      "step": 2866
    },
    {
      "epoch": 1.8874259381171825,
      "grad_norm": 0.9149413704872131,
      "learning_rate": 6.053274038807967e-06,
      "loss": 1.5367,
      "step": 2867
    },
    {
      "epoch": 1.8880842659644503,
      "grad_norm": 14.810041427612305,
      "learning_rate": 6.046940625891049e-06,
      "loss": 1.9277,
      "step": 2868
    },
    {
      "epoch": 1.8887425938117182,
      "grad_norm": 27.866634368896484,
      "learning_rate": 6.040609091750151e-06,
      "loss": 2.2178,
      "step": 2869
    },
    {
      "epoch": 1.8894009216589862,
      "grad_norm": 8.767921447753906,
      "learning_rate": 6.0342794393944705e-06,
      "loss": 1.916,
      "step": 2870
    },
    {
      "epoch": 1.8900592495062543,
      "grad_norm": 12.20610523223877,
      "learning_rate": 6.027951671832308e-06,
      "loss": 1.9584,
      "step": 2871
    },
    {
      "epoch": 1.890717577353522,
      "grad_norm": 15.814908027648926,
      "learning_rate": 6.021625792071075e-06,
      "loss": 1.8471,
      "step": 2872
    },
    {
      "epoch": 1.89137590520079,
      "grad_norm": 1.4190205335617065,
      "learning_rate": 6.015301803117277e-06,
      "loss": 1.5422,
      "step": 2873
    },
    {
      "epoch": 1.892034233048058,
      "grad_norm": 3.8391547203063965,
      "learning_rate": 6.008979707976526e-06,
      "loss": 1.5663,
      "step": 2874
    },
    {
      "epoch": 1.892692560895326,
      "grad_norm": 7.3191328048706055,
      "learning_rate": 6.0026595096535364e-06,
      "loss": 1.7985,
      "step": 2875
    },
    {
      "epoch": 1.8933508887425938,
      "grad_norm": 10.154099464416504,
      "learning_rate": 5.996341211152112e-06,
      "loss": 1.803,
      "step": 2876
    },
    {
      "epoch": 1.8940092165898617,
      "grad_norm": 23.17438316345215,
      "learning_rate": 5.990024815475162e-06,
      "loss": 2.712,
      "step": 2877
    },
    {
      "epoch": 1.8946675444371297,
      "grad_norm": 1.2436648607254028,
      "learning_rate": 5.983710325624689e-06,
      "loss": 1.5403,
      "step": 2878
    },
    {
      "epoch": 1.8953258722843978,
      "grad_norm": 20.754152297973633,
      "learning_rate": 5.97739774460179e-06,
      "loss": 2.5631,
      "step": 2879
    },
    {
      "epoch": 1.8959842001316656,
      "grad_norm": 1.2847415208816528,
      "learning_rate": 5.9710870754066505e-06,
      "loss": 1.5446,
      "step": 2880
    },
    {
      "epoch": 1.8966425279789334,
      "grad_norm": 6.770719051361084,
      "learning_rate": 5.964778321038556e-06,
      "loss": 1.7547,
      "step": 2881
    },
    {
      "epoch": 1.8973008558262014,
      "grad_norm": 24.859312057495117,
      "learning_rate": 5.9584714844958735e-06,
      "loss": 2.2209,
      "step": 2882
    },
    {
      "epoch": 1.8979591836734695,
      "grad_norm": 0.9611076712608337,
      "learning_rate": 5.952166568776062e-06,
      "loss": 1.5444,
      "step": 2883
    },
    {
      "epoch": 1.8986175115207373,
      "grad_norm": 7.888507843017578,
      "learning_rate": 5.94586357687567e-06,
      "loss": 1.8016,
      "step": 2884
    },
    {
      "epoch": 1.8992758393680051,
      "grad_norm": 25.213075637817383,
      "learning_rate": 5.9395625117903265e-06,
      "loss": 1.8189,
      "step": 2885
    },
    {
      "epoch": 1.8999341672152732,
      "grad_norm": 17.939838409423828,
      "learning_rate": 5.933263376514748e-06,
      "loss": 2.071,
      "step": 2886
    },
    {
      "epoch": 1.9005924950625412,
      "grad_norm": 1.9561777114868164,
      "learning_rate": 5.92696617404274e-06,
      "loss": 1.539,
      "step": 2887
    },
    {
      "epoch": 1.901250822909809,
      "grad_norm": 13.787981033325195,
      "learning_rate": 5.92067090736717e-06,
      "loss": 2.0339,
      "step": 2888
    },
    {
      "epoch": 1.901909150757077,
      "grad_norm": 1.552351951599121,
      "learning_rate": 5.914377579480005e-06,
      "loss": 1.5604,
      "step": 2889
    },
    {
      "epoch": 1.902567478604345,
      "grad_norm": 4.782097339630127,
      "learning_rate": 5.908086193372289e-06,
      "loss": 1.7691,
      "step": 2890
    },
    {
      "epoch": 1.903225806451613,
      "grad_norm": 14.557422637939453,
      "learning_rate": 5.901796752034128e-06,
      "loss": 1.871,
      "step": 2891
    },
    {
      "epoch": 1.9038841342988808,
      "grad_norm": 11.100743293762207,
      "learning_rate": 5.8955092584547144e-06,
      "loss": 1.9273,
      "step": 2892
    },
    {
      "epoch": 1.9045424621461486,
      "grad_norm": 10.486655235290527,
      "learning_rate": 5.889223715622325e-06,
      "loss": 1.8102,
      "step": 2893
    },
    {
      "epoch": 1.9052007899934167,
      "grad_norm": 11.176044464111328,
      "learning_rate": 5.8829401265242856e-06,
      "loss": 1.8622,
      "step": 2894
    },
    {
      "epoch": 1.9058591178406847,
      "grad_norm": 11.110316276550293,
      "learning_rate": 5.876658494147011e-06,
      "loss": 1.9582,
      "step": 2895
    },
    {
      "epoch": 1.9065174456879526,
      "grad_norm": 7.008092880249023,
      "learning_rate": 5.8703788214759875e-06,
      "loss": 1.7647,
      "step": 2896
    },
    {
      "epoch": 1.9071757735352204,
      "grad_norm": 9.052305221557617,
      "learning_rate": 5.864101111495754e-06,
      "loss": 1.8435,
      "step": 2897
    },
    {
      "epoch": 1.9078341013824884,
      "grad_norm": 23.685152053833008,
      "learning_rate": 5.857825367189931e-06,
      "loss": 2.0902,
      "step": 2898
    },
    {
      "epoch": 1.9084924292297565,
      "grad_norm": 3.302607536315918,
      "learning_rate": 5.851551591541201e-06,
      "loss": 1.5644,
      "step": 2899
    },
    {
      "epoch": 1.9091507570770243,
      "grad_norm": 1.2514283657073975,
      "learning_rate": 5.845279787531308e-06,
      "loss": 1.5458,
      "step": 2900
    },
    {
      "epoch": 1.9098090849242921,
      "grad_norm": 0.9683914184570312,
      "learning_rate": 5.839009958141062e-06,
      "loss": 1.5489,
      "step": 2901
    },
    {
      "epoch": 1.9104674127715602,
      "grad_norm": 1.8090269565582275,
      "learning_rate": 5.832742106350331e-06,
      "loss": 1.6018,
      "step": 2902
    },
    {
      "epoch": 1.9111257406188282,
      "grad_norm": 2.352203845977783,
      "learning_rate": 5.826476235138054e-06,
      "loss": 1.5919,
      "step": 2903
    },
    {
      "epoch": 1.9117840684660963,
      "grad_norm": 2.3617146015167236,
      "learning_rate": 5.820212347482211e-06,
      "loss": 1.5575,
      "step": 2904
    },
    {
      "epoch": 1.912442396313364,
      "grad_norm": 17.621776580810547,
      "learning_rate": 5.813950446359853e-06,
      "loss": 2.1975,
      "step": 2905
    },
    {
      "epoch": 1.913100724160632,
      "grad_norm": 1.4468525648117065,
      "learning_rate": 5.807690534747083e-06,
      "loss": 1.5458,
      "step": 2906
    },
    {
      "epoch": 1.9137590520079,
      "grad_norm": 15.509947776794434,
      "learning_rate": 5.801432615619052e-06,
      "loss": 2.3817,
      "step": 2907
    },
    {
      "epoch": 1.914417379855168,
      "grad_norm": 24.113893508911133,
      "learning_rate": 5.795176691949975e-06,
      "loss": 2.1834,
      "step": 2908
    },
    {
      "epoch": 1.9150757077024358,
      "grad_norm": 3.6922149658203125,
      "learning_rate": 5.788922766713119e-06,
      "loss": 1.6156,
      "step": 2909
    },
    {
      "epoch": 1.9157340355497037,
      "grad_norm": 4.503284931182861,
      "learning_rate": 5.782670842880784e-06,
      "loss": 1.6361,
      "step": 2910
    },
    {
      "epoch": 1.9163923633969717,
      "grad_norm": 13.026142120361328,
      "learning_rate": 5.7764209234243365e-06,
      "loss": 1.9648,
      "step": 2911
    },
    {
      "epoch": 1.9170506912442398,
      "grad_norm": 21.955778121948242,
      "learning_rate": 5.770173011314188e-06,
      "loss": 2.2136,
      "step": 2912
    },
    {
      "epoch": 1.9177090190915076,
      "grad_norm": 1.8527675867080688,
      "learning_rate": 5.763927109519784e-06,
      "loss": 1.5575,
      "step": 2913
    },
    {
      "epoch": 1.9183673469387754,
      "grad_norm": 14.879541397094727,
      "learning_rate": 5.757683221009625e-06,
      "loss": 2.3346,
      "step": 2914
    },
    {
      "epoch": 1.9190256747860435,
      "grad_norm": 13.132174491882324,
      "learning_rate": 5.7514413487512625e-06,
      "loss": 1.8962,
      "step": 2915
    },
    {
      "epoch": 1.9196840026333115,
      "grad_norm": 15.252803802490234,
      "learning_rate": 5.745201495711267e-06,
      "loss": 2.0242,
      "step": 2916
    },
    {
      "epoch": 1.9203423304805793,
      "grad_norm": 1.762513518333435,
      "learning_rate": 5.738963664855269e-06,
      "loss": 1.5443,
      "step": 2917
    },
    {
      "epoch": 1.9210006583278472,
      "grad_norm": 21.50482749938965,
      "learning_rate": 5.732727859147931e-06,
      "loss": 1.7513,
      "step": 2918
    },
    {
      "epoch": 1.9216589861751152,
      "grad_norm": 22.98495101928711,
      "learning_rate": 5.726494081552949e-06,
      "loss": 1.906,
      "step": 2919
    },
    {
      "epoch": 1.9223173140223833,
      "grad_norm": 4.362451076507568,
      "learning_rate": 5.72026233503306e-06,
      "loss": 1.6374,
      "step": 2920
    },
    {
      "epoch": 1.922975641869651,
      "grad_norm": 14.440147399902344,
      "learning_rate": 5.7140326225500384e-06,
      "loss": 2.029,
      "step": 2921
    },
    {
      "epoch": 1.923633969716919,
      "grad_norm": 21.114004135131836,
      "learning_rate": 5.7078049470646854e-06,
      "loss": 1.9446,
      "step": 2922
    },
    {
      "epoch": 1.924292297564187,
      "grad_norm": 3.889057159423828,
      "learning_rate": 5.701579311536838e-06,
      "loss": 1.5417,
      "step": 2923
    },
    {
      "epoch": 1.924950625411455,
      "grad_norm": 10.707550048828125,
      "learning_rate": 5.695355718925366e-06,
      "loss": 1.9363,
      "step": 2924
    },
    {
      "epoch": 1.9256089532587228,
      "grad_norm": 13.789945602416992,
      "learning_rate": 5.689134172188157e-06,
      "loss": 1.7442,
      "step": 2925
    },
    {
      "epoch": 1.9262672811059907,
      "grad_norm": 2.017921209335327,
      "learning_rate": 5.682914674282138e-06,
      "loss": 1.5536,
      "step": 2926
    },
    {
      "epoch": 1.9269256089532587,
      "grad_norm": 9.976170539855957,
      "learning_rate": 5.676697228163261e-06,
      "loss": 1.7146,
      "step": 2927
    },
    {
      "epoch": 1.9275839368005268,
      "grad_norm": 31.252460479736328,
      "learning_rate": 5.670481836786496e-06,
      "loss": 2.2306,
      "step": 2928
    },
    {
      "epoch": 1.9282422646477946,
      "grad_norm": 3.680901527404785,
      "learning_rate": 5.664268503105845e-06,
      "loss": 1.5671,
      "step": 2929
    },
    {
      "epoch": 1.9289005924950624,
      "grad_norm": 4.817215442657471,
      "learning_rate": 5.658057230074329e-06,
      "loss": 1.6049,
      "step": 2930
    },
    {
      "epoch": 1.9295589203423305,
      "grad_norm": 13.62411880493164,
      "learning_rate": 5.65184802064398e-06,
      "loss": 2.2731,
      "step": 2931
    },
    {
      "epoch": 1.9302172481895985,
      "grad_norm": 4.415052890777588,
      "learning_rate": 5.645640877765863e-06,
      "loss": 1.629,
      "step": 2932
    },
    {
      "epoch": 1.9308755760368663,
      "grad_norm": 1.1139758825302124,
      "learning_rate": 5.639435804390057e-06,
      "loss": 1.5291,
      "step": 2933
    },
    {
      "epoch": 1.9315339038841342,
      "grad_norm": 11.838316917419434,
      "learning_rate": 5.6332328034656515e-06,
      "loss": 1.9362,
      "step": 2934
    },
    {
      "epoch": 1.9321922317314022,
      "grad_norm": 0.9741120338439941,
      "learning_rate": 5.627031877940759e-06,
      "loss": 1.5464,
      "step": 2935
    },
    {
      "epoch": 1.9328505595786702,
      "grad_norm": 8.564905166625977,
      "learning_rate": 5.620833030762503e-06,
      "loss": 1.7071,
      "step": 2936
    },
    {
      "epoch": 1.933508887425938,
      "grad_norm": 1.8676519393920898,
      "learning_rate": 5.614636264877013e-06,
      "loss": 1.5505,
      "step": 2937
    },
    {
      "epoch": 1.934167215273206,
      "grad_norm": 19.23802947998047,
      "learning_rate": 5.608441583229437e-06,
      "loss": 1.9476,
      "step": 2938
    },
    {
      "epoch": 1.934825543120474,
      "grad_norm": 17.603252410888672,
      "learning_rate": 5.6022489887639345e-06,
      "loss": 1.8185,
      "step": 2939
    },
    {
      "epoch": 1.935483870967742,
      "grad_norm": 18.291793823242188,
      "learning_rate": 5.5960584844236565e-06,
      "loss": 2.0741,
      "step": 2940
    },
    {
      "epoch": 1.93614219881501,
      "grad_norm": 6.338949203491211,
      "learning_rate": 5.589870073150785e-06,
      "loss": 1.5755,
      "step": 2941
    },
    {
      "epoch": 1.9368005266622779,
      "grad_norm": 9.895302772521973,
      "learning_rate": 5.583683757886489e-06,
      "loss": 1.8331,
      "step": 2942
    },
    {
      "epoch": 1.9374588545095457,
      "grad_norm": 13.301717758178711,
      "learning_rate": 5.577499541570954e-06,
      "loss": 2.143,
      "step": 2943
    },
    {
      "epoch": 1.9381171823568137,
      "grad_norm": 20.37342071533203,
      "learning_rate": 5.57131742714335e-06,
      "loss": 2.0434,
      "step": 2944
    },
    {
      "epoch": 1.9387755102040818,
      "grad_norm": 1.0446897745132446,
      "learning_rate": 5.565137417541866e-06,
      "loss": 1.539,
      "step": 2945
    },
    {
      "epoch": 1.9394338380513496,
      "grad_norm": 3.848099708557129,
      "learning_rate": 5.558959515703686e-06,
      "loss": 1.6055,
      "step": 2946
    },
    {
      "epoch": 1.9400921658986174,
      "grad_norm": 0.9439194202423096,
      "learning_rate": 5.5527837245649806e-06,
      "loss": 1.5416,
      "step": 2947
    },
    {
      "epoch": 1.9407504937458855,
      "grad_norm": 30.838573455810547,
      "learning_rate": 5.5466100470609306e-06,
      "loss": 3.4129,
      "step": 2948
    },
    {
      "epoch": 1.9414088215931535,
      "grad_norm": 1.4745014905929565,
      "learning_rate": 5.540438486125718e-06,
      "loss": 1.5427,
      "step": 2949
    },
    {
      "epoch": 1.9420671494404214,
      "grad_norm": 23.381685256958008,
      "learning_rate": 5.534269044692495e-06,
      "loss": 1.895,
      "step": 2950
    },
    {
      "epoch": 1.9427254772876892,
      "grad_norm": 2.047971487045288,
      "learning_rate": 5.5281017256934285e-06,
      "loss": 1.5489,
      "step": 2951
    },
    {
      "epoch": 1.9433838051349572,
      "grad_norm": 13.134793281555176,
      "learning_rate": 5.521936532059672e-06,
      "loss": 1.9815,
      "step": 2952
    },
    {
      "epoch": 1.9440421329822253,
      "grad_norm": 32.658267974853516,
      "learning_rate": 5.515773466721354e-06,
      "loss": 2.5677,
      "step": 2953
    },
    {
      "epoch": 1.944700460829493,
      "grad_norm": 3.0151562690734863,
      "learning_rate": 5.509612532607613e-06,
      "loss": 1.5663,
      "step": 2954
    },
    {
      "epoch": 1.945358788676761,
      "grad_norm": 1.1966142654418945,
      "learning_rate": 5.50345373264656e-06,
      "loss": 1.537,
      "step": 2955
    },
    {
      "epoch": 1.946017116524029,
      "grad_norm": 13.342829704284668,
      "learning_rate": 5.497297069765297e-06,
      "loss": 2.06,
      "step": 2956
    },
    {
      "epoch": 1.946675444371297,
      "grad_norm": 1.42286217212677,
      "learning_rate": 5.491142546889913e-06,
      "loss": 1.542,
      "step": 2957
    },
    {
      "epoch": 1.9473337722185649,
      "grad_norm": 0.834494411945343,
      "learning_rate": 5.484990166945476e-06,
      "loss": 1.5249,
      "step": 2958
    },
    {
      "epoch": 1.9479921000658327,
      "grad_norm": 47.34742736816406,
      "learning_rate": 5.478839932856033e-06,
      "loss": 3.6036,
      "step": 2959
    },
    {
      "epoch": 1.9486504279131007,
      "grad_norm": 1.2981117963790894,
      "learning_rate": 5.472691847544613e-06,
      "loss": 1.5505,
      "step": 2960
    },
    {
      "epoch": 1.9493087557603688,
      "grad_norm": 15.443299293518066,
      "learning_rate": 5.46654591393323e-06,
      "loss": 2.1304,
      "step": 2961
    },
    {
      "epoch": 1.9499670836076366,
      "grad_norm": 13.370450019836426,
      "learning_rate": 5.4604021349428685e-06,
      "loss": 2.0725,
      "step": 2962
    },
    {
      "epoch": 1.9506254114549044,
      "grad_norm": 8.482951164245605,
      "learning_rate": 5.454260513493488e-06,
      "loss": 1.8266,
      "step": 2963
    },
    {
      "epoch": 1.9512837393021725,
      "grad_norm": 14.005094528198242,
      "learning_rate": 5.448121052504034e-06,
      "loss": 2.026,
      "step": 2964
    },
    {
      "epoch": 1.9519420671494405,
      "grad_norm": 15.838217735290527,
      "learning_rate": 5.441983754892406e-06,
      "loss": 2.1398,
      "step": 2965
    },
    {
      "epoch": 1.9526003949967083,
      "grad_norm": 5.881808757781982,
      "learning_rate": 5.4358486235754915e-06,
      "loss": 1.7561,
      "step": 2966
    },
    {
      "epoch": 1.9532587228439762,
      "grad_norm": 37.59132766723633,
      "learning_rate": 5.429715661469142e-06,
      "loss": 3.1778,
      "step": 2967
    },
    {
      "epoch": 1.9539170506912442,
      "grad_norm": 24.10424041748047,
      "learning_rate": 5.423584871488179e-06,
      "loss": 2.4042,
      "step": 2968
    },
    {
      "epoch": 1.9545753785385123,
      "grad_norm": 15.240917205810547,
      "learning_rate": 5.41745625654639e-06,
      "loss": 1.9591,
      "step": 2969
    },
    {
      "epoch": 1.95523370638578,
      "grad_norm": 22.46683120727539,
      "learning_rate": 5.411329819556538e-06,
      "loss": 1.9988,
      "step": 2970
    },
    {
      "epoch": 1.955892034233048,
      "grad_norm": 5.750638484954834,
      "learning_rate": 5.405205563430331e-06,
      "loss": 1.7317,
      "step": 2971
    },
    {
      "epoch": 1.956550362080316,
      "grad_norm": 17.261571884155273,
      "learning_rate": 5.399083491078461e-06,
      "loss": 2.5156,
      "step": 2972
    },
    {
      "epoch": 1.957208689927584,
      "grad_norm": 11.606846809387207,
      "learning_rate": 5.3929636054105745e-06,
      "loss": 1.967,
      "step": 2973
    },
    {
      "epoch": 1.9578670177748518,
      "grad_norm": 16.1831111907959,
      "learning_rate": 5.386845909335268e-06,
      "loss": 1.9268,
      "step": 2974
    },
    {
      "epoch": 1.9585253456221197,
      "grad_norm": 0.7563111782073975,
      "learning_rate": 5.380730405760118e-06,
      "loss": 1.5246,
      "step": 2975
    },
    {
      "epoch": 1.9591836734693877,
      "grad_norm": 17.236797332763672,
      "learning_rate": 5.37461709759165e-06,
      "loss": 2.0632,
      "step": 2976
    },
    {
      "epoch": 1.9598420013166558,
      "grad_norm": 21.777318954467773,
      "learning_rate": 5.368505987735334e-06,
      "loss": 2.047,
      "step": 2977
    },
    {
      "epoch": 1.9605003291639236,
      "grad_norm": 20.35896873474121,
      "learning_rate": 5.362397079095611e-06,
      "loss": 1.9719,
      "step": 2978
    },
    {
      "epoch": 1.9611586570111914,
      "grad_norm": 1.6001228094100952,
      "learning_rate": 5.356290374575876e-06,
      "loss": 1.545,
      "step": 2979
    },
    {
      "epoch": 1.9618169848584595,
      "grad_norm": 1.4837076663970947,
      "learning_rate": 5.350185877078462e-06,
      "loss": 1.527,
      "step": 2980
    },
    {
      "epoch": 1.9624753127057275,
      "grad_norm": 25.68528938293457,
      "learning_rate": 5.34408358950466e-06,
      "loss": 1.9725,
      "step": 2981
    },
    {
      "epoch": 1.9631336405529956,
      "grad_norm": 2.8743515014648438,
      "learning_rate": 5.337983514754723e-06,
      "loss": 1.5537,
      "step": 2982
    },
    {
      "epoch": 1.9637919684002634,
      "grad_norm": 3.2780849933624268,
      "learning_rate": 5.331885655727842e-06,
      "loss": 1.6017,
      "step": 2983
    },
    {
      "epoch": 1.9644502962475312,
      "grad_norm": 7.424475193023682,
      "learning_rate": 5.325790015322145e-06,
      "loss": 1.7723,
      "step": 2984
    },
    {
      "epoch": 1.9651086240947993,
      "grad_norm": 2.1364481449127197,
      "learning_rate": 5.319696596434723e-06,
      "loss": 1.5458,
      "step": 2985
    },
    {
      "epoch": 1.9657669519420673,
      "grad_norm": 6.093891620635986,
      "learning_rate": 5.3136054019616025e-06,
      "loss": 1.645,
      "step": 2986
    },
    {
      "epoch": 1.9664252797893351,
      "grad_norm": 1.2413673400878906,
      "learning_rate": 5.3075164347977506e-06,
      "loss": 1.5337,
      "step": 2987
    },
    {
      "epoch": 1.967083607636603,
      "grad_norm": 1.180930495262146,
      "learning_rate": 5.301429697837077e-06,
      "loss": 1.5345,
      "step": 2988
    },
    {
      "epoch": 1.967741935483871,
      "grad_norm": 12.810670852661133,
      "learning_rate": 5.295345193972445e-06,
      "loss": 1.9892,
      "step": 2989
    },
    {
      "epoch": 1.968400263331139,
      "grad_norm": 12.077692985534668,
      "learning_rate": 5.289262926095635e-06,
      "loss": 1.7529,
      "step": 2990
    },
    {
      "epoch": 1.9690585911784069,
      "grad_norm": 7.848756790161133,
      "learning_rate": 5.283182897097379e-06,
      "loss": 1.7812,
      "step": 2991
    },
    {
      "epoch": 1.9697169190256747,
      "grad_norm": 9.974495887756348,
      "learning_rate": 5.277105109867343e-06,
      "loss": 1.8829,
      "step": 2992
    },
    {
      "epoch": 1.9703752468729427,
      "grad_norm": 23.22978973388672,
      "learning_rate": 5.271029567294119e-06,
      "loss": 2.1845,
      "step": 2993
    },
    {
      "epoch": 1.9710335747202108,
      "grad_norm": 20.022811889648438,
      "learning_rate": 5.26495627226524e-06,
      "loss": 2.0849,
      "step": 2994
    },
    {
      "epoch": 1.9716919025674786,
      "grad_norm": 1.485155463218689,
      "learning_rate": 5.2588852276671744e-06,
      "loss": 1.5317,
      "step": 2995
    },
    {
      "epoch": 1.9723502304147464,
      "grad_norm": 2.4141032695770264,
      "learning_rate": 5.252816436385311e-06,
      "loss": 1.5644,
      "step": 2996
    },
    {
      "epoch": 1.9730085582620145,
      "grad_norm": 9.940485000610352,
      "learning_rate": 5.2467499013039745e-06,
      "loss": 1.8774,
      "step": 2997
    },
    {
      "epoch": 1.9736668861092825,
      "grad_norm": 11.318877220153809,
      "learning_rate": 5.240685625306421e-06,
      "loss": 1.871,
      "step": 2998
    },
    {
      "epoch": 1.9743252139565504,
      "grad_norm": 0.677824079990387,
      "learning_rate": 5.2346236112748165e-06,
      "loss": 1.5258,
      "step": 2999
    },
    {
      "epoch": 1.9749835418038182,
      "grad_norm": 18.693323135375977,
      "learning_rate": 5.228563862090268e-06,
      "loss": 2.0525,
      "step": 3000
    },
    {
      "epoch": 1.9756418696510862,
      "grad_norm": 15.841072082519531,
      "learning_rate": 5.222506380632802e-06,
      "loss": 2.1177,
      "step": 3001
    },
    {
      "epoch": 1.9763001974983543,
      "grad_norm": 1.4510551691055298,
      "learning_rate": 5.216451169781364e-06,
      "loss": 1.5509,
      "step": 3002
    },
    {
      "epoch": 1.976958525345622,
      "grad_norm": 10.317154884338379,
      "learning_rate": 5.210398232413824e-06,
      "loss": 1.8787,
      "step": 3003
    },
    {
      "epoch": 1.97761685319289,
      "grad_norm": 12.877923011779785,
      "learning_rate": 5.204347571406976e-06,
      "loss": 1.9848,
      "step": 3004
    },
    {
      "epoch": 1.978275181040158,
      "grad_norm": 11.95698356628418,
      "learning_rate": 5.198299189636513e-06,
      "loss": 1.8896,
      "step": 3005
    },
    {
      "epoch": 1.978933508887426,
      "grad_norm": 1.4249556064605713,
      "learning_rate": 5.192253089977066e-06,
      "loss": 1.5412,
      "step": 3006
    },
    {
      "epoch": 1.9795918367346939,
      "grad_norm": 1.275725245475769,
      "learning_rate": 5.186209275302175e-06,
      "loss": 1.5448,
      "step": 3007
    },
    {
      "epoch": 1.9802501645819617,
      "grad_norm": 1.4741076231002808,
      "learning_rate": 5.180167748484281e-06,
      "loss": 1.5383,
      "step": 3008
    },
    {
      "epoch": 1.9809084924292297,
      "grad_norm": 15.52432632446289,
      "learning_rate": 5.174128512394762e-06,
      "loss": 1.9637,
      "step": 3009
    },
    {
      "epoch": 1.9815668202764978,
      "grad_norm": 1.7994718551635742,
      "learning_rate": 5.168091569903892e-06,
      "loss": 1.5417,
      "step": 3010
    },
    {
      "epoch": 1.9822251481237656,
      "grad_norm": 3.274019241333008,
      "learning_rate": 5.16205692388085e-06,
      "loss": 1.6029,
      "step": 3011
    },
    {
      "epoch": 1.9828834759710334,
      "grad_norm": 15.490240097045898,
      "learning_rate": 5.156024577193733e-06,
      "loss": 1.9063,
      "step": 3012
    },
    {
      "epoch": 1.9835418038183015,
      "grad_norm": 1.932360291481018,
      "learning_rate": 5.14999453270955e-06,
      "loss": 1.5368,
      "step": 3013
    },
    {
      "epoch": 1.9842001316655695,
      "grad_norm": 1.5038807392120361,
      "learning_rate": 5.1439667932941995e-06,
      "loss": 1.5345,
      "step": 3014
    },
    {
      "epoch": 1.9848584595128373,
      "grad_norm": 15.6879301071167,
      "learning_rate": 5.137941361812491e-06,
      "loss": 2.0335,
      "step": 3015
    },
    {
      "epoch": 1.9855167873601052,
      "grad_norm": 4.625171184539795,
      "learning_rate": 5.131918241128156e-06,
      "loss": 1.7164,
      "step": 3016
    },
    {
      "epoch": 1.9861751152073732,
      "grad_norm": 0.9149374961853027,
      "learning_rate": 5.125897434103795e-06,
      "loss": 1.5316,
      "step": 3017
    },
    {
      "epoch": 1.9868334430546413,
      "grad_norm": 17.895719528198242,
      "learning_rate": 5.119878943600933e-06,
      "loss": 2.5001,
      "step": 3018
    },
    {
      "epoch": 1.9874917709019093,
      "grad_norm": 6.770808219909668,
      "learning_rate": 5.113862772479985e-06,
      "loss": 1.7355,
      "step": 3019
    },
    {
      "epoch": 1.9881500987491771,
      "grad_norm": 12.265523910522461,
      "learning_rate": 5.107848923600267e-06,
      "loss": 1.9191,
      "step": 3020
    },
    {
      "epoch": 1.988808426596445,
      "grad_norm": 7.597638130187988,
      "learning_rate": 5.1018373998199845e-06,
      "loss": 1.7478,
      "step": 3021
    },
    {
      "epoch": 1.989466754443713,
      "grad_norm": 5.236419677734375,
      "learning_rate": 5.0958282039962415e-06,
      "loss": 1.6193,
      "step": 3022
    },
    {
      "epoch": 1.990125082290981,
      "grad_norm": 0.6255812048912048,
      "learning_rate": 5.0898213389850486e-06,
      "loss": 1.5224,
      "step": 3023
    },
    {
      "epoch": 1.9907834101382489,
      "grad_norm": 27.108535766601562,
      "learning_rate": 5.0838168076412844e-06,
      "loss": 3.5622,
      "step": 3024
    },
    {
      "epoch": 1.9914417379855167,
      "grad_norm": 7.297523021697998,
      "learning_rate": 5.077814612818737e-06,
      "loss": 1.7904,
      "step": 3025
    },
    {
      "epoch": 1.9921000658327848,
      "grad_norm": 21.051977157592773,
      "learning_rate": 5.071814757370079e-06,
      "loss": 1.7945,
      "step": 3026
    },
    {
      "epoch": 1.9927583936800528,
      "grad_norm": 1.0070123672485352,
      "learning_rate": 5.065817244146864e-06,
      "loss": 1.5322,
      "step": 3027
    },
    {
      "epoch": 1.9934167215273206,
      "grad_norm": 18.57644271850586,
      "learning_rate": 5.059822075999541e-06,
      "loss": 2.3239,
      "step": 3028
    },
    {
      "epoch": 1.9940750493745885,
      "grad_norm": 5.343885898590088,
      "learning_rate": 5.053829255777443e-06,
      "loss": 1.6691,
      "step": 3029
    },
    {
      "epoch": 1.9947333772218565,
      "grad_norm": 12.420957565307617,
      "learning_rate": 5.047838786328786e-06,
      "loss": 1.8578,
      "step": 3030
    },
    {
      "epoch": 1.9953917050691246,
      "grad_norm": 21.966176986694336,
      "learning_rate": 5.0418506705006676e-06,
      "loss": 2.0407,
      "step": 3031
    },
    {
      "epoch": 1.9960500329163924,
      "grad_norm": 11.090127944946289,
      "learning_rate": 5.035864911139074e-06,
      "loss": 1.8153,
      "step": 3032
    },
    {
      "epoch": 1.9967083607636602,
      "grad_norm": 1.3543353080749512,
      "learning_rate": 5.0298815110888566e-06,
      "loss": 1.533,
      "step": 3033
    },
    {
      "epoch": 1.9973666886109283,
      "grad_norm": 7.976407527923584,
      "learning_rate": 5.023900473193757e-06,
      "loss": 1.8321,
      "step": 3034
    },
    {
      "epoch": 1.9980250164581963,
      "grad_norm": 15.347561836242676,
      "learning_rate": 5.0179218002963905e-06,
      "loss": 2.159,
      "step": 3035
    },
    {
      "epoch": 1.9986833443054641,
      "grad_norm": 0.5506375432014465,
      "learning_rate": 5.011945495238253e-06,
      "loss": 1.5259,
      "step": 3036
    },
    {
      "epoch": 1.999341672152732,
      "grad_norm": 1.887793779373169,
      "learning_rate": 5.005971560859708e-06,
      "loss": 1.5395,
      "step": 3037
    },
    {
      "epoch": 2.0,
      "grad_norm": 16.70285415649414,
      "learning_rate": 5.000000000000003e-06,
      "loss": 1.784,
      "step": 3038
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.5443786982248521,
      "eval_loss": 1.9320577383041382,
      "eval_runtime": 87.3775,
      "eval_samples_per_second": 1.934,
      "eval_steps_per_second": 1.934,
      "step": 3038
    },
    {
      "epoch": 2.000658327847268,
      "grad_norm": 13.639501571655273,
      "learning_rate": 4.9940308154972384e-06,
      "loss": 2.0929,
      "step": 3039
    },
    {
      "epoch": 2.001316655694536,
      "grad_norm": 2.527225971221924,
      "learning_rate": 4.9880640101884016e-06,
      "loss": 1.552,
      "step": 3040
    },
    {
      "epoch": 2.0019749835418037,
      "grad_norm": 12.708702087402344,
      "learning_rate": 4.982099586909346e-06,
      "loss": 2.0394,
      "step": 3041
    },
    {
      "epoch": 2.0026333113890717,
      "grad_norm": 17.1424503326416,
      "learning_rate": 4.976137548494791e-06,
      "loss": 1.9065,
      "step": 3042
    },
    {
      "epoch": 2.00329163923634,
      "grad_norm": 0.9494034647941589,
      "learning_rate": 4.970177897778321e-06,
      "loss": 1.5226,
      "step": 3043
    },
    {
      "epoch": 2.003949967083608,
      "grad_norm": 1.2334089279174805,
      "learning_rate": 4.964220637592394e-06,
      "loss": 1.5403,
      "step": 3044
    },
    {
      "epoch": 2.0046082949308754,
      "grad_norm": 16.996387481689453,
      "learning_rate": 4.958265770768315e-06,
      "loss": 1.8771,
      "step": 3045
    },
    {
      "epoch": 2.0052666227781435,
      "grad_norm": 29.34343147277832,
      "learning_rate": 4.952313300136268e-06,
      "loss": 2.2943,
      "step": 3046
    },
    {
      "epoch": 2.0059249506254115,
      "grad_norm": 16.749658584594727,
      "learning_rate": 4.946363228525295e-06,
      "loss": 1.887,
      "step": 3047
    },
    {
      "epoch": 2.0065832784726796,
      "grad_norm": 11.362223625183105,
      "learning_rate": 4.940415558763286e-06,
      "loss": 1.8163,
      "step": 3048
    },
    {
      "epoch": 2.007241606319947,
      "grad_norm": 8.04216480255127,
      "learning_rate": 4.934470293676999e-06,
      "loss": 1.7838,
      "step": 3049
    },
    {
      "epoch": 2.0078999341672152,
      "grad_norm": 8.16796875,
      "learning_rate": 4.928527436092062e-06,
      "loss": 1.7245,
      "step": 3050
    },
    {
      "epoch": 2.0085582620144833,
      "grad_norm": 36.22764587402344,
      "learning_rate": 4.922586988832931e-06,
      "loss": 2.3982,
      "step": 3051
    },
    {
      "epoch": 2.0092165898617513,
      "grad_norm": 21.932321548461914,
      "learning_rate": 4.916648954722935e-06,
      "loss": 2.3932,
      "step": 3052
    },
    {
      "epoch": 2.009874917709019,
      "grad_norm": 26.10807228088379,
      "learning_rate": 4.910713336584254e-06,
      "loss": 2.226,
      "step": 3053
    },
    {
      "epoch": 2.010533245556287,
      "grad_norm": 20.445466995239258,
      "learning_rate": 4.904780137237913e-06,
      "loss": 2.1284,
      "step": 3054
    },
    {
      "epoch": 2.011191573403555,
      "grad_norm": 31.831087112426758,
      "learning_rate": 4.898849359503795e-06,
      "loss": 2.3286,
      "step": 3055
    },
    {
      "epoch": 2.011849901250823,
      "grad_norm": 18.381410598754883,
      "learning_rate": 4.892921006200629e-06,
      "loss": 2.7164,
      "step": 3056
    },
    {
      "epoch": 2.0125082290980907,
      "grad_norm": 1.9108407497406006,
      "learning_rate": 4.8869950801459886e-06,
      "loss": 1.5497,
      "step": 3057
    },
    {
      "epoch": 2.0131665569453587,
      "grad_norm": 1.324478030204773,
      "learning_rate": 4.8810715841563004e-06,
      "loss": 1.5417,
      "step": 3058
    },
    {
      "epoch": 2.013824884792627,
      "grad_norm": 14.248045921325684,
      "learning_rate": 4.875150521046832e-06,
      "loss": 1.8425,
      "step": 3059
    },
    {
      "epoch": 2.014483212639895,
      "grad_norm": 1.6631959676742554,
      "learning_rate": 4.869231893631699e-06,
      "loss": 1.537,
      "step": 3060
    },
    {
      "epoch": 2.0151415404871624,
      "grad_norm": 6.1383867263793945,
      "learning_rate": 4.863315704723846e-06,
      "loss": 1.7167,
      "step": 3061
    },
    {
      "epoch": 2.0157998683344305,
      "grad_norm": 24.596406936645508,
      "learning_rate": 4.857401957135075e-06,
      "loss": 2.2629,
      "step": 3062
    },
    {
      "epoch": 2.0164581961816985,
      "grad_norm": 8.252472877502441,
      "learning_rate": 4.851490653676019e-06,
      "loss": 1.8021,
      "step": 3063
    },
    {
      "epoch": 2.0171165240289666,
      "grad_norm": 9.028497695922852,
      "learning_rate": 4.845581797156153e-06,
      "loss": 1.9574,
      "step": 3064
    },
    {
      "epoch": 2.017774851876234,
      "grad_norm": 3.608157157897949,
      "learning_rate": 4.839675390383786e-06,
      "loss": 1.6565,
      "step": 3065
    },
    {
      "epoch": 2.0184331797235022,
      "grad_norm": 8.720467567443848,
      "learning_rate": 4.83377143616607e-06,
      "loss": 1.684,
      "step": 3066
    },
    {
      "epoch": 2.0190915075707703,
      "grad_norm": 22.093141555786133,
      "learning_rate": 4.827869937308974e-06,
      "loss": 1.8595,
      "step": 3067
    },
    {
      "epoch": 2.0197498354180383,
      "grad_norm": 2.3878297805786133,
      "learning_rate": 4.821970896617319e-06,
      "loss": 1.5459,
      "step": 3068
    },
    {
      "epoch": 2.020408163265306,
      "grad_norm": 20.078405380249023,
      "learning_rate": 4.81607431689475e-06,
      "loss": 1.7709,
      "step": 3069
    },
    {
      "epoch": 2.021066491112574,
      "grad_norm": 0.9748732447624207,
      "learning_rate": 4.8101802009437415e-06,
      "loss": 1.5352,
      "step": 3070
    },
    {
      "epoch": 2.021724818959842,
      "grad_norm": 1.9508347511291504,
      "learning_rate": 4.804288551565598e-06,
      "loss": 1.5499,
      "step": 3071
    },
    {
      "epoch": 2.02238314680711,
      "grad_norm": 7.948154926300049,
      "learning_rate": 4.798399371560457e-06,
      "loss": 1.7947,
      "step": 3072
    },
    {
      "epoch": 2.0230414746543777,
      "grad_norm": 14.389053344726562,
      "learning_rate": 4.792512663727268e-06,
      "loss": 1.7596,
      "step": 3073
    },
    {
      "epoch": 2.0236998025016457,
      "grad_norm": 15.927227973937988,
      "learning_rate": 4.78662843086382e-06,
      "loss": 1.8866,
      "step": 3074
    },
    {
      "epoch": 2.0243581303489138,
      "grad_norm": 26.902637481689453,
      "learning_rate": 4.780746675766719e-06,
      "loss": 2.7453,
      "step": 3075
    },
    {
      "epoch": 2.025016458196182,
      "grad_norm": 4.473295211791992,
      "learning_rate": 4.774867401231396e-06,
      "loss": 1.6273,
      "step": 3076
    },
    {
      "epoch": 2.0256747860434494,
      "grad_norm": 23.604036331176758,
      "learning_rate": 4.7689906100521036e-06,
      "loss": 2.7277,
      "step": 3077
    },
    {
      "epoch": 2.0263331138907175,
      "grad_norm": 16.653106689453125,
      "learning_rate": 4.763116305021912e-06,
      "loss": 2.2159,
      "step": 3078
    },
    {
      "epoch": 2.0269914417379855,
      "grad_norm": 4.712000370025635,
      "learning_rate": 4.757244488932707e-06,
      "loss": 1.5724,
      "step": 3079
    },
    {
      "epoch": 2.0276497695852536,
      "grad_norm": 1.2676854133605957,
      "learning_rate": 4.7513751645751956e-06,
      "loss": 1.5272,
      "step": 3080
    },
    {
      "epoch": 2.0283080974325216,
      "grad_norm": 16.075393676757812,
      "learning_rate": 4.745508334738906e-06,
      "loss": 2.0974,
      "step": 3081
    },
    {
      "epoch": 2.028966425279789,
      "grad_norm": 15.207296371459961,
      "learning_rate": 4.739644002212161e-06,
      "loss": 1.931,
      "step": 3082
    },
    {
      "epoch": 2.0296247531270573,
      "grad_norm": 1.452094554901123,
      "learning_rate": 4.733782169782124e-06,
      "loss": 1.543,
      "step": 3083
    },
    {
      "epoch": 2.0302830809743253,
      "grad_norm": 2.28922438621521,
      "learning_rate": 4.727922840234753e-06,
      "loss": 1.5863,
      "step": 3084
    },
    {
      "epoch": 2.0309414088215934,
      "grad_norm": 12.025707244873047,
      "learning_rate": 4.722066016354815e-06,
      "loss": 1.9935,
      "step": 3085
    },
    {
      "epoch": 2.031599736668861,
      "grad_norm": 1.6463404893875122,
      "learning_rate": 4.716211700925892e-06,
      "loss": 1.5436,
      "step": 3086
    },
    {
      "epoch": 2.032258064516129,
      "grad_norm": 2.7269296646118164,
      "learning_rate": 4.710359896730379e-06,
      "loss": 1.5539,
      "step": 3087
    },
    {
      "epoch": 2.032916392363397,
      "grad_norm": 11.05787467956543,
      "learning_rate": 4.704510606549462e-06,
      "loss": 1.911,
      "step": 3088
    },
    {
      "epoch": 2.033574720210665,
      "grad_norm": 16.88355827331543,
      "learning_rate": 4.698663833163142e-06,
      "loss": 2.0749,
      "step": 3089
    },
    {
      "epoch": 2.0342330480579327,
      "grad_norm": 10.494444847106934,
      "learning_rate": 4.692819579350233e-06,
      "loss": 1.8503,
      "step": 3090
    },
    {
      "epoch": 2.0348913759052007,
      "grad_norm": 1.0769222974777222,
      "learning_rate": 4.686977847888332e-06,
      "loss": 1.5237,
      "step": 3091
    },
    {
      "epoch": 2.035549703752469,
      "grad_norm": 7.235124111175537,
      "learning_rate": 4.681138641553849e-06,
      "loss": 1.6731,
      "step": 3092
    },
    {
      "epoch": 2.036208031599737,
      "grad_norm": 1.1163665056228638,
      "learning_rate": 4.675301963121998e-06,
      "loss": 1.5228,
      "step": 3093
    },
    {
      "epoch": 2.0368663594470044,
      "grad_norm": 4.34904670715332,
      "learning_rate": 4.669467815366776e-06,
      "loss": 1.691,
      "step": 3094
    },
    {
      "epoch": 2.0375246872942725,
      "grad_norm": 8.764323234558105,
      "learning_rate": 4.663636201060989e-06,
      "loss": 1.7772,
      "step": 3095
    },
    {
      "epoch": 2.0381830151415405,
      "grad_norm": 14.552464485168457,
      "learning_rate": 4.657807122976239e-06,
      "loss": 1.7004,
      "step": 3096
    },
    {
      "epoch": 2.0388413429888086,
      "grad_norm": 0.7668858766555786,
      "learning_rate": 4.651980583882919e-06,
      "loss": 1.5206,
      "step": 3097
    },
    {
      "epoch": 2.039499670836076,
      "grad_norm": 0.9999001026153564,
      "learning_rate": 4.646156586550216e-06,
      "loss": 1.5261,
      "step": 3098
    },
    {
      "epoch": 2.0401579986833442,
      "grad_norm": 6.58170223236084,
      "learning_rate": 4.640335133746109e-06,
      "loss": 1.9062,
      "step": 3099
    },
    {
      "epoch": 2.0408163265306123,
      "grad_norm": 6.332595348358154,
      "learning_rate": 4.634516228237372e-06,
      "loss": 1.5721,
      "step": 3100
    },
    {
      "epoch": 2.0414746543778803,
      "grad_norm": 5.563839912414551,
      "learning_rate": 4.628699872789558e-06,
      "loss": 1.6444,
      "step": 3101
    },
    {
      "epoch": 2.042132982225148,
      "grad_norm": 5.678096771240234,
      "learning_rate": 4.622886070167018e-06,
      "loss": 1.7042,
      "step": 3102
    },
    {
      "epoch": 2.042791310072416,
      "grad_norm": 7.145232677459717,
      "learning_rate": 4.6170748231328834e-06,
      "loss": 1.8479,
      "step": 3103
    },
    {
      "epoch": 2.043449637919684,
      "grad_norm": 1.0650992393493652,
      "learning_rate": 4.611266134449076e-06,
      "loss": 1.5248,
      "step": 3104
    },
    {
      "epoch": 2.044107965766952,
      "grad_norm": 1.5028352737426758,
      "learning_rate": 4.605460006876299e-06,
      "loss": 1.5318,
      "step": 3105
    },
    {
      "epoch": 2.0447662936142197,
      "grad_norm": 19.511499404907227,
      "learning_rate": 4.599656443174042e-06,
      "loss": 2.4508,
      "step": 3106
    },
    {
      "epoch": 2.0454246214614877,
      "grad_norm": 13.78318977355957,
      "learning_rate": 4.593855446100565e-06,
      "loss": 1.8982,
      "step": 3107
    },
    {
      "epoch": 2.046082949308756,
      "grad_norm": 21.318775177001953,
      "learning_rate": 4.588057018412922e-06,
      "loss": 1.9805,
      "step": 3108
    },
    {
      "epoch": 2.046741277156024,
      "grad_norm": 9.70817756652832,
      "learning_rate": 4.582261162866937e-06,
      "loss": 1.9014,
      "step": 3109
    },
    {
      "epoch": 2.0473996050032914,
      "grad_norm": 26.32903289794922,
      "learning_rate": 4.576467882217215e-06,
      "loss": 1.8479,
      "step": 3110
    },
    {
      "epoch": 2.0480579328505595,
      "grad_norm": 22.83112144470215,
      "learning_rate": 4.5706771792171365e-06,
      "loss": 2.4012,
      "step": 3111
    },
    {
      "epoch": 2.0487162606978275,
      "grad_norm": 12.637843132019043,
      "learning_rate": 4.564889056618862e-06,
      "loss": 1.6857,
      "step": 3112
    },
    {
      "epoch": 2.0493745885450956,
      "grad_norm": 19.71337890625,
      "learning_rate": 4.5591035171733115e-06,
      "loss": 1.9909,
      "step": 3113
    },
    {
      "epoch": 2.050032916392363,
      "grad_norm": 9.538995742797852,
      "learning_rate": 4.553320563630191e-06,
      "loss": 1.8941,
      "step": 3114
    },
    {
      "epoch": 2.0506912442396312,
      "grad_norm": 8.018773078918457,
      "learning_rate": 4.547540198737976e-06,
      "loss": 1.8755,
      "step": 3115
    },
    {
      "epoch": 2.0513495720868993,
      "grad_norm": 8.178069114685059,
      "learning_rate": 4.541762425243899e-06,
      "loss": 1.8444,
      "step": 3116
    },
    {
      "epoch": 2.0520078999341673,
      "grad_norm": 1.2305573225021362,
      "learning_rate": 4.535987245893979e-06,
      "loss": 1.5342,
      "step": 3117
    },
    {
      "epoch": 2.0526662277814354,
      "grad_norm": 2.377352714538574,
      "learning_rate": 4.5302146634329945e-06,
      "loss": 1.6,
      "step": 3118
    },
    {
      "epoch": 2.053324555628703,
      "grad_norm": 15.1307954788208,
      "learning_rate": 4.524444680604483e-06,
      "loss": 2.3556,
      "step": 3119
    },
    {
      "epoch": 2.053982883475971,
      "grad_norm": 2.785667657852173,
      "learning_rate": 4.518677300150755e-06,
      "loss": 1.5923,
      "step": 3120
    },
    {
      "epoch": 2.054641211323239,
      "grad_norm": 0.8210690021514893,
      "learning_rate": 4.512912524812883e-06,
      "loss": 1.5214,
      "step": 3121
    },
    {
      "epoch": 2.055299539170507,
      "grad_norm": 4.276653289794922,
      "learning_rate": 4.507150357330695e-06,
      "loss": 1.5987,
      "step": 3122
    },
    {
      "epoch": 2.0559578670177747,
      "grad_norm": 4.285785675048828,
      "learning_rate": 4.501390800442783e-06,
      "loss": 1.5669,
      "step": 3123
    },
    {
      "epoch": 2.0566161948650428,
      "grad_norm": 0.9206286072731018,
      "learning_rate": 4.495633856886512e-06,
      "loss": 1.5321,
      "step": 3124
    },
    {
      "epoch": 2.057274522712311,
      "grad_norm": 12.542604446411133,
      "learning_rate": 4.48987952939798e-06,
      "loss": 1.8845,
      "step": 3125
    },
    {
      "epoch": 2.057932850559579,
      "grad_norm": 1.0550787448883057,
      "learning_rate": 4.48412782071206e-06,
      "loss": 1.5298,
      "step": 3126
    },
    {
      "epoch": 2.0585911784068465,
      "grad_norm": 1.4131797552108765,
      "learning_rate": 4.478378733562375e-06,
      "loss": 1.5501,
      "step": 3127
    },
    {
      "epoch": 2.0592495062541145,
      "grad_norm": 20.032989501953125,
      "learning_rate": 4.472632270681297e-06,
      "loss": 2.2669,
      "step": 3128
    },
    {
      "epoch": 2.0599078341013826,
      "grad_norm": 24.48497772216797,
      "learning_rate": 4.466888434799959e-06,
      "loss": 2.1598,
      "step": 3129
    },
    {
      "epoch": 2.0605661619486506,
      "grad_norm": 5.375173091888428,
      "learning_rate": 4.4611472286482396e-06,
      "loss": 1.6048,
      "step": 3130
    },
    {
      "epoch": 2.061224489795918,
      "grad_norm": 1.3937276601791382,
      "learning_rate": 4.455408654954771e-06,
      "loss": 1.5291,
      "step": 3131
    },
    {
      "epoch": 2.0618828176431863,
      "grad_norm": 1.104659080505371,
      "learning_rate": 4.449672716446936e-06,
      "loss": 1.5264,
      "step": 3132
    },
    {
      "epoch": 2.0625411454904543,
      "grad_norm": 1.0569007396697998,
      "learning_rate": 4.443939415850861e-06,
      "loss": 1.531,
      "step": 3133
    },
    {
      "epoch": 2.0631994733377224,
      "grad_norm": 1.5685820579528809,
      "learning_rate": 4.438208755891415e-06,
      "loss": 1.5361,
      "step": 3134
    },
    {
      "epoch": 2.06385780118499,
      "grad_norm": 25.086936950683594,
      "learning_rate": 4.432480739292219e-06,
      "loss": 1.954,
      "step": 3135
    },
    {
      "epoch": 2.064516129032258,
      "grad_norm": 8.922459602355957,
      "learning_rate": 4.426755368775637e-06,
      "loss": 1.988,
      "step": 3136
    },
    {
      "epoch": 2.065174456879526,
      "grad_norm": 6.256037712097168,
      "learning_rate": 4.421032647062773e-06,
      "loss": 1.7453,
      "step": 3137
    },
    {
      "epoch": 2.065832784726794,
      "grad_norm": 10.629599571228027,
      "learning_rate": 4.415312576873473e-06,
      "loss": 1.8524,
      "step": 3138
    },
    {
      "epoch": 2.0664911125740617,
      "grad_norm": 29.172855377197266,
      "learning_rate": 4.409595160926323e-06,
      "loss": 2.3537,
      "step": 3139
    },
    {
      "epoch": 2.0671494404213298,
      "grad_norm": 11.772259712219238,
      "learning_rate": 4.40388040193865e-06,
      "loss": 1.7135,
      "step": 3140
    },
    {
      "epoch": 2.067807768268598,
      "grad_norm": 5.9637627601623535,
      "learning_rate": 4.39816830262651e-06,
      "loss": 1.5771,
      "step": 3141
    },
    {
      "epoch": 2.068466096115866,
      "grad_norm": 1.886681079864502,
      "learning_rate": 4.392458865704702e-06,
      "loss": 1.5453,
      "step": 3142
    },
    {
      "epoch": 2.0691244239631335,
      "grad_norm": 14.702670097351074,
      "learning_rate": 4.386752093886758e-06,
      "loss": 1.7537,
      "step": 3143
    },
    {
      "epoch": 2.0697827518104015,
      "grad_norm": 10.208102226257324,
      "learning_rate": 4.381047989884945e-06,
      "loss": 1.8824,
      "step": 3144
    },
    {
      "epoch": 2.0704410796576695,
      "grad_norm": 11.488174438476562,
      "learning_rate": 4.375346556410259e-06,
      "loss": 1.907,
      "step": 3145
    },
    {
      "epoch": 2.0710994075049376,
      "grad_norm": 14.031866073608398,
      "learning_rate": 4.369647796172433e-06,
      "loss": 1.924,
      "step": 3146
    },
    {
      "epoch": 2.071757735352205,
      "grad_norm": 0.9744650721549988,
      "learning_rate": 4.3639517118799155e-06,
      "loss": 1.5322,
      "step": 3147
    },
    {
      "epoch": 2.0724160631994732,
      "grad_norm": 22.602706909179688,
      "learning_rate": 4.358258306239899e-06,
      "loss": 2.2431,
      "step": 3148
    },
    {
      "epoch": 2.0730743910467413,
      "grad_norm": 42.162513732910156,
      "learning_rate": 4.352567581958295e-06,
      "loss": 2.3774,
      "step": 3149
    },
    {
      "epoch": 2.0737327188940093,
      "grad_norm": 21.200361251831055,
      "learning_rate": 4.34687954173974e-06,
      "loss": 2.2389,
      "step": 3150
    },
    {
      "epoch": 2.074391046741277,
      "grad_norm": 2.3093581199645996,
      "learning_rate": 4.341194188287599e-06,
      "loss": 1.5821,
      "step": 3151
    },
    {
      "epoch": 2.075049374588545,
      "grad_norm": 49.87071228027344,
      "learning_rate": 4.3355115243039604e-06,
      "loss": 2.9333,
      "step": 3152
    },
    {
      "epoch": 2.075707702435813,
      "grad_norm": 1.8391978740692139,
      "learning_rate": 4.329831552489626e-06,
      "loss": 1.5407,
      "step": 3153
    },
    {
      "epoch": 2.076366030283081,
      "grad_norm": 14.62883472442627,
      "learning_rate": 4.324154275544126e-06,
      "loss": 2.0222,
      "step": 3154
    },
    {
      "epoch": 2.077024358130349,
      "grad_norm": 7.633486747741699,
      "learning_rate": 4.318479696165711e-06,
      "loss": 1.7731,
      "step": 3155
    },
    {
      "epoch": 2.0776826859776167,
      "grad_norm": 1.6540776491165161,
      "learning_rate": 4.312807817051341e-06,
      "loss": 1.5413,
      "step": 3156
    },
    {
      "epoch": 2.078341013824885,
      "grad_norm": 1.6732209920883179,
      "learning_rate": 4.307138640896696e-06,
      "loss": 1.5502,
      "step": 3157
    },
    {
      "epoch": 2.078999341672153,
      "grad_norm": 21.644493103027344,
      "learning_rate": 4.301472170396185e-06,
      "loss": 2.425,
      "step": 3158
    },
    {
      "epoch": 2.079657669519421,
      "grad_norm": 8.308216094970703,
      "learning_rate": 4.295808408242908e-06,
      "loss": 1.8407,
      "step": 3159
    },
    {
      "epoch": 2.0803159973666885,
      "grad_norm": 24.892288208007812,
      "learning_rate": 4.2901473571286935e-06,
      "loss": 2.4036,
      "step": 3160
    },
    {
      "epoch": 2.0809743252139565,
      "grad_norm": 10.832432746887207,
      "learning_rate": 4.284489019744081e-06,
      "loss": 1.9408,
      "step": 3161
    },
    {
      "epoch": 2.0816326530612246,
      "grad_norm": 3.71894907951355,
      "learning_rate": 4.278833398778306e-06,
      "loss": 1.5656,
      "step": 3162
    },
    {
      "epoch": 2.0822909809084926,
      "grad_norm": 11.923434257507324,
      "learning_rate": 4.27318049691933e-06,
      "loss": 1.7843,
      "step": 3163
    },
    {
      "epoch": 2.0829493087557602,
      "grad_norm": 41.323814392089844,
      "learning_rate": 4.2675303168538165e-06,
      "loss": 2.8019,
      "step": 3164
    },
    {
      "epoch": 2.0836076366030283,
      "grad_norm": 1.2600754499435425,
      "learning_rate": 4.261882861267131e-06,
      "loss": 1.5307,
      "step": 3165
    },
    {
      "epoch": 2.0842659644502963,
      "grad_norm": 10.671416282653809,
      "learning_rate": 4.256238132843349e-06,
      "loss": 1.7532,
      "step": 3166
    },
    {
      "epoch": 2.0849242922975644,
      "grad_norm": 0.8939134478569031,
      "learning_rate": 4.250596134265254e-06,
      "loss": 1.5283,
      "step": 3167
    },
    {
      "epoch": 2.085582620144832,
      "grad_norm": 9.156166076660156,
      "learning_rate": 4.244956868214316e-06,
      "loss": 1.7753,
      "step": 3168
    },
    {
      "epoch": 2.0862409479921,
      "grad_norm": 0.6334848403930664,
      "learning_rate": 4.239320337370722e-06,
      "loss": 1.5185,
      "step": 3169
    },
    {
      "epoch": 2.086899275839368,
      "grad_norm": 1.3044480085372925,
      "learning_rate": 4.233686544413355e-06,
      "loss": 1.5413,
      "step": 3170
    },
    {
      "epoch": 2.087557603686636,
      "grad_norm": 1.5495526790618896,
      "learning_rate": 4.228055492019794e-06,
      "loss": 1.5423,
      "step": 3171
    },
    {
      "epoch": 2.0882159315339037,
      "grad_norm": 8.414419174194336,
      "learning_rate": 4.2224271828663175e-06,
      "loss": 1.6383,
      "step": 3172
    },
    {
      "epoch": 2.0888742593811718,
      "grad_norm": 13.114579200744629,
      "learning_rate": 4.216801619627904e-06,
      "loss": 1.708,
      "step": 3173
    },
    {
      "epoch": 2.08953258722844,
      "grad_norm": 19.69734001159668,
      "learning_rate": 4.211178804978214e-06,
      "loss": 2.0343,
      "step": 3174
    },
    {
      "epoch": 2.090190915075708,
      "grad_norm": 15.628570556640625,
      "learning_rate": 4.205558741589615e-06,
      "loss": 1.7595,
      "step": 3175
    },
    {
      "epoch": 2.0908492429229755,
      "grad_norm": 35.034706115722656,
      "learning_rate": 4.1999414321331635e-06,
      "loss": 2.0522,
      "step": 3176
    },
    {
      "epoch": 2.0915075707702435,
      "grad_norm": 0.7846643328666687,
      "learning_rate": 4.194326879278604e-06,
      "loss": 1.5232,
      "step": 3177
    },
    {
      "epoch": 2.0921658986175116,
      "grad_norm": 11.115484237670898,
      "learning_rate": 4.188715085694374e-06,
      "loss": 1.6399,
      "step": 3178
    },
    {
      "epoch": 2.0928242264647796,
      "grad_norm": 30.969322204589844,
      "learning_rate": 4.183106054047597e-06,
      "loss": 3.1106,
      "step": 3179
    },
    {
      "epoch": 2.093482554312047,
      "grad_norm": 25.992713928222656,
      "learning_rate": 4.1774997870040924e-06,
      "loss": 2.4959,
      "step": 3180
    },
    {
      "epoch": 2.0941408821593153,
      "grad_norm": 2.9262990951538086,
      "learning_rate": 4.171896287228348e-06,
      "loss": 1.5991,
      "step": 3181
    },
    {
      "epoch": 2.0947992100065833,
      "grad_norm": 27.017011642456055,
      "learning_rate": 4.166295557383549e-06,
      "loss": 2.0784,
      "step": 3182
    },
    {
      "epoch": 2.0954575378538514,
      "grad_norm": 20.442354202270508,
      "learning_rate": 4.160697600131568e-06,
      "loss": 2.2202,
      "step": 3183
    },
    {
      "epoch": 2.096115865701119,
      "grad_norm": 3.2020297050476074,
      "learning_rate": 4.155102418132948e-06,
      "loss": 1.5501,
      "step": 3184
    },
    {
      "epoch": 2.096774193548387,
      "grad_norm": 1.563876748085022,
      "learning_rate": 4.149510014046922e-06,
      "loss": 1.5403,
      "step": 3185
    },
    {
      "epoch": 2.097432521395655,
      "grad_norm": 8.09697151184082,
      "learning_rate": 4.143920390531403e-06,
      "loss": 1.7612,
      "step": 3186
    },
    {
      "epoch": 2.098090849242923,
      "grad_norm": 12.302054405212402,
      "learning_rate": 4.138333550242971e-06,
      "loss": 1.9356,
      "step": 3187
    },
    {
      "epoch": 2.0987491770901907,
      "grad_norm": 18.08731460571289,
      "learning_rate": 4.132749495836896e-06,
      "loss": 2.0184,
      "step": 3188
    },
    {
      "epoch": 2.0994075049374588,
      "grad_norm": 23.91269874572754,
      "learning_rate": 4.127168229967123e-06,
      "loss": 1.9767,
      "step": 3189
    },
    {
      "epoch": 2.100065832784727,
      "grad_norm": 1.1937432289123535,
      "learning_rate": 4.121589755286255e-06,
      "loss": 1.5304,
      "step": 3190
    },
    {
      "epoch": 2.100724160631995,
      "grad_norm": 15.872015953063965,
      "learning_rate": 4.1160140744455945e-06,
      "loss": 2.137,
      "step": 3191
    },
    {
      "epoch": 2.1013824884792625,
      "grad_norm": 10.812342643737793,
      "learning_rate": 4.110441190095102e-06,
      "loss": 1.8389,
      "step": 3192
    },
    {
      "epoch": 2.1020408163265305,
      "grad_norm": 16.622631072998047,
      "learning_rate": 4.104871104883403e-06,
      "loss": 2.0328,
      "step": 3193
    },
    {
      "epoch": 2.1026991441737986,
      "grad_norm": 18.32738494873047,
      "learning_rate": 4.0993038214578044e-06,
      "loss": 1.9967,
      "step": 3194
    },
    {
      "epoch": 2.1033574720210666,
      "grad_norm": 4.17785120010376,
      "learning_rate": 4.093739342464278e-06,
      "loss": 1.5954,
      "step": 3195
    },
    {
      "epoch": 2.1040157998683346,
      "grad_norm": 21.909135818481445,
      "learning_rate": 4.088177670547457e-06,
      "loss": 2.1597,
      "step": 3196
    },
    {
      "epoch": 2.1046741277156022,
      "grad_norm": 8.076067924499512,
      "learning_rate": 4.082618808350645e-06,
      "loss": 1.7605,
      "step": 3197
    },
    {
      "epoch": 2.1053324555628703,
      "grad_norm": 11.750758171081543,
      "learning_rate": 4.077062758515814e-06,
      "loss": 1.8409,
      "step": 3198
    },
    {
      "epoch": 2.1059907834101383,
      "grad_norm": 3.6702160835266113,
      "learning_rate": 4.071509523683592e-06,
      "loss": 1.5606,
      "step": 3199
    },
    {
      "epoch": 2.1066491112574064,
      "grad_norm": 14.989404678344727,
      "learning_rate": 4.065959106493276e-06,
      "loss": 1.8013,
      "step": 3200
    },
    {
      "epoch": 2.107307439104674,
      "grad_norm": 7.7220611572265625,
      "learning_rate": 4.0604115095828225e-06,
      "loss": 1.5788,
      "step": 3201
    },
    {
      "epoch": 2.107965766951942,
      "grad_norm": 10.431357383728027,
      "learning_rate": 4.054866735588837e-06,
      "loss": 1.8142,
      "step": 3202
    },
    {
      "epoch": 2.10862409479921,
      "grad_norm": 1.9667054414749146,
      "learning_rate": 4.049324787146598e-06,
      "loss": 1.5604,
      "step": 3203
    },
    {
      "epoch": 2.109282422646478,
      "grad_norm": 2.717057704925537,
      "learning_rate": 4.043785666890035e-06,
      "loss": 1.551,
      "step": 3204
    },
    {
      "epoch": 2.1099407504937457,
      "grad_norm": 1.021364688873291,
      "learning_rate": 4.038249377451731e-06,
      "loss": 1.5318,
      "step": 3205
    },
    {
      "epoch": 2.110599078341014,
      "grad_norm": 4.139526844024658,
      "learning_rate": 4.032715921462926e-06,
      "loss": 1.6135,
      "step": 3206
    },
    {
      "epoch": 2.111257406188282,
      "grad_norm": 34.15175247192383,
      "learning_rate": 4.02718530155352e-06,
      "loss": 1.9085,
      "step": 3207
    },
    {
      "epoch": 2.11191573403555,
      "grad_norm": 0.9351062178611755,
      "learning_rate": 4.021657520352049e-06,
      "loss": 1.5294,
      "step": 3208
    },
    {
      "epoch": 2.1125740618828175,
      "grad_norm": 0.8970409035682678,
      "learning_rate": 4.016132580485713e-06,
      "loss": 1.5245,
      "step": 3209
    },
    {
      "epoch": 2.1132323897300855,
      "grad_norm": 9.639949798583984,
      "learning_rate": 4.010610484580359e-06,
      "loss": 1.8257,
      "step": 3210
    },
    {
      "epoch": 2.1138907175773536,
      "grad_norm": 1.6573480367660522,
      "learning_rate": 4.00509123526048e-06,
      "loss": 1.536,
      "step": 3211
    },
    {
      "epoch": 2.1145490454246216,
      "grad_norm": 11.38719654083252,
      "learning_rate": 3.999574835149217e-06,
      "loss": 2.0159,
      "step": 3212
    },
    {
      "epoch": 2.1152073732718892,
      "grad_norm": 16.5308895111084,
      "learning_rate": 3.994061286868361e-06,
      "loss": 1.7096,
      "step": 3213
    },
    {
      "epoch": 2.1158657011191573,
      "grad_norm": 10.328378677368164,
      "learning_rate": 3.988550593038338e-06,
      "loss": 1.8398,
      "step": 3214
    },
    {
      "epoch": 2.1165240289664253,
      "grad_norm": 9.474419593811035,
      "learning_rate": 3.983042756278224e-06,
      "loss": 1.7935,
      "step": 3215
    },
    {
      "epoch": 2.1171823568136934,
      "grad_norm": 3.155315637588501,
      "learning_rate": 3.9775377792057405e-06,
      "loss": 1.5502,
      "step": 3216
    },
    {
      "epoch": 2.117840684660961,
      "grad_norm": 25.31998062133789,
      "learning_rate": 3.97203566443724e-06,
      "loss": 1.8372,
      "step": 3217
    },
    {
      "epoch": 2.118499012508229,
      "grad_norm": 20.880178451538086,
      "learning_rate": 3.966536414587725e-06,
      "loss": 2.0154,
      "step": 3218
    },
    {
      "epoch": 2.119157340355497,
      "grad_norm": 1.0436841249465942,
      "learning_rate": 3.96104003227083e-06,
      "loss": 1.5237,
      "step": 3219
    },
    {
      "epoch": 2.119815668202765,
      "grad_norm": 4.1488938331604,
      "learning_rate": 3.955546520098831e-06,
      "loss": 1.5633,
      "step": 3220
    },
    {
      "epoch": 2.1204739960500327,
      "grad_norm": 1.7044727802276611,
      "learning_rate": 3.9500558806826295e-06,
      "loss": 1.5349,
      "step": 3221
    },
    {
      "epoch": 2.1211323238973008,
      "grad_norm": 1.202337384223938,
      "learning_rate": 3.944568116631775e-06,
      "loss": 1.5242,
      "step": 3222
    },
    {
      "epoch": 2.121790651744569,
      "grad_norm": 1.5857577323913574,
      "learning_rate": 3.939083230554448e-06,
      "loss": 1.5395,
      "step": 3223
    },
    {
      "epoch": 2.122448979591837,
      "grad_norm": 34.434940338134766,
      "learning_rate": 3.933601225057446e-06,
      "loss": 2.5593,
      "step": 3224
    },
    {
      "epoch": 2.1231073074391045,
      "grad_norm": 12.985528945922852,
      "learning_rate": 3.92812210274622e-06,
      "loss": 1.9818,
      "step": 3225
    },
    {
      "epoch": 2.1237656352863725,
      "grad_norm": 1.441785216331482,
      "learning_rate": 3.922645866224842e-06,
      "loss": 1.5342,
      "step": 3226
    },
    {
      "epoch": 2.1244239631336406,
      "grad_norm": 21.87578773498535,
      "learning_rate": 3.917172518096001e-06,
      "loss": 2.1428,
      "step": 3227
    },
    {
      "epoch": 2.1250822909809086,
      "grad_norm": 3.8091135025024414,
      "learning_rate": 3.911702060961028e-06,
      "loss": 1.6555,
      "step": 3228
    },
    {
      "epoch": 2.1257406188281767,
      "grad_norm": 1.1149033308029175,
      "learning_rate": 3.906234497419878e-06,
      "loss": 1.5284,
      "step": 3229
    },
    {
      "epoch": 2.1263989466754443,
      "grad_norm": 12.308364868164062,
      "learning_rate": 3.9007698300711195e-06,
      "loss": 1.781,
      "step": 3230
    },
    {
      "epoch": 2.1270572745227123,
      "grad_norm": 21.98902130126953,
      "learning_rate": 3.895308061511953e-06,
      "loss": 2.0343,
      "step": 3231
    },
    {
      "epoch": 2.1277156023699804,
      "grad_norm": 10.562919616699219,
      "learning_rate": 3.889849194338213e-06,
      "loss": 1.8512,
      "step": 3232
    },
    {
      "epoch": 2.128373930217248,
      "grad_norm": 33.990238189697266,
      "learning_rate": 3.88439323114433e-06,
      "loss": 2.9698,
      "step": 3233
    },
    {
      "epoch": 2.129032258064516,
      "grad_norm": 4.230515956878662,
      "learning_rate": 3.878940174523371e-06,
      "loss": 1.5653,
      "step": 3234
    },
    {
      "epoch": 2.129690585911784,
      "grad_norm": 21.171600341796875,
      "learning_rate": 3.873490027067023e-06,
      "loss": 1.7903,
      "step": 3235
    },
    {
      "epoch": 2.130348913759052,
      "grad_norm": 17.494518280029297,
      "learning_rate": 3.868042791365577e-06,
      "loss": 1.7593,
      "step": 3236
    },
    {
      "epoch": 2.13100724160632,
      "grad_norm": 8.643966674804688,
      "learning_rate": 3.862598470007952e-06,
      "loss": 1.6677,
      "step": 3237
    },
    {
      "epoch": 2.1316655694535878,
      "grad_norm": 2.8857436180114746,
      "learning_rate": 3.857157065581678e-06,
      "loss": 1.5448,
      "step": 3238
    },
    {
      "epoch": 2.132323897300856,
      "grad_norm": 3.913640022277832,
      "learning_rate": 3.851718580672898e-06,
      "loss": 1.6225,
      "step": 3239
    },
    {
      "epoch": 2.132982225148124,
      "grad_norm": 1.0521292686462402,
      "learning_rate": 3.846283017866371e-06,
      "loss": 1.5353,
      "step": 3240
    },
    {
      "epoch": 2.133640552995392,
      "grad_norm": 0.8215958476066589,
      "learning_rate": 3.840850379745466e-06,
      "loss": 1.5209,
      "step": 3241
    },
    {
      "epoch": 2.1342988808426595,
      "grad_norm": 9.397256851196289,
      "learning_rate": 3.8354206688921524e-06,
      "loss": 1.9127,
      "step": 3242
    },
    {
      "epoch": 2.1349572086899276,
      "grad_norm": 21.66330337524414,
      "learning_rate": 3.8299938878870215e-06,
      "loss": 2.1903,
      "step": 3243
    },
    {
      "epoch": 2.1356155365371956,
      "grad_norm": 7.002859115600586,
      "learning_rate": 3.824570039309267e-06,
      "loss": 1.7608,
      "step": 3244
    },
    {
      "epoch": 2.1362738643844636,
      "grad_norm": 11.085033416748047,
      "learning_rate": 3.819149125736689e-06,
      "loss": 1.9544,
      "step": 3245
    },
    {
      "epoch": 2.1369321922317313,
      "grad_norm": 0.59232497215271,
      "learning_rate": 3.8137311497456908e-06,
      "loss": 1.5171,
      "step": 3246
    },
    {
      "epoch": 2.1375905200789993,
      "grad_norm": 16.43545913696289,
      "learning_rate": 3.808316113911288e-06,
      "loss": 2.0804,
      "step": 3247
    },
    {
      "epoch": 2.1382488479262673,
      "grad_norm": 24.297056198120117,
      "learning_rate": 3.8029040208070833e-06,
      "loss": 1.8991,
      "step": 3248
    },
    {
      "epoch": 2.1389071757735354,
      "grad_norm": 24.437545776367188,
      "learning_rate": 3.797494873005293e-06,
      "loss": 2.1457,
      "step": 3249
    },
    {
      "epoch": 2.139565503620803,
      "grad_norm": 1.2435168027877808,
      "learning_rate": 3.79208867307673e-06,
      "loss": 1.5325,
      "step": 3250
    },
    {
      "epoch": 2.140223831468071,
      "grad_norm": 1.4878603219985962,
      "learning_rate": 3.786685423590809e-06,
      "loss": 1.537,
      "step": 3251
    },
    {
      "epoch": 2.140882159315339,
      "grad_norm": 1.6555842161178589,
      "learning_rate": 3.781285127115537e-06,
      "loss": 1.533,
      "step": 3252
    },
    {
      "epoch": 2.141540487162607,
      "grad_norm": 13.09241771697998,
      "learning_rate": 3.7758877862175224e-06,
      "loss": 1.9338,
      "step": 3253
    },
    {
      "epoch": 2.1421988150098747,
      "grad_norm": 1.7709555625915527,
      "learning_rate": 3.770493403461969e-06,
      "loss": 1.5572,
      "step": 3254
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.6023691892623901,
      "learning_rate": 3.7651019814126656e-06,
      "loss": 1.5189,
      "step": 3255
    },
    {
      "epoch": 2.143515470704411,
      "grad_norm": 1.1884268522262573,
      "learning_rate": 3.759713522632006e-06,
      "loss": 1.5314,
      "step": 3256
    },
    {
      "epoch": 2.144173798551679,
      "grad_norm": 1.569549798965454,
      "learning_rate": 3.7543280296809714e-06,
      "loss": 1.5273,
      "step": 3257
    },
    {
      "epoch": 2.1448321263989465,
      "grad_norm": 23.10031509399414,
      "learning_rate": 3.7489455051191246e-06,
      "loss": 2.0273,
      "step": 3258
    },
    {
      "epoch": 2.1454904542462145,
      "grad_norm": 5.356490135192871,
      "learning_rate": 3.743565951504634e-06,
      "loss": 1.7173,
      "step": 3259
    },
    {
      "epoch": 2.1461487820934826,
      "grad_norm": 0.9972993731498718,
      "learning_rate": 3.7381893713942495e-06,
      "loss": 1.5313,
      "step": 3260
    },
    {
      "epoch": 2.1468071099407506,
      "grad_norm": 18.393630981445312,
      "learning_rate": 3.7328157673432976e-06,
      "loss": 2.0753,
      "step": 3261
    },
    {
      "epoch": 2.1474654377880182,
      "grad_norm": 0.8472112417221069,
      "learning_rate": 3.7274451419057012e-06,
      "loss": 1.5293,
      "step": 3262
    },
    {
      "epoch": 2.1481237656352863,
      "grad_norm": 13.771841049194336,
      "learning_rate": 3.72207749763397e-06,
      "loss": 1.9219,
      "step": 3263
    },
    {
      "epoch": 2.1487820934825543,
      "grad_norm": 0.7511740922927856,
      "learning_rate": 3.716712837079184e-06,
      "loss": 1.5215,
      "step": 3264
    },
    {
      "epoch": 2.1494404213298224,
      "grad_norm": 7.012297630310059,
      "learning_rate": 3.711351162791014e-06,
      "loss": 1.7927,
      "step": 3265
    },
    {
      "epoch": 2.15009874917709,
      "grad_norm": 9.128912925720215,
      "learning_rate": 3.705992477317718e-06,
      "loss": 1.8297,
      "step": 3266
    },
    {
      "epoch": 2.150757077024358,
      "grad_norm": 0.8973445892333984,
      "learning_rate": 3.7006367832061186e-06,
      "loss": 1.5295,
      "step": 3267
    },
    {
      "epoch": 2.151415404871626,
      "grad_norm": 1.374778151512146,
      "learning_rate": 3.695284083001626e-06,
      "loss": 1.5304,
      "step": 3268
    },
    {
      "epoch": 2.152073732718894,
      "grad_norm": 18.104381561279297,
      "learning_rate": 3.689934379248229e-06,
      "loss": 1.9758,
      "step": 3269
    },
    {
      "epoch": 2.152732060566162,
      "grad_norm": 13.075568199157715,
      "learning_rate": 3.684587674488481e-06,
      "loss": 2.0974,
      "step": 3270
    },
    {
      "epoch": 2.15339038841343,
      "grad_norm": 0.8769646883010864,
      "learning_rate": 3.679243971263523e-06,
      "loss": 1.5253,
      "step": 3271
    },
    {
      "epoch": 2.154048716260698,
      "grad_norm": 9.03377628326416,
      "learning_rate": 3.673903272113064e-06,
      "loss": 1.8476,
      "step": 3272
    },
    {
      "epoch": 2.154707044107966,
      "grad_norm": 8.370966911315918,
      "learning_rate": 3.6685655795753836e-06,
      "loss": 1.8464,
      "step": 3273
    },
    {
      "epoch": 2.1553653719552335,
      "grad_norm": 1.133839726448059,
      "learning_rate": 3.6632308961873376e-06,
      "loss": 1.5309,
      "step": 3274
    },
    {
      "epoch": 2.1560236998025015,
      "grad_norm": 1.111791729927063,
      "learning_rate": 3.657899224484348e-06,
      "loss": 1.5313,
      "step": 3275
    },
    {
      "epoch": 2.1566820276497696,
      "grad_norm": 10.762011528015137,
      "learning_rate": 3.652570567000402e-06,
      "loss": 1.9597,
      "step": 3276
    },
    {
      "epoch": 2.1573403554970376,
      "grad_norm": 14.42566967010498,
      "learning_rate": 3.64724492626806e-06,
      "loss": 2.2267,
      "step": 3277
    },
    {
      "epoch": 2.1579986833443057,
      "grad_norm": 1.173632264137268,
      "learning_rate": 3.641922304818446e-06,
      "loss": 1.5267,
      "step": 3278
    },
    {
      "epoch": 2.1586570111915733,
      "grad_norm": 27.214914321899414,
      "learning_rate": 3.6366027051812515e-06,
      "loss": 2.7048,
      "step": 3279
    },
    {
      "epoch": 2.1593153390388413,
      "grad_norm": 12.759880065917969,
      "learning_rate": 3.631286129884729e-06,
      "loss": 1.8175,
      "step": 3280
    },
    {
      "epoch": 2.1599736668861094,
      "grad_norm": 1.581131935119629,
      "learning_rate": 3.625972581455699e-06,
      "loss": 1.5285,
      "step": 3281
    },
    {
      "epoch": 2.1606319947333774,
      "grad_norm": 12.981587409973145,
      "learning_rate": 3.620662062419531e-06,
      "loss": 1.6844,
      "step": 3282
    },
    {
      "epoch": 2.161290322580645,
      "grad_norm": 4.897148609161377,
      "learning_rate": 3.6153545753001663e-06,
      "loss": 1.6358,
      "step": 3283
    },
    {
      "epoch": 2.161948650427913,
      "grad_norm": 10.810563087463379,
      "learning_rate": 3.6100501226201023e-06,
      "loss": 1.8575,
      "step": 3284
    },
    {
      "epoch": 2.162606978275181,
      "grad_norm": 1.7409902811050415,
      "learning_rate": 3.604748706900393e-06,
      "loss": 1.5655,
      "step": 3285
    },
    {
      "epoch": 2.163265306122449,
      "grad_norm": 12.411520004272461,
      "learning_rate": 3.5994503306606497e-06,
      "loss": 1.8772,
      "step": 3286
    },
    {
      "epoch": 2.1639236339697168,
      "grad_norm": 14.884979248046875,
      "learning_rate": 3.5941549964190438e-06,
      "loss": 2.1964,
      "step": 3287
    },
    {
      "epoch": 2.164581961816985,
      "grad_norm": 19.427661895751953,
      "learning_rate": 3.5888627066922865e-06,
      "loss": 2.136,
      "step": 3288
    },
    {
      "epoch": 2.165240289664253,
      "grad_norm": 0.861907422542572,
      "learning_rate": 3.5835734639956574e-06,
      "loss": 1.5209,
      "step": 3289
    },
    {
      "epoch": 2.165898617511521,
      "grad_norm": 8.749040603637695,
      "learning_rate": 3.578287270842986e-06,
      "loss": 1.804,
      "step": 3290
    },
    {
      "epoch": 2.1665569453587885,
      "grad_norm": 6.849996566772461,
      "learning_rate": 3.5730041297466366e-06,
      "loss": 1.7307,
      "step": 3291
    },
    {
      "epoch": 2.1672152732060566,
      "grad_norm": 16.980297088623047,
      "learning_rate": 3.5677240432175463e-06,
      "loss": 1.9645,
      "step": 3292
    },
    {
      "epoch": 2.1678736010533246,
      "grad_norm": 3.2992825508117676,
      "learning_rate": 3.5624470137651856e-06,
      "loss": 1.5923,
      "step": 3293
    },
    {
      "epoch": 2.1685319289005927,
      "grad_norm": 11.646958351135254,
      "learning_rate": 3.5571730438975794e-06,
      "loss": 2.1338,
      "step": 3294
    },
    {
      "epoch": 2.1691902567478603,
      "grad_norm": 9.426980972290039,
      "learning_rate": 3.5519021361212857e-06,
      "loss": 1.8054,
      "step": 3295
    },
    {
      "epoch": 2.1698485845951283,
      "grad_norm": 16.343528747558594,
      "learning_rate": 3.5466342929414233e-06,
      "loss": 2.0434,
      "step": 3296
    },
    {
      "epoch": 2.1705069124423964,
      "grad_norm": 8.654054641723633,
      "learning_rate": 3.541369516861648e-06,
      "loss": 1.8831,
      "step": 3297
    },
    {
      "epoch": 2.1711652402896644,
      "grad_norm": 19.355998992919922,
      "learning_rate": 3.5361078103841507e-06,
      "loss": 1.9241,
      "step": 3298
    },
    {
      "epoch": 2.171823568136932,
      "grad_norm": 2.4356000423431396,
      "learning_rate": 3.530849176009671e-06,
      "loss": 1.5415,
      "step": 3299
    },
    {
      "epoch": 2.1724818959842,
      "grad_norm": 0.7197352647781372,
      "learning_rate": 3.525593616237497e-06,
      "loss": 1.5252,
      "step": 3300
    },
    {
      "epoch": 2.173140223831468,
      "grad_norm": 3.7981643676757812,
      "learning_rate": 3.520341133565436e-06,
      "loss": 1.5597,
      "step": 3301
    },
    {
      "epoch": 2.173798551678736,
      "grad_norm": 1.1220266819000244,
      "learning_rate": 3.5150917304898445e-06,
      "loss": 1.5252,
      "step": 3302
    },
    {
      "epoch": 2.1744568795260037,
      "grad_norm": 1.1736327409744263,
      "learning_rate": 3.50984540950562e-06,
      "loss": 1.562,
      "step": 3303
    },
    {
      "epoch": 2.175115207373272,
      "grad_norm": 12.23529052734375,
      "learning_rate": 3.5046021731061786e-06,
      "loss": 1.8859,
      "step": 3304
    },
    {
      "epoch": 2.17577353522054,
      "grad_norm": 28.18927574157715,
      "learning_rate": 3.4993620237834867e-06,
      "loss": 2.179,
      "step": 3305
    },
    {
      "epoch": 2.176431863067808,
      "grad_norm": 1.5944432020187378,
      "learning_rate": 3.494124964028036e-06,
      "loss": 1.5338,
      "step": 3306
    },
    {
      "epoch": 2.1770901909150755,
      "grad_norm": 11.725680351257324,
      "learning_rate": 3.4888909963288518e-06,
      "loss": 1.9071,
      "step": 3307
    },
    {
      "epoch": 2.1777485187623435,
      "grad_norm": 4.425623893737793,
      "learning_rate": 3.483660123173489e-06,
      "loss": 1.6823,
      "step": 3308
    },
    {
      "epoch": 2.1784068466096116,
      "grad_norm": 23.840620040893555,
      "learning_rate": 3.478432347048034e-06,
      "loss": 2.0744,
      "step": 3309
    },
    {
      "epoch": 2.1790651744568796,
      "grad_norm": 2.4444212913513184,
      "learning_rate": 3.4732076704370954e-06,
      "loss": 1.5293,
      "step": 3310
    },
    {
      "epoch": 2.1797235023041477,
      "grad_norm": 0.9160996675491333,
      "learning_rate": 3.4679860958238134e-06,
      "loss": 1.5287,
      "step": 3311
    },
    {
      "epoch": 2.1803818301514153,
      "grad_norm": 23.78939437866211,
      "learning_rate": 3.4627676256898536e-06,
      "loss": 2.5158,
      "step": 3312
    },
    {
      "epoch": 2.1810401579986833,
      "grad_norm": 4.551077842712402,
      "learning_rate": 3.4575522625154047e-06,
      "loss": 1.5557,
      "step": 3313
    },
    {
      "epoch": 2.1816984858459514,
      "grad_norm": 4.051579475402832,
      "learning_rate": 3.452340008779181e-06,
      "loss": 1.5504,
      "step": 3314
    },
    {
      "epoch": 2.182356813693219,
      "grad_norm": 10.025001525878906,
      "learning_rate": 3.4471308669584215e-06,
      "loss": 1.8489,
      "step": 3315
    },
    {
      "epoch": 2.183015141540487,
      "grad_norm": 16.57248306274414,
      "learning_rate": 3.441924839528874e-06,
      "loss": 2.0515,
      "step": 3316
    },
    {
      "epoch": 2.183673469387755,
      "grad_norm": 1.0368815660476685,
      "learning_rate": 3.4367219289648192e-06,
      "loss": 1.5255,
      "step": 3317
    },
    {
      "epoch": 2.184331797235023,
      "grad_norm": 22.557708740234375,
      "learning_rate": 3.4315221377390496e-06,
      "loss": 2.2024,
      "step": 3318
    },
    {
      "epoch": 2.184990125082291,
      "grad_norm": 17.05095672607422,
      "learning_rate": 3.4263254683228786e-06,
      "loss": 2.1931,
      "step": 3319
    },
    {
      "epoch": 2.185648452929559,
      "grad_norm": 8.121626853942871,
      "learning_rate": 3.421131923186135e-06,
      "loss": 1.7682,
      "step": 3320
    },
    {
      "epoch": 2.186306780776827,
      "grad_norm": 12.56992244720459,
      "learning_rate": 3.4159415047971643e-06,
      "loss": 1.8922,
      "step": 3321
    },
    {
      "epoch": 2.186965108624095,
      "grad_norm": 10.686150550842285,
      "learning_rate": 3.410754215622818e-06,
      "loss": 1.8105,
      "step": 3322
    },
    {
      "epoch": 2.187623436471363,
      "grad_norm": 2.440568447113037,
      "learning_rate": 3.405570058128469e-06,
      "loss": 1.5759,
      "step": 3323
    },
    {
      "epoch": 2.1882817643186305,
      "grad_norm": 0.8343307971954346,
      "learning_rate": 3.400389034778002e-06,
      "loss": 1.5209,
      "step": 3324
    },
    {
      "epoch": 2.1889400921658986,
      "grad_norm": 11.817789077758789,
      "learning_rate": 3.3952111480338003e-06,
      "loss": 1.9456,
      "step": 3325
    },
    {
      "epoch": 2.1895984200131666,
      "grad_norm": 18.94065284729004,
      "learning_rate": 3.3900364003567754e-06,
      "loss": 2.1024,
      "step": 3326
    },
    {
      "epoch": 2.1902567478604347,
      "grad_norm": 0.8696060180664062,
      "learning_rate": 3.384864794206335e-06,
      "loss": 1.525,
      "step": 3327
    },
    {
      "epoch": 2.1909150757077023,
      "grad_norm": 0.8891588449478149,
      "learning_rate": 3.379696332040391e-06,
      "loss": 1.5248,
      "step": 3328
    },
    {
      "epoch": 2.1915734035549703,
      "grad_norm": 2.794013261795044,
      "learning_rate": 3.3745310163153676e-06,
      "loss": 1.6005,
      "step": 3329
    },
    {
      "epoch": 2.1922317314022384,
      "grad_norm": 1.758853554725647,
      "learning_rate": 3.3693688494861963e-06,
      "loss": 1.5382,
      "step": 3330
    },
    {
      "epoch": 2.1928900592495064,
      "grad_norm": 10.941884994506836,
      "learning_rate": 3.364209834006299e-06,
      "loss": 1.7755,
      "step": 3331
    },
    {
      "epoch": 2.193548387096774,
      "grad_norm": 0.9530640244483948,
      "learning_rate": 3.3590539723276083e-06,
      "loss": 1.5222,
      "step": 3332
    },
    {
      "epoch": 2.194206714944042,
      "grad_norm": 2.4930081367492676,
      "learning_rate": 3.3539012669005653e-06,
      "loss": 1.5428,
      "step": 3333
    },
    {
      "epoch": 2.19486504279131,
      "grad_norm": 0.7844785451889038,
      "learning_rate": 3.3487517201741036e-06,
      "loss": 1.5147,
      "step": 3334
    },
    {
      "epoch": 2.195523370638578,
      "grad_norm": 6.8407063484191895,
      "learning_rate": 3.3436053345956477e-06,
      "loss": 1.6587,
      "step": 3335
    },
    {
      "epoch": 2.1961816984858458,
      "grad_norm": 2.400546073913574,
      "learning_rate": 3.338462112611132e-06,
      "loss": 1.5808,
      "step": 3336
    },
    {
      "epoch": 2.196840026333114,
      "grad_norm": 20.209096908569336,
      "learning_rate": 3.3333220566649847e-06,
      "loss": 2.1564,
      "step": 3337
    },
    {
      "epoch": 2.197498354180382,
      "grad_norm": 46.32029342651367,
      "learning_rate": 3.328185169200122e-06,
      "loss": 2.8276,
      "step": 3338
    },
    {
      "epoch": 2.19815668202765,
      "grad_norm": 3.380683183670044,
      "learning_rate": 3.3230514526579617e-06,
      "loss": 1.5575,
      "step": 3339
    },
    {
      "epoch": 2.1988150098749175,
      "grad_norm": 14.708478927612305,
      "learning_rate": 3.317920909478414e-06,
      "loss": 2.4383,
      "step": 3340
    },
    {
      "epoch": 2.1994733377221856,
      "grad_norm": 34.646385192871094,
      "learning_rate": 3.312793542099877e-06,
      "loss": 2.2036,
      "step": 3341
    },
    {
      "epoch": 2.2001316655694536,
      "grad_norm": 31.45797348022461,
      "learning_rate": 3.307669352959243e-06,
      "loss": 2.3307,
      "step": 3342
    },
    {
      "epoch": 2.2007899934167217,
      "grad_norm": 0.8648625016212463,
      "learning_rate": 3.302548344491896e-06,
      "loss": 1.5212,
      "step": 3343
    },
    {
      "epoch": 2.2014483212639893,
      "grad_norm": 20.150232315063477,
      "learning_rate": 3.2974305191316978e-06,
      "loss": 1.8332,
      "step": 3344
    },
    {
      "epoch": 2.2021066491112573,
      "grad_norm": 19.20638656616211,
      "learning_rate": 3.292315879311009e-06,
      "loss": 1.9891,
      "step": 3345
    },
    {
      "epoch": 2.2027649769585254,
      "grad_norm": 3.9646809101104736,
      "learning_rate": 3.2872044274606696e-06,
      "loss": 1.56,
      "step": 3346
    },
    {
      "epoch": 2.2034233048057934,
      "grad_norm": 22.898170471191406,
      "learning_rate": 3.282096166010007e-06,
      "loss": 2.0146,
      "step": 3347
    },
    {
      "epoch": 2.204081632653061,
      "grad_norm": 21.870689392089844,
      "learning_rate": 3.2769910973868314e-06,
      "loss": 2.1087,
      "step": 3348
    },
    {
      "epoch": 2.204739960500329,
      "grad_norm": 17.132387161254883,
      "learning_rate": 3.2718892240174407e-06,
      "loss": 2.5413,
      "step": 3349
    },
    {
      "epoch": 2.205398288347597,
      "grad_norm": 0.5769400596618652,
      "learning_rate": 3.2667905483266017e-06,
      "loss": 1.5171,
      "step": 3350
    },
    {
      "epoch": 2.206056616194865,
      "grad_norm": 12.518957138061523,
      "learning_rate": 3.2616950727375717e-06,
      "loss": 1.7329,
      "step": 3351
    },
    {
      "epoch": 2.206714944042133,
      "grad_norm": 28.021883010864258,
      "learning_rate": 3.256602799672085e-06,
      "loss": 2.0243,
      "step": 3352
    },
    {
      "epoch": 2.207373271889401,
      "grad_norm": 4.081078052520752,
      "learning_rate": 3.2515137315503542e-06,
      "loss": 1.6859,
      "step": 3353
    },
    {
      "epoch": 2.208031599736669,
      "grad_norm": 0.9718250036239624,
      "learning_rate": 3.246427870791067e-06,
      "loss": 1.5226,
      "step": 3354
    },
    {
      "epoch": 2.208689927583937,
      "grad_norm": 0.7997021079063416,
      "learning_rate": 3.2413452198113902e-06,
      "loss": 1.522,
      "step": 3355
    },
    {
      "epoch": 2.209348255431205,
      "grad_norm": 13.676578521728516,
      "learning_rate": 3.236265781026957e-06,
      "loss": 1.8295,
      "step": 3356
    },
    {
      "epoch": 2.2100065832784725,
      "grad_norm": 31.683551788330078,
      "learning_rate": 3.2311895568518825e-06,
      "loss": 1.9516,
      "step": 3357
    },
    {
      "epoch": 2.2106649111257406,
      "grad_norm": 0.9600024819374084,
      "learning_rate": 3.2261165496987523e-06,
      "loss": 1.5334,
      "step": 3358
    },
    {
      "epoch": 2.2113232389730086,
      "grad_norm": 5.090270042419434,
      "learning_rate": 3.2210467619786147e-06,
      "loss": 1.6721,
      "step": 3359
    },
    {
      "epoch": 2.2119815668202767,
      "grad_norm": 0.6985039114952087,
      "learning_rate": 3.215980196101002e-06,
      "loss": 1.5208,
      "step": 3360
    },
    {
      "epoch": 2.2126398946675443,
      "grad_norm": 1.2092912197113037,
      "learning_rate": 3.210916854473909e-06,
      "loss": 1.5215,
      "step": 3361
    },
    {
      "epoch": 2.2132982225148123,
      "grad_norm": 22.962629318237305,
      "learning_rate": 3.205856739503791e-06,
      "loss": 2.2043,
      "step": 3362
    },
    {
      "epoch": 2.2139565503620804,
      "grad_norm": 15.761300086975098,
      "learning_rate": 3.2007998535955774e-06,
      "loss": 2.155,
      "step": 3363
    },
    {
      "epoch": 2.2146148782093484,
      "grad_norm": 0.7817463874816895,
      "learning_rate": 3.1957461991526672e-06,
      "loss": 1.5214,
      "step": 3364
    },
    {
      "epoch": 2.215273206056616,
      "grad_norm": 2.285968542098999,
      "learning_rate": 3.19069577857691e-06,
      "loss": 1.5711,
      "step": 3365
    },
    {
      "epoch": 2.215931533903884,
      "grad_norm": 16.468355178833008,
      "learning_rate": 3.1856485942686254e-06,
      "loss": 1.9263,
      "step": 3366
    },
    {
      "epoch": 2.216589861751152,
      "grad_norm": 12.424514770507812,
      "learning_rate": 3.180604648626606e-06,
      "loss": 1.7813,
      "step": 3367
    },
    {
      "epoch": 2.21724818959842,
      "grad_norm": 1.6984810829162598,
      "learning_rate": 3.1755639440480867e-06,
      "loss": 1.5388,
      "step": 3368
    },
    {
      "epoch": 2.217906517445688,
      "grad_norm": 20.926706314086914,
      "learning_rate": 3.1705264829287717e-06,
      "loss": 2.0042,
      "step": 3369
    },
    {
      "epoch": 2.218564845292956,
      "grad_norm": 17.22784423828125,
      "learning_rate": 3.165492267662822e-06,
      "loss": 2.4354,
      "step": 3370
    },
    {
      "epoch": 2.219223173140224,
      "grad_norm": 1.3035024404525757,
      "learning_rate": 3.1604613006428598e-06,
      "loss": 1.5359,
      "step": 3371
    },
    {
      "epoch": 2.219881500987492,
      "grad_norm": 3.036212682723999,
      "learning_rate": 3.1554335842599537e-06,
      "loss": 1.552,
      "step": 3372
    },
    {
      "epoch": 2.2205398288347595,
      "grad_norm": 8.290584564208984,
      "learning_rate": 3.1504091209036325e-06,
      "loss": 1.739,
      "step": 3373
    },
    {
      "epoch": 2.2211981566820276,
      "grad_norm": 24.709257125854492,
      "learning_rate": 3.1453879129618904e-06,
      "loss": 2.3082,
      "step": 3374
    },
    {
      "epoch": 2.2218564845292956,
      "grad_norm": 6.8661208152771,
      "learning_rate": 3.1403699628211537e-06,
      "loss": 1.7566,
      "step": 3375
    },
    {
      "epoch": 2.2225148123765637,
      "grad_norm": 0.7761836051940918,
      "learning_rate": 3.135355272866315e-06,
      "loss": 1.5215,
      "step": 3376
    },
    {
      "epoch": 2.2231731402238313,
      "grad_norm": 12.202799797058105,
      "learning_rate": 3.1303438454807145e-06,
      "loss": 1.9325,
      "step": 3377
    },
    {
      "epoch": 2.2238314680710993,
      "grad_norm": 14.07808780670166,
      "learning_rate": 3.1253356830461344e-06,
      "loss": 1.96,
      "step": 3378
    },
    {
      "epoch": 2.2244897959183674,
      "grad_norm": 8.6798734664917,
      "learning_rate": 3.1203307879428146e-06,
      "loss": 1.8123,
      "step": 3379
    },
    {
      "epoch": 2.2251481237656354,
      "grad_norm": 32.41801071166992,
      "learning_rate": 3.1153291625494374e-06,
      "loss": 2.596,
      "step": 3380
    },
    {
      "epoch": 2.225806451612903,
      "grad_norm": 13.0961275100708,
      "learning_rate": 3.110330809243134e-06,
      "loss": 1.8619,
      "step": 3381
    },
    {
      "epoch": 2.226464779460171,
      "grad_norm": 13.611313819885254,
      "learning_rate": 3.105335730399478e-06,
      "loss": 1.9456,
      "step": 3382
    },
    {
      "epoch": 2.227123107307439,
      "grad_norm": 1.6709206104278564,
      "learning_rate": 3.100343928392491e-06,
      "loss": 1.5335,
      "step": 3383
    },
    {
      "epoch": 2.227781435154707,
      "grad_norm": 14.998666763305664,
      "learning_rate": 3.0953554055946256e-06,
      "loss": 2.036,
      "step": 3384
    },
    {
      "epoch": 2.228439763001975,
      "grad_norm": 17.476980209350586,
      "learning_rate": 3.090370164376788e-06,
      "loss": 2.0259,
      "step": 3385
    },
    {
      "epoch": 2.229098090849243,
      "grad_norm": 39.859405517578125,
      "learning_rate": 3.0853882071083218e-06,
      "loss": 2.4944,
      "step": 3386
    },
    {
      "epoch": 2.229756418696511,
      "grad_norm": 6.750274181365967,
      "learning_rate": 3.0804095361570086e-06,
      "loss": 1.7364,
      "step": 3387
    },
    {
      "epoch": 2.230414746543779,
      "grad_norm": 17.772247314453125,
      "learning_rate": 3.0754341538890673e-06,
      "loss": 2.1279,
      "step": 3388
    },
    {
      "epoch": 2.2310730743910465,
      "grad_norm": 9.25960636138916,
      "learning_rate": 3.0704620626691585e-06,
      "loss": 1.6048,
      "step": 3389
    },
    {
      "epoch": 2.2317314022383146,
      "grad_norm": 2.484569549560547,
      "learning_rate": 3.0654932648603676e-06,
      "loss": 1.5493,
      "step": 3390
    },
    {
      "epoch": 2.2323897300855826,
      "grad_norm": 6.4737725257873535,
      "learning_rate": 3.0605277628242256e-06,
      "loss": 1.7004,
      "step": 3391
    },
    {
      "epoch": 2.2330480579328507,
      "grad_norm": 39.97100067138672,
      "learning_rate": 3.055565558920696e-06,
      "loss": 3.3214,
      "step": 3392
    },
    {
      "epoch": 2.2337063857801187,
      "grad_norm": 1.0687767267227173,
      "learning_rate": 3.0506066555081683e-06,
      "loss": 1.5195,
      "step": 3393
    },
    {
      "epoch": 2.2343647136273863,
      "grad_norm": 1.6235612630844116,
      "learning_rate": 3.0456510549434694e-06,
      "loss": 1.5283,
      "step": 3394
    },
    {
      "epoch": 2.2350230414746544,
      "grad_norm": 32.9397087097168,
      "learning_rate": 3.040698759581858e-06,
      "loss": 2.2024,
      "step": 3395
    },
    {
      "epoch": 2.2356813693219224,
      "grad_norm": 11.147954940795898,
      "learning_rate": 3.0357497717770124e-06,
      "loss": 1.843,
      "step": 3396
    },
    {
      "epoch": 2.2363396971691905,
      "grad_norm": 0.8973908424377441,
      "learning_rate": 3.0308040938810466e-06,
      "loss": 1.5214,
      "step": 3397
    },
    {
      "epoch": 2.236998025016458,
      "grad_norm": 24.768598556518555,
      "learning_rate": 3.0258617282445035e-06,
      "loss": 1.9115,
      "step": 3398
    },
    {
      "epoch": 2.237656352863726,
      "grad_norm": 12.186241149902344,
      "learning_rate": 3.0209226772163425e-06,
      "loss": 1.8873,
      "step": 3399
    },
    {
      "epoch": 2.238314680710994,
      "grad_norm": 51.17948532104492,
      "learning_rate": 3.0159869431439526e-06,
      "loss": 2.7153,
      "step": 3400
    },
    {
      "epoch": 2.238973008558262,
      "grad_norm": 1.2552136182785034,
      "learning_rate": 3.011054528373156e-06,
      "loss": 1.5312,
      "step": 3401
    },
    {
      "epoch": 2.23963133640553,
      "grad_norm": 1.897532343864441,
      "learning_rate": 3.0061254352481806e-06,
      "loss": 1.5251,
      "step": 3402
    },
    {
      "epoch": 2.240289664252798,
      "grad_norm": 6.066438674926758,
      "learning_rate": 3.001199666111684e-06,
      "loss": 1.6064,
      "step": 3403
    },
    {
      "epoch": 2.240947992100066,
      "grad_norm": 16.579341888427734,
      "learning_rate": 2.996277223304749e-06,
      "loss": 1.9997,
      "step": 3404
    },
    {
      "epoch": 2.241606319947334,
      "grad_norm": 20.723064422607422,
      "learning_rate": 2.991358109166863e-06,
      "loss": 2.1238,
      "step": 3405
    },
    {
      "epoch": 2.2422646477946016,
      "grad_norm": 9.78436279296875,
      "learning_rate": 2.9864423260359455e-06,
      "loss": 1.8517,
      "step": 3406
    },
    {
      "epoch": 2.2429229756418696,
      "grad_norm": 3.826432228088379,
      "learning_rate": 2.981529876248327e-06,
      "loss": 1.584,
      "step": 3407
    },
    {
      "epoch": 2.2435813034891376,
      "grad_norm": 1.8445619344711304,
      "learning_rate": 2.9766207621387535e-06,
      "loss": 1.5878,
      "step": 3408
    },
    {
      "epoch": 2.2442396313364057,
      "grad_norm": 2.9318559169769287,
      "learning_rate": 2.971714986040387e-06,
      "loss": 1.5383,
      "step": 3409
    },
    {
      "epoch": 2.2448979591836733,
      "grad_norm": 2.357614278793335,
      "learning_rate": 2.9668125502848035e-06,
      "loss": 1.5335,
      "step": 3410
    },
    {
      "epoch": 2.2455562870309413,
      "grad_norm": 19.030967712402344,
      "learning_rate": 2.9619134572019923e-06,
      "loss": 2.0609,
      "step": 3411
    },
    {
      "epoch": 2.2462146148782094,
      "grad_norm": 1.0476773977279663,
      "learning_rate": 2.9570177091203456e-06,
      "loss": 1.5197,
      "step": 3412
    },
    {
      "epoch": 2.2468729427254774,
      "grad_norm": 3.257227659225464,
      "learning_rate": 2.9521253083666767e-06,
      "loss": 1.5735,
      "step": 3413
    },
    {
      "epoch": 2.247531270572745,
      "grad_norm": 1.3821074962615967,
      "learning_rate": 2.947236257266204e-06,
      "loss": 1.5658,
      "step": 3414
    },
    {
      "epoch": 2.248189598420013,
      "grad_norm": 41.68031311035156,
      "learning_rate": 2.9423505581425514e-06,
      "loss": 2.3129,
      "step": 3415
    },
    {
      "epoch": 2.248847926267281,
      "grad_norm": 10.956440925598145,
      "learning_rate": 2.9374682133177547e-06,
      "loss": 1.8429,
      "step": 3416
    },
    {
      "epoch": 2.249506254114549,
      "grad_norm": 1.013033390045166,
      "learning_rate": 2.9325892251122533e-06,
      "loss": 1.5206,
      "step": 3417
    },
    {
      "epoch": 2.2501645819618172,
      "grad_norm": 2.0543088912963867,
      "learning_rate": 2.9277135958448864e-06,
      "loss": 1.5803,
      "step": 3418
    },
    {
      "epoch": 2.250822909809085,
      "grad_norm": 8.164177894592285,
      "learning_rate": 2.922841327832904e-06,
      "loss": 1.7803,
      "step": 3419
    },
    {
      "epoch": 2.251481237656353,
      "grad_norm": 10.403339385986328,
      "learning_rate": 2.9179724233919538e-06,
      "loss": 1.8201,
      "step": 3420
    },
    {
      "epoch": 2.252139565503621,
      "grad_norm": 17.885719299316406,
      "learning_rate": 2.913106884836089e-06,
      "loss": 2.187,
      "step": 3421
    },
    {
      "epoch": 2.2527978933508885,
      "grad_norm": 1.0199137926101685,
      "learning_rate": 2.9082447144777594e-06,
      "loss": 1.5241,
      "step": 3422
    },
    {
      "epoch": 2.2534562211981566,
      "grad_norm": 2.4952633380889893,
      "learning_rate": 2.90338591462782e-06,
      "loss": 1.5795,
      "step": 3423
    },
    {
      "epoch": 2.2541145490454246,
      "grad_norm": 34.71809005737305,
      "learning_rate": 2.898530487595511e-06,
      "loss": 3.1761,
      "step": 3424
    },
    {
      "epoch": 2.2547728768926927,
      "grad_norm": 9.593502044677734,
      "learning_rate": 2.893678435688483e-06,
      "loss": 1.7909,
      "step": 3425
    },
    {
      "epoch": 2.2554312047399607,
      "grad_norm": 15.990410804748535,
      "learning_rate": 2.888829761212777e-06,
      "loss": 1.6408,
      "step": 3426
    },
    {
      "epoch": 2.2560895325872283,
      "grad_norm": 1.6982077360153198,
      "learning_rate": 2.883984466472829e-06,
      "loss": 1.5287,
      "step": 3427
    },
    {
      "epoch": 2.2567478604344964,
      "grad_norm": 23.822778701782227,
      "learning_rate": 2.879142553771469e-06,
      "loss": 2.6816,
      "step": 3428
    },
    {
      "epoch": 2.2574061882817644,
      "grad_norm": 21.13420295715332,
      "learning_rate": 2.874304025409923e-06,
      "loss": 2.2275,
      "step": 3429
    },
    {
      "epoch": 2.258064516129032,
      "grad_norm": 2.6894521713256836,
      "learning_rate": 2.869468883687798e-06,
      "loss": 1.5409,
      "step": 3430
    },
    {
      "epoch": 2.2587228439763,
      "grad_norm": 33.152191162109375,
      "learning_rate": 2.8646371309031017e-06,
      "loss": 2.5232,
      "step": 3431
    },
    {
      "epoch": 2.259381171823568,
      "grad_norm": 1.4900691509246826,
      "learning_rate": 2.8598087693522324e-06,
      "loss": 1.5262,
      "step": 3432
    },
    {
      "epoch": 2.260039499670836,
      "grad_norm": 0.9422346353530884,
      "learning_rate": 2.8549838013299623e-06,
      "loss": 1.5211,
      "step": 3433
    },
    {
      "epoch": 2.260697827518104,
      "grad_norm": 1.6899980306625366,
      "learning_rate": 2.8501622291294693e-06,
      "loss": 1.5646,
      "step": 3434
    },
    {
      "epoch": 2.261356155365372,
      "grad_norm": 12.519771575927734,
      "learning_rate": 2.8453440550423096e-06,
      "loss": 1.798,
      "step": 3435
    },
    {
      "epoch": 2.26201448321264,
      "grad_norm": 9.383724212646484,
      "learning_rate": 2.840529281358416e-06,
      "loss": 1.6639,
      "step": 3436
    },
    {
      "epoch": 2.262672811059908,
      "grad_norm": 10.214488983154297,
      "learning_rate": 2.835717910366117e-06,
      "loss": 1.8176,
      "step": 3437
    },
    {
      "epoch": 2.263331138907176,
      "grad_norm": 14.939912796020508,
      "learning_rate": 2.8309099443521225e-06,
      "loss": 1.9766,
      "step": 3438
    },
    {
      "epoch": 2.2639894667544436,
      "grad_norm": 41.06425094604492,
      "learning_rate": 2.8261053856015152e-06,
      "loss": 2.2496,
      "step": 3439
    },
    {
      "epoch": 2.2646477946017116,
      "grad_norm": 14.578287124633789,
      "learning_rate": 2.821304236397766e-06,
      "loss": 2.0291,
      "step": 3440
    },
    {
      "epoch": 2.2653061224489797,
      "grad_norm": 18.69501304626465,
      "learning_rate": 2.8165064990227255e-06,
      "loss": 2.2402,
      "step": 3441
    },
    {
      "epoch": 2.2659644502962477,
      "grad_norm": 38.48764419555664,
      "learning_rate": 2.81171217575662e-06,
      "loss": 3.1633,
      "step": 3442
    },
    {
      "epoch": 2.2666227781435153,
      "grad_norm": 1.69942045211792,
      "learning_rate": 2.8069212688780545e-06,
      "loss": 1.5325,
      "step": 3443
    },
    {
      "epoch": 2.2672811059907834,
      "grad_norm": 1.1307705640792847,
      "learning_rate": 2.8021337806640137e-06,
      "loss": 1.5486,
      "step": 3444
    },
    {
      "epoch": 2.2679394338380514,
      "grad_norm": 19.30167579650879,
      "learning_rate": 2.7973497133898466e-06,
      "loss": 2.2923,
      "step": 3445
    },
    {
      "epoch": 2.2685977616853195,
      "grad_norm": 22.082836151123047,
      "learning_rate": 2.792569069329287e-06,
      "loss": 2.1845,
      "step": 3446
    },
    {
      "epoch": 2.269256089532587,
      "grad_norm": 6.321751594543457,
      "learning_rate": 2.7877918507544375e-06,
      "loss": 1.7438,
      "step": 3447
    },
    {
      "epoch": 2.269914417379855,
      "grad_norm": 5.561441898345947,
      "learning_rate": 2.7830180599357747e-06,
      "loss": 1.7106,
      "step": 3448
    },
    {
      "epoch": 2.270572745227123,
      "grad_norm": 3.74446177482605,
      "learning_rate": 2.7782476991421447e-06,
      "loss": 1.5568,
      "step": 3449
    },
    {
      "epoch": 2.271231073074391,
      "grad_norm": 5.789182186126709,
      "learning_rate": 2.7734807706407628e-06,
      "loss": 1.6157,
      "step": 3450
    },
    {
      "epoch": 2.271889400921659,
      "grad_norm": 7.45664119720459,
      "learning_rate": 2.7687172766972172e-06,
      "loss": 1.7596,
      "step": 3451
    },
    {
      "epoch": 2.272547728768927,
      "grad_norm": 28.99958610534668,
      "learning_rate": 2.7639572195754526e-06,
      "loss": 2.693,
      "step": 3452
    },
    {
      "epoch": 2.273206056616195,
      "grad_norm": 39.82332229614258,
      "learning_rate": 2.7592006015377937e-06,
      "loss": 2.6252,
      "step": 3453
    },
    {
      "epoch": 2.273864384463463,
      "grad_norm": 0.9387783408164978,
      "learning_rate": 2.7544474248449238e-06,
      "loss": 1.5232,
      "step": 3454
    },
    {
      "epoch": 2.2745227123107306,
      "grad_norm": 23.13486671447754,
      "learning_rate": 2.7496976917558904e-06,
      "loss": 2.1812,
      "step": 3455
    },
    {
      "epoch": 2.2751810401579986,
      "grad_norm": 20.43665313720703,
      "learning_rate": 2.744951404528108e-06,
      "loss": 2.1409,
      "step": 3456
    },
    {
      "epoch": 2.2758393680052666,
      "grad_norm": 4.72874641418457,
      "learning_rate": 2.7402085654173527e-06,
      "loss": 1.6227,
      "step": 3457
    },
    {
      "epoch": 2.2764976958525347,
      "grad_norm": 6.623614311218262,
      "learning_rate": 2.7354691766777554e-06,
      "loss": 1.7585,
      "step": 3458
    },
    {
      "epoch": 2.2771560236998027,
      "grad_norm": 14.993070602416992,
      "learning_rate": 2.7307332405618126e-06,
      "loss": 1.6356,
      "step": 3459
    },
    {
      "epoch": 2.2778143515470703,
      "grad_norm": 9.593070983886719,
      "learning_rate": 2.726000759320382e-06,
      "loss": 1.8609,
      "step": 3460
    },
    {
      "epoch": 2.2784726793943384,
      "grad_norm": 1.365556240081787,
      "learning_rate": 2.7212717352026752e-06,
      "loss": 1.5203,
      "step": 3461
    },
    {
      "epoch": 2.2791310072416064,
      "grad_norm": 16.1507625579834,
      "learning_rate": 2.7165461704562625e-06,
      "loss": 2.0073,
      "step": 3462
    },
    {
      "epoch": 2.279789335088874,
      "grad_norm": 1.9912800788879395,
      "learning_rate": 2.7118240673270714e-06,
      "loss": 1.5403,
      "step": 3463
    },
    {
      "epoch": 2.280447662936142,
      "grad_norm": 7.52643346786499,
      "learning_rate": 2.707105428059379e-06,
      "loss": 1.6033,
      "step": 3464
    },
    {
      "epoch": 2.28110599078341,
      "grad_norm": 14.617250442504883,
      "learning_rate": 2.7023902548958193e-06,
      "loss": 1.8526,
      "step": 3465
    },
    {
      "epoch": 2.281764318630678,
      "grad_norm": 1.065665602684021,
      "learning_rate": 2.697678550077385e-06,
      "loss": 1.5211,
      "step": 3466
    },
    {
      "epoch": 2.2824226464779462,
      "grad_norm": 2.2913146018981934,
      "learning_rate": 2.692970315843404e-06,
      "loss": 1.5365,
      "step": 3467
    },
    {
      "epoch": 2.283080974325214,
      "grad_norm": 15.981865882873535,
      "learning_rate": 2.688265554431574e-06,
      "loss": 2.0325,
      "step": 3468
    },
    {
      "epoch": 2.283739302172482,
      "grad_norm": 15.666546821594238,
      "learning_rate": 2.683564268077935e-06,
      "loss": 1.7212,
      "step": 3469
    },
    {
      "epoch": 2.28439763001975,
      "grad_norm": 20.036762237548828,
      "learning_rate": 2.678866459016867e-06,
      "loss": 2.2223,
      "step": 3470
    },
    {
      "epoch": 2.2850559578670175,
      "grad_norm": 26.989940643310547,
      "learning_rate": 2.6741721294811073e-06,
      "loss": 2.1754,
      "step": 3471
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 47.58056640625,
      "learning_rate": 2.669481281701739e-06,
      "loss": 2.5394,
      "step": 3472
    },
    {
      "epoch": 2.2863726135615536,
      "grad_norm": 14.521533966064453,
      "learning_rate": 2.664793917908184e-06,
      "loss": 1.6868,
      "step": 3473
    },
    {
      "epoch": 2.2870309414088217,
      "grad_norm": 0.9099947214126587,
      "learning_rate": 2.660110040328209e-06,
      "loss": 1.5155,
      "step": 3474
    },
    {
      "epoch": 2.2876892692560897,
      "grad_norm": 15.534582138061523,
      "learning_rate": 2.6554296511879372e-06,
      "loss": 1.927,
      "step": 3475
    },
    {
      "epoch": 2.2883475971033573,
      "grad_norm": 1.9281543493270874,
      "learning_rate": 2.650752752711816e-06,
      "loss": 1.5362,
      "step": 3476
    },
    {
      "epoch": 2.2890059249506254,
      "grad_norm": 0.836057722568512,
      "learning_rate": 2.6460793471226433e-06,
      "loss": 1.5255,
      "step": 3477
    },
    {
      "epoch": 2.2896642527978934,
      "grad_norm": 1.3735774755477905,
      "learning_rate": 2.6414094366415577e-06,
      "loss": 1.5335,
      "step": 3478
    },
    {
      "epoch": 2.2903225806451615,
      "grad_norm": 1.2376148700714111,
      "learning_rate": 2.6367430234880286e-06,
      "loss": 1.5448,
      "step": 3479
    },
    {
      "epoch": 2.290980908492429,
      "grad_norm": 1.5713019371032715,
      "learning_rate": 2.632080109879872e-06,
      "loss": 1.5459,
      "step": 3480
    },
    {
      "epoch": 2.291639236339697,
      "grad_norm": 1.1613526344299316,
      "learning_rate": 2.627420698033237e-06,
      "loss": 1.5413,
      "step": 3481
    },
    {
      "epoch": 2.292297564186965,
      "grad_norm": 6.087282180786133,
      "learning_rate": 2.6227647901626084e-06,
      "loss": 1.7098,
      "step": 3482
    },
    {
      "epoch": 2.2929558920342332,
      "grad_norm": 31.301820755004883,
      "learning_rate": 2.6181123884808056e-06,
      "loss": 2.823,
      "step": 3483
    },
    {
      "epoch": 2.293614219881501,
      "grad_norm": 17.03606605529785,
      "learning_rate": 2.613463495198987e-06,
      "loss": 1.8932,
      "step": 3484
    },
    {
      "epoch": 2.294272547728769,
      "grad_norm": 10.605435371398926,
      "learning_rate": 2.60881811252663e-06,
      "loss": 1.8965,
      "step": 3485
    },
    {
      "epoch": 2.294930875576037,
      "grad_norm": 46.0726318359375,
      "learning_rate": 2.6041762426715565e-06,
      "loss": 2.5825,
      "step": 3486
    },
    {
      "epoch": 2.295589203423305,
      "grad_norm": 2.3079147338867188,
      "learning_rate": 2.5995378878399146e-06,
      "loss": 1.5395,
      "step": 3487
    },
    {
      "epoch": 2.2962475312705726,
      "grad_norm": 18.7735595703125,
      "learning_rate": 2.5949030502361818e-06,
      "loss": 2.1261,
      "step": 3488
    },
    {
      "epoch": 2.2969058591178406,
      "grad_norm": 25.07903480529785,
      "learning_rate": 2.590271732063162e-06,
      "loss": 2.1444,
      "step": 3489
    },
    {
      "epoch": 2.2975641869651087,
      "grad_norm": 6.183948040008545,
      "learning_rate": 2.585643935521991e-06,
      "loss": 1.707,
      "step": 3490
    },
    {
      "epoch": 2.2982225148123767,
      "grad_norm": 1.188022255897522,
      "learning_rate": 2.5810196628121287e-06,
      "loss": 1.5212,
      "step": 3491
    },
    {
      "epoch": 2.2988808426596443,
      "grad_norm": 10.255402565002441,
      "learning_rate": 2.5763989161313552e-06,
      "loss": 1.6239,
      "step": 3492
    },
    {
      "epoch": 2.2995391705069124,
      "grad_norm": 36.90974426269531,
      "learning_rate": 2.5717816976757803e-06,
      "loss": 2.2921,
      "step": 3493
    },
    {
      "epoch": 2.3001974983541804,
      "grad_norm": 19.450483322143555,
      "learning_rate": 2.5671680096398377e-06,
      "loss": 2.2346,
      "step": 3494
    },
    {
      "epoch": 2.3008558262014485,
      "grad_norm": 1.5393112897872925,
      "learning_rate": 2.562557854216281e-06,
      "loss": 1.5464,
      "step": 3495
    },
    {
      "epoch": 2.301514154048716,
      "grad_norm": 7.453502655029297,
      "learning_rate": 2.557951233596184e-06,
      "loss": 1.721,
      "step": 3496
    },
    {
      "epoch": 2.302172481895984,
      "grad_norm": 9.333966255187988,
      "learning_rate": 2.5533481499689462e-06,
      "loss": 1.8136,
      "step": 3497
    },
    {
      "epoch": 2.302830809743252,
      "grad_norm": 9.943741798400879,
      "learning_rate": 2.5487486055222753e-06,
      "loss": 1.8432,
      "step": 3498
    },
    {
      "epoch": 2.30348913759052,
      "grad_norm": 22.98344612121582,
      "learning_rate": 2.544152602442205e-06,
      "loss": 2.1192,
      "step": 3499
    },
    {
      "epoch": 2.3041474654377883,
      "grad_norm": 5.319775581359863,
      "learning_rate": 2.539560142913088e-06,
      "loss": 1.6294,
      "step": 3500
    },
    {
      "epoch": 2.304805793285056,
      "grad_norm": 13.4505615234375,
      "learning_rate": 2.53497122911758e-06,
      "loss": 2.0456,
      "step": 3501
    },
    {
      "epoch": 2.305464121132324,
      "grad_norm": 7.468748569488525,
      "learning_rate": 2.5303858632366686e-06,
      "loss": 1.6384,
      "step": 3502
    },
    {
      "epoch": 2.306122448979592,
      "grad_norm": 1.0909491777420044,
      "learning_rate": 2.5258040474496483e-06,
      "loss": 1.5245,
      "step": 3503
    },
    {
      "epoch": 2.3067807768268596,
      "grad_norm": 8.195344924926758,
      "learning_rate": 2.521225783934117e-06,
      "loss": 1.7744,
      "step": 3504
    },
    {
      "epoch": 2.3074391046741276,
      "grad_norm": 12.414652824401855,
      "learning_rate": 2.5166510748659978e-06,
      "loss": 2.1533,
      "step": 3505
    },
    {
      "epoch": 2.3080974325213957,
      "grad_norm": 0.9020734429359436,
      "learning_rate": 2.5120799224195213e-06,
      "loss": 1.5231,
      "step": 3506
    },
    {
      "epoch": 2.3087557603686637,
      "grad_norm": 28.538177490234375,
      "learning_rate": 2.5075123287672174e-06,
      "loss": 2.2442,
      "step": 3507
    },
    {
      "epoch": 2.3094140882159317,
      "grad_norm": 20.280736923217773,
      "learning_rate": 2.5029482960799367e-06,
      "loss": 1.6894,
      "step": 3508
    },
    {
      "epoch": 2.3100724160631994,
      "grad_norm": 21.888994216918945,
      "learning_rate": 2.498387826526838e-06,
      "loss": 2.3601,
      "step": 3509
    },
    {
      "epoch": 2.3107307439104674,
      "grad_norm": 17.721683502197266,
      "learning_rate": 2.4938309222753767e-06,
      "loss": 2.086,
      "step": 3510
    },
    {
      "epoch": 2.3113890717577354,
      "grad_norm": 2.497664213180542,
      "learning_rate": 2.4892775854913197e-06,
      "loss": 1.5268,
      "step": 3511
    },
    {
      "epoch": 2.312047399605003,
      "grad_norm": 1.1027400493621826,
      "learning_rate": 2.484727818338741e-06,
      "loss": 1.5247,
      "step": 3512
    },
    {
      "epoch": 2.312705727452271,
      "grad_norm": 1.857689380645752,
      "learning_rate": 2.480181622980009e-06,
      "loss": 1.5236,
      "step": 3513
    },
    {
      "epoch": 2.313364055299539,
      "grad_norm": 2.3705263137817383,
      "learning_rate": 2.4756390015758037e-06,
      "loss": 1.5397,
      "step": 3514
    },
    {
      "epoch": 2.314022383146807,
      "grad_norm": 1.5061230659484863,
      "learning_rate": 2.4710999562851034e-06,
      "loss": 1.533,
      "step": 3515
    },
    {
      "epoch": 2.3146807109940752,
      "grad_norm": 1.2573981285095215,
      "learning_rate": 2.4665644892651853e-06,
      "loss": 1.5381,
      "step": 3516
    },
    {
      "epoch": 2.315339038841343,
      "grad_norm": 8.942224502563477,
      "learning_rate": 2.4620326026716266e-06,
      "loss": 1.804,
      "step": 3517
    },
    {
      "epoch": 2.315997366688611,
      "grad_norm": 11.492033004760742,
      "learning_rate": 2.457504298658309e-06,
      "loss": 1.6319,
      "step": 3518
    },
    {
      "epoch": 2.316655694535879,
      "grad_norm": 14.706575393676758,
      "learning_rate": 2.452979579377398e-06,
      "loss": 1.8986,
      "step": 3519
    },
    {
      "epoch": 2.317314022383147,
      "grad_norm": 8.420012474060059,
      "learning_rate": 2.448458446979366e-06,
      "loss": 1.7761,
      "step": 3520
    },
    {
      "epoch": 2.3179723502304146,
      "grad_norm": 23.009674072265625,
      "learning_rate": 2.443940903612978e-06,
      "loss": 2.0593,
      "step": 3521
    },
    {
      "epoch": 2.3186306780776826,
      "grad_norm": 8.915581703186035,
      "learning_rate": 2.4394269514252943e-06,
      "loss": 1.7992,
      "step": 3522
    },
    {
      "epoch": 2.3192890059249507,
      "grad_norm": 11.803868293762207,
      "learning_rate": 2.4349165925616657e-06,
      "loss": 1.8274,
      "step": 3523
    },
    {
      "epoch": 2.3199473337722187,
      "grad_norm": 13.679093360900879,
      "learning_rate": 2.4304098291657407e-06,
      "loss": 1.8725,
      "step": 3524
    },
    {
      "epoch": 2.3206056616194863,
      "grad_norm": 19.88897705078125,
      "learning_rate": 2.425906663379448e-06,
      "loss": 2.3378,
      "step": 3525
    },
    {
      "epoch": 2.3212639894667544,
      "grad_norm": 33.637821197509766,
      "learning_rate": 2.4214070973430183e-06,
      "loss": 3.5805,
      "step": 3526
    },
    {
      "epoch": 2.3219223173140224,
      "grad_norm": 1.2002726793289185,
      "learning_rate": 2.4169111331949647e-06,
      "loss": 1.5297,
      "step": 3527
    },
    {
      "epoch": 2.3225806451612905,
      "grad_norm": 2.9027888774871826,
      "learning_rate": 2.4124187730720916e-06,
      "loss": 1.5583,
      "step": 3528
    },
    {
      "epoch": 2.323238973008558,
      "grad_norm": 10.92726993560791,
      "learning_rate": 2.407930019109488e-06,
      "loss": 1.8331,
      "step": 3529
    },
    {
      "epoch": 2.323897300855826,
      "grad_norm": 15.058769226074219,
      "learning_rate": 2.4034448734405314e-06,
      "loss": 1.8665,
      "step": 3530
    },
    {
      "epoch": 2.324555628703094,
      "grad_norm": 1.9758145809173584,
      "learning_rate": 2.3989633381968856e-06,
      "loss": 1.5293,
      "step": 3531
    },
    {
      "epoch": 2.3252139565503622,
      "grad_norm": 1.873902678489685,
      "learning_rate": 2.3944854155084894e-06,
      "loss": 1.5406,
      "step": 3532
    },
    {
      "epoch": 2.32587228439763,
      "grad_norm": 24.12653923034668,
      "learning_rate": 2.390011107503577e-06,
      "loss": 1.8542,
      "step": 3533
    },
    {
      "epoch": 2.326530612244898,
      "grad_norm": 14.949602127075195,
      "learning_rate": 2.3855404163086558e-06,
      "loss": 2.0777,
      "step": 3534
    },
    {
      "epoch": 2.327188940092166,
      "grad_norm": 6.022607803344727,
      "learning_rate": 2.381073344048519e-06,
      "loss": 1.6126,
      "step": 3535
    },
    {
      "epoch": 2.327847267939434,
      "grad_norm": 10.998358726501465,
      "learning_rate": 2.3766098928462377e-06,
      "loss": 2.1623,
      "step": 3536
    },
    {
      "epoch": 2.3285055957867016,
      "grad_norm": 0.7950843572616577,
      "learning_rate": 2.3721500648231666e-06,
      "loss": 1.5196,
      "step": 3537
    },
    {
      "epoch": 2.3291639236339696,
      "grad_norm": 19.985687255859375,
      "learning_rate": 2.3676938620989277e-06,
      "loss": 2.2418,
      "step": 3538
    },
    {
      "epoch": 2.3298222514812377,
      "grad_norm": 2.0546154975891113,
      "learning_rate": 2.3632412867914288e-06,
      "loss": 1.5635,
      "step": 3539
    },
    {
      "epoch": 2.3304805793285057,
      "grad_norm": 13.829314231872559,
      "learning_rate": 2.3587923410168558e-06,
      "loss": 1.9096,
      "step": 3540
    },
    {
      "epoch": 2.3311389071757738,
      "grad_norm": 18.513652801513672,
      "learning_rate": 2.354347026889655e-06,
      "loss": 1.7697,
      "step": 3541
    },
    {
      "epoch": 2.3317972350230414,
      "grad_norm": 0.6160383224487305,
      "learning_rate": 2.3499053465225684e-06,
      "loss": 1.5146,
      "step": 3542
    },
    {
      "epoch": 2.3324555628703094,
      "grad_norm": 1.5514163970947266,
      "learning_rate": 2.3454673020265973e-06,
      "loss": 1.5209,
      "step": 3543
    },
    {
      "epoch": 2.3331138907175775,
      "grad_norm": 0.7833569645881653,
      "learning_rate": 2.3410328955110117e-06,
      "loss": 1.5133,
      "step": 3544
    },
    {
      "epoch": 2.333772218564845,
      "grad_norm": 16.71873664855957,
      "learning_rate": 2.336602129083362e-06,
      "loss": 2.1408,
      "step": 3545
    },
    {
      "epoch": 2.334430546412113,
      "grad_norm": 2.3024609088897705,
      "learning_rate": 2.3321750048494663e-06,
      "loss": 1.5671,
      "step": 3546
    },
    {
      "epoch": 2.335088874259381,
      "grad_norm": 53.881439208984375,
      "learning_rate": 2.3277515249134063e-06,
      "loss": 2.0529,
      "step": 3547
    },
    {
      "epoch": 2.335747202106649,
      "grad_norm": 1.0553081035614014,
      "learning_rate": 2.3233316913775363e-06,
      "loss": 1.5236,
      "step": 3548
    },
    {
      "epoch": 2.3364055299539173,
      "grad_norm": 16.787338256835938,
      "learning_rate": 2.3189155063424784e-06,
      "loss": 2.0203,
      "step": 3549
    },
    {
      "epoch": 2.337063857801185,
      "grad_norm": 10.712693214416504,
      "learning_rate": 2.3145029719071176e-06,
      "loss": 1.7821,
      "step": 3550
    },
    {
      "epoch": 2.337722185648453,
      "grad_norm": 23.161439895629883,
      "learning_rate": 2.3100940901686054e-06,
      "loss": 1.9461,
      "step": 3551
    },
    {
      "epoch": 2.338380513495721,
      "grad_norm": 4.532089710235596,
      "learning_rate": 2.3056888632223616e-06,
      "loss": 1.6303,
      "step": 3552
    },
    {
      "epoch": 2.3390388413429886,
      "grad_norm": 2.1194353103637695,
      "learning_rate": 2.3012872931620567e-06,
      "loss": 1.558,
      "step": 3553
    },
    {
      "epoch": 2.3396971691902566,
      "grad_norm": 16.77530860900879,
      "learning_rate": 2.2968893820796357e-06,
      "loss": 2.0578,
      "step": 3554
    },
    {
      "epoch": 2.3403554970375247,
      "grad_norm": 24.839256286621094,
      "learning_rate": 2.2924951320652975e-06,
      "loss": 2.5651,
      "step": 3555
    },
    {
      "epoch": 2.3410138248847927,
      "grad_norm": 13.250277519226074,
      "learning_rate": 2.288104545207507e-06,
      "loss": 2.1252,
      "step": 3556
    },
    {
      "epoch": 2.3416721527320608,
      "grad_norm": 2.3857998847961426,
      "learning_rate": 2.2837176235929814e-06,
      "loss": 1.5457,
      "step": 3557
    },
    {
      "epoch": 2.3423304805793284,
      "grad_norm": 15.766363143920898,
      "learning_rate": 2.2793343693067026e-06,
      "loss": 1.6173,
      "step": 3558
    },
    {
      "epoch": 2.3429888084265964,
      "grad_norm": 7.920743465423584,
      "learning_rate": 2.274954784431901e-06,
      "loss": 1.7014,
      "step": 3559
    },
    {
      "epoch": 2.3436471362738645,
      "grad_norm": 0.734096348285675,
      "learning_rate": 2.270578871050071e-06,
      "loss": 1.5155,
      "step": 3560
    },
    {
      "epoch": 2.3443054641211325,
      "grad_norm": 12.746752738952637,
      "learning_rate": 2.266206631240958e-06,
      "loss": 2.1557,
      "step": 3561
    },
    {
      "epoch": 2.3449637919684,
      "grad_norm": 3.2130630016326904,
      "learning_rate": 2.2618380670825622e-06,
      "loss": 1.6116,
      "step": 3562
    },
    {
      "epoch": 2.345622119815668,
      "grad_norm": 1.547188639640808,
      "learning_rate": 2.2574731806511397e-06,
      "loss": 1.5307,
      "step": 3563
    },
    {
      "epoch": 2.346280447662936,
      "grad_norm": 1.9029157161712646,
      "learning_rate": 2.2531119740211924e-06,
      "loss": 1.5597,
      "step": 3564
    },
    {
      "epoch": 2.3469387755102042,
      "grad_norm": 8.473313331604004,
      "learning_rate": 2.2487544492654832e-06,
      "loss": 1.8823,
      "step": 3565
    },
    {
      "epoch": 2.347597103357472,
      "grad_norm": 8.078512191772461,
      "learning_rate": 2.244400608455012e-06,
      "loss": 1.7916,
      "step": 3566
    },
    {
      "epoch": 2.34825543120474,
      "grad_norm": 1.4731318950653076,
      "learning_rate": 2.240050453659036e-06,
      "loss": 1.5262,
      "step": 3567
    },
    {
      "epoch": 2.348913759052008,
      "grad_norm": 23.06790542602539,
      "learning_rate": 2.2357039869450625e-06,
      "loss": 2.4205,
      "step": 3568
    },
    {
      "epoch": 2.349572086899276,
      "grad_norm": 16.00461196899414,
      "learning_rate": 2.2313612103788406e-06,
      "loss": 1.6497,
      "step": 3569
    },
    {
      "epoch": 2.3502304147465436,
      "grad_norm": 0.8707455396652222,
      "learning_rate": 2.2270221260243675e-06,
      "loss": 1.5131,
      "step": 3570
    },
    {
      "epoch": 2.3508887425938116,
      "grad_norm": 8.41691780090332,
      "learning_rate": 2.2226867359438897e-06,
      "loss": 1.7926,
      "step": 3571
    },
    {
      "epoch": 2.3515470704410797,
      "grad_norm": 8.9900541305542,
      "learning_rate": 2.2183550421978885e-06,
      "loss": 1.8227,
      "step": 3572
    },
    {
      "epoch": 2.3522053982883477,
      "grad_norm": 0.8037930130958557,
      "learning_rate": 2.2140270468450966e-06,
      "loss": 1.5272,
      "step": 3573
    },
    {
      "epoch": 2.352863726135616,
      "grad_norm": 1.491220474243164,
      "learning_rate": 2.2097027519424886e-06,
      "loss": 1.5323,
      "step": 3574
    },
    {
      "epoch": 2.3535220539828834,
      "grad_norm": 18.623674392700195,
      "learning_rate": 2.2053821595452705e-06,
      "loss": 2.204,
      "step": 3575
    },
    {
      "epoch": 2.3541803818301514,
      "grad_norm": 16.71721076965332,
      "learning_rate": 2.201065271706905e-06,
      "loss": 2.136,
      "step": 3576
    },
    {
      "epoch": 2.3548387096774195,
      "grad_norm": 1.0653102397918701,
      "learning_rate": 2.196752090479083e-06,
      "loss": 1.5203,
      "step": 3577
    },
    {
      "epoch": 2.355497037524687,
      "grad_norm": 11.46030044555664,
      "learning_rate": 2.192442617911733e-06,
      "loss": 1.7825,
      "step": 3578
    },
    {
      "epoch": 2.356155365371955,
      "grad_norm": 9.228653907775879,
      "learning_rate": 2.188136856053027e-06,
      "loss": 1.8933,
      "step": 3579
    },
    {
      "epoch": 2.356813693219223,
      "grad_norm": 0.6908730864524841,
      "learning_rate": 2.183834806949371e-06,
      "loss": 1.5132,
      "step": 3580
    },
    {
      "epoch": 2.3574720210664912,
      "grad_norm": 15.357704162597656,
      "learning_rate": 2.1795364726454016e-06,
      "loss": 2.4187,
      "step": 3581
    },
    {
      "epoch": 2.3581303489137593,
      "grad_norm": 0.9134604930877686,
      "learning_rate": 2.175241855183995e-06,
      "loss": 1.5168,
      "step": 3582
    },
    {
      "epoch": 2.358788676761027,
      "grad_norm": 36.7554931640625,
      "learning_rate": 2.170950956606267e-06,
      "loss": 2.1478,
      "step": 3583
    },
    {
      "epoch": 2.359447004608295,
      "grad_norm": 11.127273559570312,
      "learning_rate": 2.16666377895155e-06,
      "loss": 2.1149,
      "step": 3584
    },
    {
      "epoch": 2.360105332455563,
      "grad_norm": 1.0533874034881592,
      "learning_rate": 2.1623803242574214e-06,
      "loss": 1.5434,
      "step": 3585
    },
    {
      "epoch": 2.3607636603028306,
      "grad_norm": 42.709651947021484,
      "learning_rate": 2.158100594559688e-06,
      "loss": 2.9999,
      "step": 3586
    },
    {
      "epoch": 2.3614219881500986,
      "grad_norm": 0.9934638142585754,
      "learning_rate": 2.1538245918923738e-06,
      "loss": 1.5196,
      "step": 3587
    },
    {
      "epoch": 2.3620803159973667,
      "grad_norm": 31.30917739868164,
      "learning_rate": 2.1495523182877466e-06,
      "loss": 2.5547,
      "step": 3588
    },
    {
      "epoch": 2.3627386438446347,
      "grad_norm": 36.941097259521484,
      "learning_rate": 2.1452837757762944e-06,
      "loss": 2.6296,
      "step": 3589
    },
    {
      "epoch": 2.3633969716919028,
      "grad_norm": 54.2052001953125,
      "learning_rate": 2.1410189663867344e-06,
      "loss": 3.4343,
      "step": 3590
    },
    {
      "epoch": 2.3640552995391704,
      "grad_norm": 2.1915836334228516,
      "learning_rate": 2.1367578921460075e-06,
      "loss": 1.5739,
      "step": 3591
    },
    {
      "epoch": 2.3647136273864384,
      "grad_norm": 14.282505989074707,
      "learning_rate": 2.132500555079283e-06,
      "loss": 1.9669,
      "step": 3592
    },
    {
      "epoch": 2.3653719552337065,
      "grad_norm": 2.57006573677063,
      "learning_rate": 2.128246957209946e-06,
      "loss": 1.5721,
      "step": 3593
    },
    {
      "epoch": 2.3660302830809745,
      "grad_norm": 1.2615207433700562,
      "learning_rate": 2.123997100559614e-06,
      "loss": 1.5196,
      "step": 3594
    },
    {
      "epoch": 2.366688610928242,
      "grad_norm": 0.9666427373886108,
      "learning_rate": 2.1197509871481204e-06,
      "loss": 1.5146,
      "step": 3595
    },
    {
      "epoch": 2.36734693877551,
      "grad_norm": 11.437259674072266,
      "learning_rate": 2.1155086189935227e-06,
      "loss": 1.6434,
      "step": 3596
    },
    {
      "epoch": 2.368005266622778,
      "grad_norm": 13.448596000671387,
      "learning_rate": 2.1112699981120953e-06,
      "loss": 1.9625,
      "step": 3597
    },
    {
      "epoch": 2.3686635944700463,
      "grad_norm": 14.342863082885742,
      "learning_rate": 2.1070351265183385e-06,
      "loss": 1.9461,
      "step": 3598
    },
    {
      "epoch": 2.369321922317314,
      "grad_norm": 1.1039198637008667,
      "learning_rate": 2.10280400622496e-06,
      "loss": 1.5172,
      "step": 3599
    },
    {
      "epoch": 2.369980250164582,
      "grad_norm": 1.4594112634658813,
      "learning_rate": 2.0985766392428907e-06,
      "loss": 1.5255,
      "step": 3600
    },
    {
      "epoch": 2.37063857801185,
      "grad_norm": 37.930484771728516,
      "learning_rate": 2.0943530275812816e-06,
      "loss": 2.1971,
      "step": 3601
    },
    {
      "epoch": 2.371296905859118,
      "grad_norm": 14.511642456054688,
      "learning_rate": 2.090133173247486e-06,
      "loss": 1.7197,
      "step": 3602
    },
    {
      "epoch": 2.3719552337063856,
      "grad_norm": 9.685101509094238,
      "learning_rate": 2.0859170782470873e-06,
      "loss": 1.6988,
      "step": 3603
    },
    {
      "epoch": 2.3726135615536537,
      "grad_norm": 2.436659574508667,
      "learning_rate": 2.0817047445838735e-06,
      "loss": 1.534,
      "step": 3604
    },
    {
      "epoch": 2.3732718894009217,
      "grad_norm": 30.509204864501953,
      "learning_rate": 2.077496174259849e-06,
      "loss": 2.1203,
      "step": 3605
    },
    {
      "epoch": 2.3739302172481898,
      "grad_norm": 5.269779682159424,
      "learning_rate": 2.07329136927522e-06,
      "loss": 1.5706,
      "step": 3606
    },
    {
      "epoch": 2.3745885450954574,
      "grad_norm": 32.19617462158203,
      "learning_rate": 2.0690903316284137e-06,
      "loss": 1.8941,
      "step": 3607
    },
    {
      "epoch": 2.3752468729427254,
      "grad_norm": 13.679991722106934,
      "learning_rate": 2.064893063316066e-06,
      "loss": 2.0412,
      "step": 3608
    },
    {
      "epoch": 2.3759052007899935,
      "grad_norm": 46.15958023071289,
      "learning_rate": 2.0606995663330097e-06,
      "loss": 2.8132,
      "step": 3609
    },
    {
      "epoch": 2.3765635286372615,
      "grad_norm": 22.611225128173828,
      "learning_rate": 2.056509842672302e-06,
      "loss": 2.2648,
      "step": 3610
    },
    {
      "epoch": 2.377221856484529,
      "grad_norm": 1.2907814979553223,
      "learning_rate": 2.0523238943251987e-06,
      "loss": 1.5247,
      "step": 3611
    },
    {
      "epoch": 2.377880184331797,
      "grad_norm": 21.905546188354492,
      "learning_rate": 2.0481417232811575e-06,
      "loss": 2.6641,
      "step": 3612
    },
    {
      "epoch": 2.378538512179065,
      "grad_norm": 1.2726194858551025,
      "learning_rate": 2.043963331527845e-06,
      "loss": 1.5177,
      "step": 3613
    },
    {
      "epoch": 2.3791968400263332,
      "grad_norm": 9.857342720031738,
      "learning_rate": 2.0397887210511345e-06,
      "loss": 1.8812,
      "step": 3614
    },
    {
      "epoch": 2.3798551678736013,
      "grad_norm": 35.54317092895508,
      "learning_rate": 2.0356178938350945e-06,
      "loss": 2.7025,
      "step": 3615
    },
    {
      "epoch": 2.380513495720869,
      "grad_norm": 1.2994524240493774,
      "learning_rate": 2.031450851862e-06,
      "loss": 1.5206,
      "step": 3616
    },
    {
      "epoch": 2.381171823568137,
      "grad_norm": 19.330244064331055,
      "learning_rate": 2.027287597112333e-06,
      "loss": 2.203,
      "step": 3617
    },
    {
      "epoch": 2.381830151415405,
      "grad_norm": 34.194915771484375,
      "learning_rate": 2.0231281315647633e-06,
      "loss": 2.6345,
      "step": 3618
    },
    {
      "epoch": 2.3824884792626726,
      "grad_norm": 1.6281108856201172,
      "learning_rate": 2.0189724571961677e-06,
      "loss": 1.527,
      "step": 3619
    },
    {
      "epoch": 2.3831468071099406,
      "grad_norm": 5.8346099853515625,
      "learning_rate": 2.0148205759816218e-06,
      "loss": 1.7297,
      "step": 3620
    },
    {
      "epoch": 2.3838051349572087,
      "grad_norm": 14.588346481323242,
      "learning_rate": 2.010672489894392e-06,
      "loss": 1.962,
      "step": 3621
    },
    {
      "epoch": 2.3844634628044767,
      "grad_norm": 21.082521438598633,
      "learning_rate": 2.006528200905945e-06,
      "loss": 2.325,
      "step": 3622
    },
    {
      "epoch": 2.385121790651745,
      "grad_norm": 12.287524223327637,
      "learning_rate": 2.002387710985945e-06,
      "loss": 1.918,
      "step": 3623
    },
    {
      "epoch": 2.3857801184990124,
      "grad_norm": 19.68651008605957,
      "learning_rate": 1.998251022102248e-06,
      "loss": 2.1805,
      "step": 3624
    },
    {
      "epoch": 2.3864384463462804,
      "grad_norm": 17.661725997924805,
      "learning_rate": 1.9941181362209026e-06,
      "loss": 1.8916,
      "step": 3625
    },
    {
      "epoch": 2.3870967741935485,
      "grad_norm": 10.72072982788086,
      "learning_rate": 1.9899890553061565e-06,
      "loss": 1.8484,
      "step": 3626
    },
    {
      "epoch": 2.387755102040816,
      "grad_norm": 5.490837097167969,
      "learning_rate": 1.9858637813204352e-06,
      "loss": 1.5921,
      "step": 3627
    },
    {
      "epoch": 2.388413429888084,
      "grad_norm": 6.586779594421387,
      "learning_rate": 1.981742316224368e-06,
      "loss": 1.709,
      "step": 3628
    },
    {
      "epoch": 2.389071757735352,
      "grad_norm": 1.6330630779266357,
      "learning_rate": 1.977624661976769e-06,
      "loss": 1.5195,
      "step": 3629
    },
    {
      "epoch": 2.3897300855826202,
      "grad_norm": 17.342702865600586,
      "learning_rate": 1.973510820534641e-06,
      "loss": 2.0059,
      "step": 3630
    },
    {
      "epoch": 2.3903884134298883,
      "grad_norm": 16.623470306396484,
      "learning_rate": 1.9694007938531756e-06,
      "loss": 1.9978,
      "step": 3631
    },
    {
      "epoch": 2.391046741277156,
      "grad_norm": 11.32774829864502,
      "learning_rate": 1.965294583885754e-06,
      "loss": 1.6224,
      "step": 3632
    },
    {
      "epoch": 2.391705069124424,
      "grad_norm": 11.488201141357422,
      "learning_rate": 1.961192192583934e-06,
      "loss": 1.851,
      "step": 3633
    },
    {
      "epoch": 2.392363396971692,
      "grad_norm": 2.156064033508301,
      "learning_rate": 1.957093621897469e-06,
      "loss": 1.5267,
      "step": 3634
    },
    {
      "epoch": 2.39302172481896,
      "grad_norm": 1.4332925081253052,
      "learning_rate": 1.95299887377429e-06,
      "loss": 1.5146,
      "step": 3635
    },
    {
      "epoch": 2.3936800526662276,
      "grad_norm": 16.716224670410156,
      "learning_rate": 1.9489079501605156e-06,
      "loss": 1.9679,
      "step": 3636
    },
    {
      "epoch": 2.3943383805134957,
      "grad_norm": 39.00023651123047,
      "learning_rate": 1.9448208530004444e-06,
      "loss": 1.951,
      "step": 3637
    },
    {
      "epoch": 2.3949967083607637,
      "grad_norm": 22.849876403808594,
      "learning_rate": 1.9407375842365584e-06,
      "loss": 2.1869,
      "step": 3638
    },
    {
      "epoch": 2.3956550362080318,
      "grad_norm": 10.719441413879395,
      "learning_rate": 1.9366581458095133e-06,
      "loss": 1.8352,
      "step": 3639
    },
    {
      "epoch": 2.3963133640552994,
      "grad_norm": 18.082826614379883,
      "learning_rate": 1.932582539658154e-06,
      "loss": 1.7165,
      "step": 3640
    },
    {
      "epoch": 2.3969716919025674,
      "grad_norm": 7.502495765686035,
      "learning_rate": 1.9285107677195003e-06,
      "loss": 1.8022,
      "step": 3641
    },
    {
      "epoch": 2.3976300197498355,
      "grad_norm": 7.810822486877441,
      "learning_rate": 1.9244428319287413e-06,
      "loss": 1.756,
      "step": 3642
    },
    {
      "epoch": 2.3982883475971035,
      "grad_norm": 1.5517319440841675,
      "learning_rate": 1.920378734219259e-06,
      "loss": 1.5252,
      "step": 3643
    },
    {
      "epoch": 2.398946675444371,
      "grad_norm": 0.5561437010765076,
      "learning_rate": 1.9163184765225993e-06,
      "loss": 1.5148,
      "step": 3644
    },
    {
      "epoch": 2.399605003291639,
      "grad_norm": 41.34284973144531,
      "learning_rate": 1.9122620607684896e-06,
      "loss": 2.2118,
      "step": 3645
    },
    {
      "epoch": 2.400263331138907,
      "grad_norm": 3.7831358909606934,
      "learning_rate": 1.9082094888848224e-06,
      "loss": 1.5452,
      "step": 3646
    },
    {
      "epoch": 2.4009216589861753,
      "grad_norm": 16.445762634277344,
      "learning_rate": 1.9041607627976733e-06,
      "loss": 1.7522,
      "step": 3647
    },
    {
      "epoch": 2.401579986833443,
      "grad_norm": 3.002725601196289,
      "learning_rate": 1.9001158844312872e-06,
      "loss": 1.5978,
      "step": 3648
    },
    {
      "epoch": 2.402238314680711,
      "grad_norm": 5.503793716430664,
      "learning_rate": 1.8960748557080743e-06,
      "loss": 1.6125,
      "step": 3649
    },
    {
      "epoch": 2.402896642527979,
      "grad_norm": 9.00409984588623,
      "learning_rate": 1.8920376785486194e-06,
      "loss": 1.8193,
      "step": 3650
    },
    {
      "epoch": 2.403554970375247,
      "grad_norm": 0.9945113062858582,
      "learning_rate": 1.8880043548716864e-06,
      "loss": 1.5201,
      "step": 3651
    },
    {
      "epoch": 2.4042132982225146,
      "grad_norm": 8.108057022094727,
      "learning_rate": 1.8839748865941898e-06,
      "loss": 1.6141,
      "step": 3652
    },
    {
      "epoch": 2.4048716260697827,
      "grad_norm": 3.4797935485839844,
      "learning_rate": 1.8799492756312233e-06,
      "loss": 1.5954,
      "step": 3653
    },
    {
      "epoch": 2.4055299539170507,
      "grad_norm": 3.3278920650482178,
      "learning_rate": 1.8759275238960473e-06,
      "loss": 1.5558,
      "step": 3654
    },
    {
      "epoch": 2.4061882817643188,
      "grad_norm": 7.291049480438232,
      "learning_rate": 1.8719096333000808e-06,
      "loss": 1.6962,
      "step": 3655
    },
    {
      "epoch": 2.406846609611587,
      "grad_norm": 39.91020202636719,
      "learning_rate": 1.867895605752913e-06,
      "loss": 4.26,
      "step": 3656
    },
    {
      "epoch": 2.4075049374588544,
      "grad_norm": 1.1943506002426147,
      "learning_rate": 1.8638854431622988e-06,
      "loss": 1.5235,
      "step": 3657
    },
    {
      "epoch": 2.4081632653061225,
      "grad_norm": 23.21625328063965,
      "learning_rate": 1.8598791474341516e-06,
      "loss": 2.2913,
      "step": 3658
    },
    {
      "epoch": 2.4088215931533905,
      "grad_norm": 14.782649993896484,
      "learning_rate": 1.8558767204725503e-06,
      "loss": 1.9104,
      "step": 3659
    },
    {
      "epoch": 2.409479921000658,
      "grad_norm": 8.993706703186035,
      "learning_rate": 1.8518781641797368e-06,
      "loss": 1.7649,
      "step": 3660
    },
    {
      "epoch": 2.410138248847926,
      "grad_norm": 45.086360931396484,
      "learning_rate": 1.847883480456104e-06,
      "loss": 2.9564,
      "step": 3661
    },
    {
      "epoch": 2.410796576695194,
      "grad_norm": 4.984930038452148,
      "learning_rate": 1.8438926712002147e-06,
      "loss": 1.6412,
      "step": 3662
    },
    {
      "epoch": 2.4114549045424623,
      "grad_norm": 4.143441200256348,
      "learning_rate": 1.8399057383087859e-06,
      "loss": 1.6767,
      "step": 3663
    },
    {
      "epoch": 2.4121132323897303,
      "grad_norm": 9.665018081665039,
      "learning_rate": 1.835922683676692e-06,
      "loss": 1.7452,
      "step": 3664
    },
    {
      "epoch": 2.412771560236998,
      "grad_norm": 15.540496826171875,
      "learning_rate": 1.8319435091969662e-06,
      "loss": 1.8656,
      "step": 3665
    },
    {
      "epoch": 2.413429888084266,
      "grad_norm": 9.994342803955078,
      "learning_rate": 1.8279682167607982e-06,
      "loss": 1.9598,
      "step": 3666
    },
    {
      "epoch": 2.414088215931534,
      "grad_norm": 15.984357833862305,
      "learning_rate": 1.8239968082575254e-06,
      "loss": 2.0501,
      "step": 3667
    },
    {
      "epoch": 2.4147465437788016,
      "grad_norm": 8.98813247680664,
      "learning_rate": 1.820029285574647e-06,
      "loss": 1.745,
      "step": 3668
    },
    {
      "epoch": 2.4154048716260696,
      "grad_norm": 1.9363391399383545,
      "learning_rate": 1.8160656505978147e-06,
      "loss": 1.5238,
      "step": 3669
    },
    {
      "epoch": 2.4160631994733377,
      "grad_norm": 1.0983809232711792,
      "learning_rate": 1.812105905210828e-06,
      "loss": 1.5173,
      "step": 3670
    },
    {
      "epoch": 2.4167215273206057,
      "grad_norm": 21.8951416015625,
      "learning_rate": 1.808150051295644e-06,
      "loss": 2.2266,
      "step": 3671
    },
    {
      "epoch": 2.417379855167874,
      "grad_norm": 0.9350737929344177,
      "learning_rate": 1.8041980907323664e-06,
      "loss": 1.5211,
      "step": 3672
    },
    {
      "epoch": 2.4180381830151414,
      "grad_norm": 10.904083251953125,
      "learning_rate": 1.800250025399245e-06,
      "loss": 1.8253,
      "step": 3673
    },
    {
      "epoch": 2.4186965108624094,
      "grad_norm": 44.54563903808594,
      "learning_rate": 1.7963058571726855e-06,
      "loss": 2.7021,
      "step": 3674
    },
    {
      "epoch": 2.4193548387096775,
      "grad_norm": 17.821834564208984,
      "learning_rate": 1.7923655879272395e-06,
      "loss": 2.0587,
      "step": 3675
    },
    {
      "epoch": 2.4200131665569455,
      "grad_norm": 11.234856605529785,
      "learning_rate": 1.7884292195355979e-06,
      "loss": 1.8612,
      "step": 3676
    },
    {
      "epoch": 2.420671494404213,
      "grad_norm": 1.0014395713806152,
      "learning_rate": 1.784496753868611e-06,
      "loss": 1.5161,
      "step": 3677
    },
    {
      "epoch": 2.421329822251481,
      "grad_norm": 20.875926971435547,
      "learning_rate": 1.7805681927952667e-06,
      "loss": 1.9756,
      "step": 3678
    },
    {
      "epoch": 2.4219881500987492,
      "grad_norm": 5.3666486740112305,
      "learning_rate": 1.776643538182693e-06,
      "loss": 1.7001,
      "step": 3679
    },
    {
      "epoch": 2.4226464779460173,
      "grad_norm": 25.725244522094727,
      "learning_rate": 1.7727227918961687e-06,
      "loss": 2.5706,
      "step": 3680
    },
    {
      "epoch": 2.423304805793285,
      "grad_norm": 0.5446825623512268,
      "learning_rate": 1.7688059557991123e-06,
      "loss": 1.5077,
      "step": 3681
    },
    {
      "epoch": 2.423963133640553,
      "grad_norm": 14.940321922302246,
      "learning_rate": 1.7648930317530867e-06,
      "loss": 1.8951,
      "step": 3682
    },
    {
      "epoch": 2.424621461487821,
      "grad_norm": 7.980448246002197,
      "learning_rate": 1.7609840216177853e-06,
      "loss": 1.8457,
      "step": 3683
    },
    {
      "epoch": 2.425279789335089,
      "grad_norm": 1.9838496446609497,
      "learning_rate": 1.7570789272510546e-06,
      "loss": 1.529,
      "step": 3684
    },
    {
      "epoch": 2.4259381171823566,
      "grad_norm": 23.545320510864258,
      "learning_rate": 1.7531777505088766e-06,
      "loss": 2.2135,
      "step": 3685
    },
    {
      "epoch": 2.4265964450296247,
      "grad_norm": 11.168214797973633,
      "learning_rate": 1.7492804932453621e-06,
      "loss": 1.8711,
      "step": 3686
    },
    {
      "epoch": 2.4272547728768927,
      "grad_norm": 11.220712661743164,
      "learning_rate": 1.74538715731277e-06,
      "loss": 1.8625,
      "step": 3687
    },
    {
      "epoch": 2.4279131007241608,
      "grad_norm": 27.626972198486328,
      "learning_rate": 1.7414977445614933e-06,
      "loss": 1.9732,
      "step": 3688
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 0.6879896521568298,
      "learning_rate": 1.7376122568400533e-06,
      "loss": 1.5158,
      "step": 3689
    },
    {
      "epoch": 2.4292297564186964,
      "grad_norm": 18.861324310302734,
      "learning_rate": 1.7337306959951128e-06,
      "loss": 2.1994,
      "step": 3690
    },
    {
      "epoch": 2.4298880842659645,
      "grad_norm": 1.8027716875076294,
      "learning_rate": 1.7298530638714695e-06,
      "loss": 1.5315,
      "step": 3691
    },
    {
      "epoch": 2.4305464121132325,
      "grad_norm": 27.93828010559082,
      "learning_rate": 1.725979362312048e-06,
      "loss": 2.5158,
      "step": 3692
    },
    {
      "epoch": 2.4312047399605,
      "grad_norm": 23.10713005065918,
      "learning_rate": 1.7221095931579091e-06,
      "loss": 2.3608,
      "step": 3693
    },
    {
      "epoch": 2.431863067807768,
      "grad_norm": 1.138879418373108,
      "learning_rate": 1.718243758248247e-06,
      "loss": 1.522,
      "step": 3694
    },
    {
      "epoch": 2.4325213956550362,
      "grad_norm": 19.109725952148438,
      "learning_rate": 1.7143818594203755e-06,
      "loss": 2.4001,
      "step": 3695
    },
    {
      "epoch": 2.4331797235023043,
      "grad_norm": 37.49034881591797,
      "learning_rate": 1.7105238985097472e-06,
      "loss": 2.0796,
      "step": 3696
    },
    {
      "epoch": 2.4338380513495723,
      "grad_norm": 1.5899633169174194,
      "learning_rate": 1.7066698773499413e-06,
      "loss": 1.5251,
      "step": 3697
    },
    {
      "epoch": 2.43449637919684,
      "grad_norm": 21.441944122314453,
      "learning_rate": 1.7028197977726647e-06,
      "loss": 2.4823,
      "step": 3698
    },
    {
      "epoch": 2.435154707044108,
      "grad_norm": 24.51542091369629,
      "learning_rate": 1.6989736616077479e-06,
      "loss": 2.4362,
      "step": 3699
    },
    {
      "epoch": 2.435813034891376,
      "grad_norm": 0.8255374431610107,
      "learning_rate": 1.6951314706831546e-06,
      "loss": 1.5138,
      "step": 3700
    },
    {
      "epoch": 2.4364713627386436,
      "grad_norm": 2.0937411785125732,
      "learning_rate": 1.6912932268249605e-06,
      "loss": 1.5234,
      "step": 3701
    },
    {
      "epoch": 2.4371296905859117,
      "grad_norm": 1.3913732767105103,
      "learning_rate": 1.6874589318573764e-06,
      "loss": 1.5188,
      "step": 3702
    },
    {
      "epoch": 2.4377880184331797,
      "grad_norm": 0.8142631649971008,
      "learning_rate": 1.6836285876027337e-06,
      "loss": 1.5138,
      "step": 3703
    },
    {
      "epoch": 2.4384463462804478,
      "grad_norm": 0.734169602394104,
      "learning_rate": 1.6798021958814858e-06,
      "loss": 1.5103,
      "step": 3704
    },
    {
      "epoch": 2.439104674127716,
      "grad_norm": 35.817665100097656,
      "learning_rate": 1.675979758512205e-06,
      "loss": 2.7735,
      "step": 3705
    },
    {
      "epoch": 2.4397630019749834,
      "grad_norm": 11.352734565734863,
      "learning_rate": 1.6721612773115913e-06,
      "loss": 1.8245,
      "step": 3706
    },
    {
      "epoch": 2.4404213298222515,
      "grad_norm": 20.395536422729492,
      "learning_rate": 1.6683467540944543e-06,
      "loss": 2.0125,
      "step": 3707
    },
    {
      "epoch": 2.4410796576695195,
      "grad_norm": 10.774995803833008,
      "learning_rate": 1.664536190673729e-06,
      "loss": 1.8013,
      "step": 3708
    },
    {
      "epoch": 2.441737985516787,
      "grad_norm": 16.94754409790039,
      "learning_rate": 1.6607295888604703e-06,
      "loss": 2.0809,
      "step": 3709
    },
    {
      "epoch": 2.442396313364055,
      "grad_norm": 1.3268277645111084,
      "learning_rate": 1.6569269504638396e-06,
      "loss": 1.5258,
      "step": 3710
    },
    {
      "epoch": 2.443054641211323,
      "grad_norm": 2.9436538219451904,
      "learning_rate": 1.6531282772911295e-06,
      "loss": 1.5271,
      "step": 3711
    },
    {
      "epoch": 2.4437129690585913,
      "grad_norm": 27.370744705200195,
      "learning_rate": 1.649333571147741e-06,
      "loss": 1.9159,
      "step": 3712
    },
    {
      "epoch": 2.4443712969058593,
      "grad_norm": 1.637170433998108,
      "learning_rate": 1.6455428338371837e-06,
      "loss": 1.53,
      "step": 3713
    },
    {
      "epoch": 2.445029624753127,
      "grad_norm": 20.923181533813477,
      "learning_rate": 1.6417560671610899e-06,
      "loss": 2.2577,
      "step": 3714
    },
    {
      "epoch": 2.445687952600395,
      "grad_norm": 16.683225631713867,
      "learning_rate": 1.6379732729192032e-06,
      "loss": 1.8355,
      "step": 3715
    },
    {
      "epoch": 2.446346280447663,
      "grad_norm": 36.33185958862305,
      "learning_rate": 1.6341944529093735e-06,
      "loss": 2.2838,
      "step": 3716
    },
    {
      "epoch": 2.447004608294931,
      "grad_norm": 29.152162551879883,
      "learning_rate": 1.630419608927566e-06,
      "loss": 2.6536,
      "step": 3717
    },
    {
      "epoch": 2.4476629361421987,
      "grad_norm": 17.809986114501953,
      "learning_rate": 1.6266487427678612e-06,
      "loss": 1.9092,
      "step": 3718
    },
    {
      "epoch": 2.4483212639894667,
      "grad_norm": 4.2885847091674805,
      "learning_rate": 1.622881856222439e-06,
      "loss": 1.6072,
      "step": 3719
    },
    {
      "epoch": 2.4489795918367347,
      "grad_norm": 29.443254470825195,
      "learning_rate": 1.6191189510815942e-06,
      "loss": 2.1565,
      "step": 3720
    },
    {
      "epoch": 2.449637919684003,
      "grad_norm": 20.567209243774414,
      "learning_rate": 1.6153600291337278e-06,
      "loss": 2.2869,
      "step": 3721
    },
    {
      "epoch": 2.4502962475312704,
      "grad_norm": 1.6315577030181885,
      "learning_rate": 1.6116050921653504e-06,
      "loss": 1.5279,
      "step": 3722
    },
    {
      "epoch": 2.4509545753785384,
      "grad_norm": 0.8694968223571777,
      "learning_rate": 1.6078541419610716e-06,
      "loss": 1.5153,
      "step": 3723
    },
    {
      "epoch": 2.4516129032258065,
      "grad_norm": 24.52498435974121,
      "learning_rate": 1.60410718030361e-06,
      "loss": 1.9732,
      "step": 3724
    },
    {
      "epoch": 2.4522712310730745,
      "grad_norm": 21.899341583251953,
      "learning_rate": 1.6003642089737969e-06,
      "loss": 1.9097,
      "step": 3725
    },
    {
      "epoch": 2.452929558920342,
      "grad_norm": 23.72138023376465,
      "learning_rate": 1.5966252297505536e-06,
      "loss": 1.9114,
      "step": 3726
    },
    {
      "epoch": 2.45358788676761,
      "grad_norm": 1.613571286201477,
      "learning_rate": 1.5928902444109107e-06,
      "loss": 1.519,
      "step": 3727
    },
    {
      "epoch": 2.4542462146148782,
      "grad_norm": 17.08993148803711,
      "learning_rate": 1.5891592547300018e-06,
      "loss": 2.0728,
      "step": 3728
    },
    {
      "epoch": 2.4549045424621463,
      "grad_norm": 1.2702082395553589,
      "learning_rate": 1.5854322624810559e-06,
      "loss": 1.5256,
      "step": 3729
    },
    {
      "epoch": 2.4555628703094143,
      "grad_norm": 0.9445201754570007,
      "learning_rate": 1.5817092694354085e-06,
      "loss": 1.522,
      "step": 3730
    },
    {
      "epoch": 2.456221198156682,
      "grad_norm": 3.1272847652435303,
      "learning_rate": 1.577990277362491e-06,
      "loss": 1.5659,
      "step": 3731
    },
    {
      "epoch": 2.45687952600395,
      "grad_norm": 24.484773635864258,
      "learning_rate": 1.574275288029834e-06,
      "loss": 2.2055,
      "step": 3732
    },
    {
      "epoch": 2.457537853851218,
      "grad_norm": 1.7996238470077515,
      "learning_rate": 1.5705643032030648e-06,
      "loss": 1.5407,
      "step": 3733
    },
    {
      "epoch": 2.4581961816984856,
      "grad_norm": 17.238414764404297,
      "learning_rate": 1.566857324645913e-06,
      "loss": 1.9734,
      "step": 3734
    },
    {
      "epoch": 2.4588545095457537,
      "grad_norm": 21.91287612915039,
      "learning_rate": 1.5631543541201922e-06,
      "loss": 2.1394,
      "step": 3735
    },
    {
      "epoch": 2.4595128373930217,
      "grad_norm": 8.020848274230957,
      "learning_rate": 1.5594553933858226e-06,
      "loss": 1.6969,
      "step": 3736
    },
    {
      "epoch": 2.46017116524029,
      "grad_norm": 1.1863161325454712,
      "learning_rate": 1.5557604442008123e-06,
      "loss": 1.5162,
      "step": 3737
    },
    {
      "epoch": 2.460829493087558,
      "grad_norm": 15.246197700500488,
      "learning_rate": 1.5520695083212678e-06,
      "loss": 2.05,
      "step": 3738
    },
    {
      "epoch": 2.4614878209348254,
      "grad_norm": 4.2694783210754395,
      "learning_rate": 1.5483825875013824e-06,
      "loss": 1.674,
      "step": 3739
    },
    {
      "epoch": 2.4621461487820935,
      "grad_norm": 12.162484169006348,
      "learning_rate": 1.544699683493449e-06,
      "loss": 1.7881,
      "step": 3740
    },
    {
      "epoch": 2.4628044766293615,
      "grad_norm": 5.40051794052124,
      "learning_rate": 1.5410207980478398e-06,
      "loss": 1.611,
      "step": 3741
    },
    {
      "epoch": 2.463462804476629,
      "grad_norm": 19.6754207611084,
      "learning_rate": 1.5373459329130268e-06,
      "loss": 1.728,
      "step": 3742
    },
    {
      "epoch": 2.464121132323897,
      "grad_norm": 45.07944869995117,
      "learning_rate": 1.533675089835569e-06,
      "loss": 3.3077,
      "step": 3743
    },
    {
      "epoch": 2.4647794601711652,
      "grad_norm": 1.6119791269302368,
      "learning_rate": 1.5300082705601117e-06,
      "loss": 1.569,
      "step": 3744
    },
    {
      "epoch": 2.4654377880184333,
      "grad_norm": 21.844778060913086,
      "learning_rate": 1.5263454768293906e-06,
      "loss": 1.801,
      "step": 3745
    },
    {
      "epoch": 2.4660961158657013,
      "grad_norm": 42.7469482421875,
      "learning_rate": 1.522686710384228e-06,
      "loss": 2.2516,
      "step": 3746
    },
    {
      "epoch": 2.466754443712969,
      "grad_norm": 16.460311889648438,
      "learning_rate": 1.5190319729635262e-06,
      "loss": 2.0144,
      "step": 3747
    },
    {
      "epoch": 2.467412771560237,
      "grad_norm": 14.054601669311523,
      "learning_rate": 1.515381266304281e-06,
      "loss": 1.9371,
      "step": 3748
    },
    {
      "epoch": 2.468071099407505,
      "grad_norm": 46.40637969970703,
      "learning_rate": 1.5117345921415705e-06,
      "loss": 2.6602,
      "step": 3749
    },
    {
      "epoch": 2.468729427254773,
      "grad_norm": 1.2510114908218384,
      "learning_rate": 1.508091952208549e-06,
      "loss": 1.5231,
      "step": 3750
    },
    {
      "epoch": 2.4693877551020407,
      "grad_norm": 20.990737915039062,
      "learning_rate": 1.504453348236461e-06,
      "loss": 2.1355,
      "step": 3751
    },
    {
      "epoch": 2.4700460829493087,
      "grad_norm": 26.10211753845215,
      "learning_rate": 1.5008187819546349e-06,
      "loss": 2.2427,
      "step": 3752
    },
    {
      "epoch": 2.4707044107965768,
      "grad_norm": 2.3177649974823,
      "learning_rate": 1.4971882550904725e-06,
      "loss": 1.5364,
      "step": 3753
    },
    {
      "epoch": 2.471362738643845,
      "grad_norm": 7.847685813903809,
      "learning_rate": 1.4935617693694594e-06,
      "loss": 1.8321,
      "step": 3754
    },
    {
      "epoch": 2.4720210664911124,
      "grad_norm": 10.727387428283691,
      "learning_rate": 1.489939326515163e-06,
      "loss": 1.9176,
      "step": 3755
    },
    {
      "epoch": 2.4726793943383805,
      "grad_norm": 0.8683363199234009,
      "learning_rate": 1.486320928249223e-06,
      "loss": 1.5185,
      "step": 3756
    },
    {
      "epoch": 2.4733377221856485,
      "grad_norm": 1.5472561120986938,
      "learning_rate": 1.4827065762913617e-06,
      "loss": 1.5146,
      "step": 3757
    },
    {
      "epoch": 2.4739960500329166,
      "grad_norm": 0.9820578098297119,
      "learning_rate": 1.4790962723593793e-06,
      "loss": 1.5157,
      "step": 3758
    },
    {
      "epoch": 2.474654377880184,
      "grad_norm": 6.18040657043457,
      "learning_rate": 1.4754900181691467e-06,
      "loss": 1.5889,
      "step": 3759
    },
    {
      "epoch": 2.475312705727452,
      "grad_norm": 12.766989707946777,
      "learning_rate": 1.4718878154346173e-06,
      "loss": 1.8036,
      "step": 3760
    },
    {
      "epoch": 2.4759710335747203,
      "grad_norm": 58.7056770324707,
      "learning_rate": 1.4682896658678114e-06,
      "loss": 2.9145,
      "step": 3761
    },
    {
      "epoch": 2.4766293614219883,
      "grad_norm": 19.721837997436523,
      "learning_rate": 1.4646955711788303e-06,
      "loss": 2.2097,
      "step": 3762
    },
    {
      "epoch": 2.477287689269256,
      "grad_norm": 1.8548939228057861,
      "learning_rate": 1.4611055330758405e-06,
      "loss": 1.5289,
      "step": 3763
    },
    {
      "epoch": 2.477946017116524,
      "grad_norm": 1.4412604570388794,
      "learning_rate": 1.4575195532650865e-06,
      "loss": 1.523,
      "step": 3764
    },
    {
      "epoch": 2.478604344963792,
      "grad_norm": 1.32888662815094,
      "learning_rate": 1.4539376334508803e-06,
      "loss": 1.5177,
      "step": 3765
    },
    {
      "epoch": 2.47926267281106,
      "grad_norm": 1.4427062273025513,
      "learning_rate": 1.450359775335608e-06,
      "loss": 1.5351,
      "step": 3766
    },
    {
      "epoch": 2.4799210006583277,
      "grad_norm": 21.950992584228516,
      "learning_rate": 1.4467859806197215e-06,
      "loss": 2.3612,
      "step": 3767
    },
    {
      "epoch": 2.4805793285055957,
      "grad_norm": 2.5725317001342773,
      "learning_rate": 1.4432162510017468e-06,
      "loss": 1.5746,
      "step": 3768
    },
    {
      "epoch": 2.4812376563528638,
      "grad_norm": 1.3687688112258911,
      "learning_rate": 1.4396505881782685e-06,
      "loss": 1.5127,
      "step": 3769
    },
    {
      "epoch": 2.481895984200132,
      "grad_norm": 16.061933517456055,
      "learning_rate": 1.436088993843947e-06,
      "loss": 1.9499,
      "step": 3770
    },
    {
      "epoch": 2.4825543120474,
      "grad_norm": 13.399356842041016,
      "learning_rate": 1.432531469691506e-06,
      "loss": 1.9901,
      "step": 3771
    },
    {
      "epoch": 2.4832126398946675,
      "grad_norm": 3.5563132762908936,
      "learning_rate": 1.428978017411735e-06,
      "loss": 1.5995,
      "step": 3772
    },
    {
      "epoch": 2.4838709677419355,
      "grad_norm": 12.054232597351074,
      "learning_rate": 1.425428638693489e-06,
      "loss": 1.7243,
      "step": 3773
    },
    {
      "epoch": 2.4845292955892035,
      "grad_norm": 37.44422149658203,
      "learning_rate": 1.4218833352236882e-06,
      "loss": 3.5614,
      "step": 3774
    },
    {
      "epoch": 2.485187623436471,
      "grad_norm": 6.009078502655029,
      "learning_rate": 1.4183421086873085e-06,
      "loss": 1.5641,
      "step": 3775
    },
    {
      "epoch": 2.485845951283739,
      "grad_norm": 10.33989143371582,
      "learning_rate": 1.4148049607673953e-06,
      "loss": 1.8785,
      "step": 3776
    },
    {
      "epoch": 2.4865042791310072,
      "grad_norm": 13.889130592346191,
      "learning_rate": 1.4112718931450554e-06,
      "loss": 1.62,
      "step": 3777
    },
    {
      "epoch": 2.4871626069782753,
      "grad_norm": 9.54580020904541,
      "learning_rate": 1.4077429074994542e-06,
      "loss": 1.8747,
      "step": 3778
    },
    {
      "epoch": 2.4878209348255433,
      "grad_norm": 10.781983375549316,
      "learning_rate": 1.404218005507817e-06,
      "loss": 1.8287,
      "step": 3779
    },
    {
      "epoch": 2.488479262672811,
      "grad_norm": 1.1216663122177124,
      "learning_rate": 1.4006971888454324e-06,
      "loss": 1.5098,
      "step": 3780
    },
    {
      "epoch": 2.489137590520079,
      "grad_norm": 4.910404682159424,
      "learning_rate": 1.3971804591856386e-06,
      "loss": 1.5633,
      "step": 3781
    },
    {
      "epoch": 2.489795918367347,
      "grad_norm": 9.803034782409668,
      "learning_rate": 1.3936678181998376e-06,
      "loss": 1.8029,
      "step": 3782
    },
    {
      "epoch": 2.4904542462146146,
      "grad_norm": 11.815642356872559,
      "learning_rate": 1.3901592675574915e-06,
      "loss": 1.9183,
      "step": 3783
    },
    {
      "epoch": 2.4911125740618827,
      "grad_norm": 15.895123481750488,
      "learning_rate": 1.3866548089261066e-06,
      "loss": 1.9105,
      "step": 3784
    },
    {
      "epoch": 2.4917709019091507,
      "grad_norm": 1.7475637197494507,
      "learning_rate": 1.3831544439712575e-06,
      "loss": 1.5605,
      "step": 3785
    },
    {
      "epoch": 2.492429229756419,
      "grad_norm": 13.310667991638184,
      "learning_rate": 1.3796581743565695e-06,
      "loss": 1.8577,
      "step": 3786
    },
    {
      "epoch": 2.493087557603687,
      "grad_norm": 1.3518686294555664,
      "learning_rate": 1.3761660017437128e-06,
      "loss": 1.5295,
      "step": 3787
    },
    {
      "epoch": 2.4937458854509544,
      "grad_norm": 14.779000282287598,
      "learning_rate": 1.3726779277924208e-06,
      "loss": 2.1408,
      "step": 3788
    },
    {
      "epoch": 2.4944042132982225,
      "grad_norm": 33.87291717529297,
      "learning_rate": 1.369193954160477e-06,
      "loss": 2.8209,
      "step": 3789
    },
    {
      "epoch": 2.4950625411454905,
      "grad_norm": 20.557907104492188,
      "learning_rate": 1.3657140825037107e-06,
      "loss": 2.2109,
      "step": 3790
    },
    {
      "epoch": 2.4957208689927586,
      "grad_norm": 23.131502151489258,
      "learning_rate": 1.3622383144760066e-06,
      "loss": 1.9817,
      "step": 3791
    },
    {
      "epoch": 2.496379196840026,
      "grad_norm": 16.66394805908203,
      "learning_rate": 1.3587666517292986e-06,
      "loss": 2.0444,
      "step": 3792
    },
    {
      "epoch": 2.4970375246872942,
      "grad_norm": 10.67495059967041,
      "learning_rate": 1.3552990959135682e-06,
      "loss": 1.7829,
      "step": 3793
    },
    {
      "epoch": 2.4976958525345623,
      "grad_norm": 1.5227586030960083,
      "learning_rate": 1.3518356486768446e-06,
      "loss": 1.5247,
      "step": 3794
    },
    {
      "epoch": 2.4983541803818303,
      "grad_norm": 0.9111734628677368,
      "learning_rate": 1.3483763116652093e-06,
      "loss": 1.5194,
      "step": 3795
    },
    {
      "epoch": 2.499012508229098,
      "grad_norm": 14.053993225097656,
      "learning_rate": 1.3449210865227801e-06,
      "loss": 1.8856,
      "step": 3796
    },
    {
      "epoch": 2.499670836076366,
      "grad_norm": 27.3727970123291,
      "learning_rate": 1.3414699748917293e-06,
      "loss": 2.2607,
      "step": 3797
    },
    {
      "epoch": 2.500329163923634,
      "grad_norm": 10.3012113571167,
      "learning_rate": 1.3380229784122722e-06,
      "loss": 1.6497,
      "step": 3798
    },
    {
      "epoch": 2.500987491770902,
      "grad_norm": 1.0122885704040527,
      "learning_rate": 1.3345800987226664e-06,
      "loss": 1.5217,
      "step": 3799
    },
    {
      "epoch": 2.5016458196181697,
      "grad_norm": 18.07947540283203,
      "learning_rate": 1.3311413374592163e-06,
      "loss": 1.9352,
      "step": 3800
    },
    {
      "epoch": 2.5023041474654377,
      "grad_norm": 0.8888469338417053,
      "learning_rate": 1.3277066962562647e-06,
      "loss": 1.5092,
      "step": 3801
    },
    {
      "epoch": 2.5029624753127058,
      "grad_norm": 22.555644989013672,
      "learning_rate": 1.3242761767462008e-06,
      "loss": 2.184,
      "step": 3802
    },
    {
      "epoch": 2.503620803159974,
      "grad_norm": 9.795912742614746,
      "learning_rate": 1.3208497805594478e-06,
      "loss": 1.7189,
      "step": 3803
    },
    {
      "epoch": 2.504279131007242,
      "grad_norm": 3.2130236625671387,
      "learning_rate": 1.3174275093244782e-06,
      "loss": 1.5737,
      "step": 3804
    },
    {
      "epoch": 2.5049374588545095,
      "grad_norm": 19.838918685913086,
      "learning_rate": 1.3140093646677965e-06,
      "loss": 2.3446,
      "step": 3805
    },
    {
      "epoch": 2.5055957867017775,
      "grad_norm": 22.70552635192871,
      "learning_rate": 1.31059534821395e-06,
      "loss": 2.2217,
      "step": 3806
    },
    {
      "epoch": 2.5062541145490456,
      "grad_norm": 13.19430923461914,
      "learning_rate": 1.3071854615855251e-06,
      "loss": 1.8987,
      "step": 3807
    },
    {
      "epoch": 2.506912442396313,
      "grad_norm": 10.17599868774414,
      "learning_rate": 1.3037797064031421e-06,
      "loss": 1.8147,
      "step": 3808
    },
    {
      "epoch": 2.507570770243581,
      "grad_norm": 0.7593955397605896,
      "learning_rate": 1.3003780842854575e-06,
      "loss": 1.5162,
      "step": 3809
    },
    {
      "epoch": 2.5082290980908493,
      "grad_norm": 8.917071342468262,
      "learning_rate": 1.2969805968491655e-06,
      "loss": 1.7283,
      "step": 3810
    },
    {
      "epoch": 2.5088874259381173,
      "grad_norm": 23.336679458618164,
      "learning_rate": 1.293587245708996e-06,
      "loss": 2.1046,
      "step": 3811
    },
    {
      "epoch": 2.5095457537853854,
      "grad_norm": 31.59115982055664,
      "learning_rate": 1.2901980324777107e-06,
      "loss": 2.092,
      "step": 3812
    },
    {
      "epoch": 2.510204081632653,
      "grad_norm": 9.855372428894043,
      "learning_rate": 1.286812958766106e-06,
      "loss": 1.7791,
      "step": 3813
    },
    {
      "epoch": 2.510862409479921,
      "grad_norm": 38.78239822387695,
      "learning_rate": 1.2834320261830125e-06,
      "loss": 2.3751,
      "step": 3814
    },
    {
      "epoch": 2.511520737327189,
      "grad_norm": 23.953845977783203,
      "learning_rate": 1.2800552363352868e-06,
      "loss": 1.7452,
      "step": 3815
    },
    {
      "epoch": 2.5121790651744567,
      "grad_norm": 1.5073336362838745,
      "learning_rate": 1.2766825908278224e-06,
      "loss": 1.5319,
      "step": 3816
    },
    {
      "epoch": 2.5128373930217247,
      "grad_norm": 1.863751769065857,
      "learning_rate": 1.2733140912635434e-06,
      "loss": 1.5301,
      "step": 3817
    },
    {
      "epoch": 2.5134957208689928,
      "grad_norm": 7.811587810516357,
      "learning_rate": 1.2699497392433945e-06,
      "loss": 1.7353,
      "step": 3818
    },
    {
      "epoch": 2.514154048716261,
      "grad_norm": 0.8547236919403076,
      "learning_rate": 1.2665895363663627e-06,
      "loss": 1.5148,
      "step": 3819
    },
    {
      "epoch": 2.514812376563529,
      "grad_norm": 15.846118927001953,
      "learning_rate": 1.263233484229457e-06,
      "loss": 2.0489,
      "step": 3820
    },
    {
      "epoch": 2.5154707044107965,
      "grad_norm": 8.396900177001953,
      "learning_rate": 1.259881584427708e-06,
      "loss": 1.6593,
      "step": 3821
    },
    {
      "epoch": 2.5161290322580645,
      "grad_norm": 12.396795272827148,
      "learning_rate": 1.2565338385541792e-06,
      "loss": 1.7297,
      "step": 3822
    },
    {
      "epoch": 2.5167873601053325,
      "grad_norm": 21.129169464111328,
      "learning_rate": 1.2531902481999613e-06,
      "loss": 1.875,
      "step": 3823
    },
    {
      "epoch": 2.5174456879526,
      "grad_norm": 29.171018600463867,
      "learning_rate": 1.2498508149541633e-06,
      "loss": 2.9336,
      "step": 3824
    },
    {
      "epoch": 2.518104015799868,
      "grad_norm": 0.5682942271232605,
      "learning_rate": 1.246515540403922e-06,
      "loss": 1.5179,
      "step": 3825
    },
    {
      "epoch": 2.5187623436471362,
      "grad_norm": 25.25163459777832,
      "learning_rate": 1.2431844261344039e-06,
      "loss": 2.5077,
      "step": 3826
    },
    {
      "epoch": 2.5194206714944043,
      "grad_norm": 34.12397384643555,
      "learning_rate": 1.239857473728786e-06,
      "loss": 2.7355,
      "step": 3827
    },
    {
      "epoch": 2.5200789993416723,
      "grad_norm": 14.090389251708984,
      "learning_rate": 1.2365346847682768e-06,
      "loss": 1.87,
      "step": 3828
    },
    {
      "epoch": 2.52073732718894,
      "grad_norm": 16.571025848388672,
      "learning_rate": 1.233216060832103e-06,
      "loss": 1.9637,
      "step": 3829
    },
    {
      "epoch": 2.521395655036208,
      "grad_norm": 13.260770797729492,
      "learning_rate": 1.2299016034975097e-06,
      "loss": 1.9142,
      "step": 3830
    },
    {
      "epoch": 2.522053982883476,
      "grad_norm": 13.546086311340332,
      "learning_rate": 1.2265913143397634e-06,
      "loss": 1.9048,
      "step": 3831
    },
    {
      "epoch": 2.5227123107307436,
      "grad_norm": 17.00983428955078,
      "learning_rate": 1.2232851949321512e-06,
      "loss": 2.1161,
      "step": 3832
    },
    {
      "epoch": 2.5233706385780117,
      "grad_norm": 0.8429945707321167,
      "learning_rate": 1.219983246845977e-06,
      "loss": 1.5135,
      "step": 3833
    },
    {
      "epoch": 2.5240289664252797,
      "grad_norm": 16.353139877319336,
      "learning_rate": 1.2166854716505616e-06,
      "loss": 1.9725,
      "step": 3834
    },
    {
      "epoch": 2.524687294272548,
      "grad_norm": 1.2162458896636963,
      "learning_rate": 1.2133918709132464e-06,
      "loss": 1.5284,
      "step": 3835
    },
    {
      "epoch": 2.525345622119816,
      "grad_norm": 15.512530326843262,
      "learning_rate": 1.2101024461993804e-06,
      "loss": 2.3316,
      "step": 3836
    },
    {
      "epoch": 2.526003949967084,
      "grad_norm": 22.450237274169922,
      "learning_rate": 1.2068171990723354e-06,
      "loss": 2.2521,
      "step": 3837
    },
    {
      "epoch": 2.5266622778143515,
      "grad_norm": 1.3265411853790283,
      "learning_rate": 1.2035361310934956e-06,
      "loss": 1.5249,
      "step": 3838
    },
    {
      "epoch": 2.5273206056616195,
      "grad_norm": 1.8287545442581177,
      "learning_rate": 1.2002592438222594e-06,
      "loss": 1.5308,
      "step": 3839
    },
    {
      "epoch": 2.5279789335088876,
      "grad_norm": 40.18183898925781,
      "learning_rate": 1.1969865388160362e-06,
      "loss": 3.2802,
      "step": 3840
    },
    {
      "epoch": 2.528637261356155,
      "grad_norm": 26.701200485229492,
      "learning_rate": 1.1937180176302488e-06,
      "loss": 2.4393,
      "step": 3841
    },
    {
      "epoch": 2.5292955892034232,
      "grad_norm": 13.709725379943848,
      "learning_rate": 1.1904536818183353e-06,
      "loss": 1.878,
      "step": 3842
    },
    {
      "epoch": 2.5299539170506913,
      "grad_norm": 25.574676513671875,
      "learning_rate": 1.1871935329317363e-06,
      "loss": 2.2202,
      "step": 3843
    },
    {
      "epoch": 2.5306122448979593,
      "grad_norm": 17.173738479614258,
      "learning_rate": 1.1839375725199098e-06,
      "loss": 2.1055,
      "step": 3844
    },
    {
      "epoch": 2.5312705727452274,
      "grad_norm": 17.91334342956543,
      "learning_rate": 1.1806858021303202e-06,
      "loss": 2.2534,
      "step": 3845
    },
    {
      "epoch": 2.531928900592495,
      "grad_norm": 0.9475672245025635,
      "learning_rate": 1.1774382233084413e-06,
      "loss": 1.5099,
      "step": 3846
    },
    {
      "epoch": 2.532587228439763,
      "grad_norm": 2.759456157684326,
      "learning_rate": 1.174194837597753e-06,
      "loss": 1.5391,
      "step": 3847
    },
    {
      "epoch": 2.533245556287031,
      "grad_norm": 1.2335187196731567,
      "learning_rate": 1.1709556465397475e-06,
      "loss": 1.5282,
      "step": 3848
    },
    {
      "epoch": 2.5339038841342987,
      "grad_norm": 1.0483778715133667,
      "learning_rate": 1.1677206516739148e-06,
      "loss": 1.5152,
      "step": 3849
    },
    {
      "epoch": 2.5345622119815667,
      "grad_norm": 1.2688387632369995,
      "learning_rate": 1.164489854537758e-06,
      "loss": 1.5204,
      "step": 3850
    },
    {
      "epoch": 2.5352205398288348,
      "grad_norm": 10.187312126159668,
      "learning_rate": 1.1612632566667836e-06,
      "loss": 1.8561,
      "step": 3851
    },
    {
      "epoch": 2.535878867676103,
      "grad_norm": 15.73888111114502,
      "learning_rate": 1.1580408595944958e-06,
      "loss": 1.8493,
      "step": 3852
    },
    {
      "epoch": 2.536537195523371,
      "grad_norm": 18.11756706237793,
      "learning_rate": 1.1548226648524141e-06,
      "loss": 2.0642,
      "step": 3853
    },
    {
      "epoch": 2.5371955233706385,
      "grad_norm": 12.276407241821289,
      "learning_rate": 1.1516086739700538e-06,
      "loss": 1.9071,
      "step": 3854
    },
    {
      "epoch": 2.5378538512179065,
      "grad_norm": 17.624866485595703,
      "learning_rate": 1.1483988884749298e-06,
      "loss": 2.0666,
      "step": 3855
    },
    {
      "epoch": 2.5385121790651746,
      "grad_norm": 2.1829593181610107,
      "learning_rate": 1.1451933098925616e-06,
      "loss": 1.5657,
      "step": 3856
    },
    {
      "epoch": 2.539170506912442,
      "grad_norm": 18.806509017944336,
      "learning_rate": 1.1419919397464718e-06,
      "loss": 1.7009,
      "step": 3857
    },
    {
      "epoch": 2.53982883475971,
      "grad_norm": 19.095918655395508,
      "learning_rate": 1.1387947795581755e-06,
      "loss": 1.8647,
      "step": 3858
    },
    {
      "epoch": 2.5404871626069783,
      "grad_norm": 6.28061580657959,
      "learning_rate": 1.1356018308471929e-06,
      "loss": 1.571,
      "step": 3859
    },
    {
      "epoch": 2.5411454904542463,
      "grad_norm": 1.1337485313415527,
      "learning_rate": 1.1324130951310463e-06,
      "loss": 1.518,
      "step": 3860
    },
    {
      "epoch": 2.5418038183015144,
      "grad_norm": 0.5030157566070557,
      "learning_rate": 1.1292285739252451e-06,
      "loss": 1.512,
      "step": 3861
    },
    {
      "epoch": 2.542462146148782,
      "grad_norm": 20.91972541809082,
      "learning_rate": 1.1260482687433016e-06,
      "loss": 2.1393,
      "step": 3862
    },
    {
      "epoch": 2.54312047399605,
      "grad_norm": 22.282690048217773,
      "learning_rate": 1.122872181096728e-06,
      "loss": 1.9453,
      "step": 3863
    },
    {
      "epoch": 2.543778801843318,
      "grad_norm": 1.8976771831512451,
      "learning_rate": 1.1197003124950223e-06,
      "loss": 1.5345,
      "step": 3864
    },
    {
      "epoch": 2.5444371296905857,
      "grad_norm": 15.92629337310791,
      "learning_rate": 1.1165326644456854e-06,
      "loss": 2.0804,
      "step": 3865
    },
    {
      "epoch": 2.5450954575378537,
      "grad_norm": 17.520652770996094,
      "learning_rate": 1.1133692384542094e-06,
      "loss": 1.9328,
      "step": 3866
    },
    {
      "epoch": 2.5457537853851218,
      "grad_norm": 7.7210845947265625,
      "learning_rate": 1.1102100360240809e-06,
      "loss": 1.6429,
      "step": 3867
    },
    {
      "epoch": 2.54641211323239,
      "grad_norm": 11.439319610595703,
      "learning_rate": 1.1070550586567775e-06,
      "loss": 1.8982,
      "step": 3868
    },
    {
      "epoch": 2.547070441079658,
      "grad_norm": 12.760443687438965,
      "learning_rate": 1.1039043078517731e-06,
      "loss": 1.8361,
      "step": 3869
    },
    {
      "epoch": 2.5477287689269255,
      "grad_norm": 10.451362609863281,
      "learning_rate": 1.1007577851065243e-06,
      "loss": 1.8222,
      "step": 3870
    },
    {
      "epoch": 2.5483870967741935,
      "grad_norm": 15.842482566833496,
      "learning_rate": 1.097615491916485e-06,
      "loss": 1.9411,
      "step": 3871
    },
    {
      "epoch": 2.5490454246214616,
      "grad_norm": 11.772855758666992,
      "learning_rate": 1.0944774297750983e-06,
      "loss": 1.7026,
      "step": 3872
    },
    {
      "epoch": 2.549703752468729,
      "grad_norm": 3.3478496074676514,
      "learning_rate": 1.0913436001737953e-06,
      "loss": 1.5542,
      "step": 3873
    },
    {
      "epoch": 2.550362080315997,
      "grad_norm": 13.736449241638184,
      "learning_rate": 1.088214004601994e-06,
      "loss": 1.9871,
      "step": 3874
    },
    {
      "epoch": 2.5510204081632653,
      "grad_norm": 1.9678574800491333,
      "learning_rate": 1.0850886445471055e-06,
      "loss": 1.5296,
      "step": 3875
    },
    {
      "epoch": 2.5516787360105333,
      "grad_norm": 14.130202293395996,
      "learning_rate": 1.081967521494519e-06,
      "loss": 2.0071,
      "step": 3876
    },
    {
      "epoch": 2.5523370638578013,
      "grad_norm": 1.3793541193008423,
      "learning_rate": 1.0788506369276176e-06,
      "loss": 1.5219,
      "step": 3877
    },
    {
      "epoch": 2.5529953917050694,
      "grad_norm": 34.62856674194336,
      "learning_rate": 1.0757379923277667e-06,
      "loss": 2.2803,
      "step": 3878
    },
    {
      "epoch": 2.553653719552337,
      "grad_norm": 7.058732032775879,
      "learning_rate": 1.0726295891743177e-06,
      "loss": 1.6809,
      "step": 3879
    },
    {
      "epoch": 2.554312047399605,
      "grad_norm": 34.51694869995117,
      "learning_rate": 1.0695254289446067e-06,
      "loss": 2.2651,
      "step": 3880
    },
    {
      "epoch": 2.554970375246873,
      "grad_norm": 19.6083984375,
      "learning_rate": 1.066425513113949e-06,
      "loss": 2.7016,
      "step": 3881
    },
    {
      "epoch": 2.5556287030941407,
      "grad_norm": 4.674924850463867,
      "learning_rate": 1.0633298431556516e-06,
      "loss": 1.6669,
      "step": 3882
    },
    {
      "epoch": 2.5562870309414087,
      "grad_norm": 11.655255317687988,
      "learning_rate": 1.0602384205409911e-06,
      "loss": 1.9183,
      "step": 3883
    },
    {
      "epoch": 2.556945358788677,
      "grad_norm": 0.9581102728843689,
      "learning_rate": 1.0571512467392363e-06,
      "loss": 1.5177,
      "step": 3884
    },
    {
      "epoch": 2.557603686635945,
      "grad_norm": 24.531923294067383,
      "learning_rate": 1.0540683232176307e-06,
      "loss": 2.3465,
      "step": 3885
    },
    {
      "epoch": 2.558262014483213,
      "grad_norm": 13.49272346496582,
      "learning_rate": 1.050989651441402e-06,
      "loss": 1.911,
      "step": 3886
    },
    {
      "epoch": 2.5589203423304805,
      "grad_norm": 17.841833114624023,
      "learning_rate": 1.047915232873752e-06,
      "loss": 1.7762,
      "step": 3887
    },
    {
      "epoch": 2.5595786701777485,
      "grad_norm": 42.69930648803711,
      "learning_rate": 1.0448450689758683e-06,
      "loss": 3.9873,
      "step": 3888
    },
    {
      "epoch": 2.5602369980250166,
      "grad_norm": 17.761878967285156,
      "learning_rate": 1.041779161206906e-06,
      "loss": 2.0516,
      "step": 3889
    },
    {
      "epoch": 2.560895325872284,
      "grad_norm": 1.9621316194534302,
      "learning_rate": 1.0387175110240066e-06,
      "loss": 1.5683,
      "step": 3890
    },
    {
      "epoch": 2.5615536537195522,
      "grad_norm": 36.014671325683594,
      "learning_rate": 1.0356601198822868e-06,
      "loss": 2.3904,
      "step": 3891
    },
    {
      "epoch": 2.5622119815668203,
      "grad_norm": 1.0575873851776123,
      "learning_rate": 1.0326069892348322e-06,
      "loss": 1.5256,
      "step": 3892
    },
    {
      "epoch": 2.5628703094140883,
      "grad_norm": 22.32207489013672,
      "learning_rate": 1.0295581205327098e-06,
      "loss": 1.7796,
      "step": 3893
    },
    {
      "epoch": 2.5635286372613564,
      "grad_norm": 11.8863525390625,
      "learning_rate": 1.026513515224965e-06,
      "loss": 1.9349,
      "step": 3894
    },
    {
      "epoch": 2.564186965108624,
      "grad_norm": 17.059803009033203,
      "learning_rate": 1.0234731747586058e-06,
      "loss": 1.9108,
      "step": 3895
    },
    {
      "epoch": 2.564845292955892,
      "grad_norm": 16.345352172851562,
      "learning_rate": 1.0204371005786206e-06,
      "loss": 2.0034,
      "step": 3896
    },
    {
      "epoch": 2.56550362080316,
      "grad_norm": 0.6150788068771362,
      "learning_rate": 1.0174052941279699e-06,
      "loss": 1.5137,
      "step": 3897
    },
    {
      "epoch": 2.5661619486504277,
      "grad_norm": 34.40896224975586,
      "learning_rate": 1.014377756847582e-06,
      "loss": 2.9031,
      "step": 3898
    },
    {
      "epoch": 2.5668202764976957,
      "grad_norm": 16.215694427490234,
      "learning_rate": 1.0113544901763606e-06,
      "loss": 1.9696,
      "step": 3899
    },
    {
      "epoch": 2.5674786043449638,
      "grad_norm": 17.659347534179688,
      "learning_rate": 1.008335495551177e-06,
      "loss": 1.7055,
      "step": 3900
    },
    {
      "epoch": 2.568136932192232,
      "grad_norm": 1.4645382165908813,
      "learning_rate": 1.0053207744068739e-06,
      "loss": 1.5278,
      "step": 3901
    },
    {
      "epoch": 2.5687952600395,
      "grad_norm": 7.053710460662842,
      "learning_rate": 1.0023103281762613e-06,
      "loss": 1.7088,
      "step": 3902
    },
    {
      "epoch": 2.5694535878867675,
      "grad_norm": 33.04802322387695,
      "learning_rate": 9.993041582901208e-07,
      "loss": 2.4358,
      "step": 3903
    },
    {
      "epoch": 2.5701119157340355,
      "grad_norm": 31.37792205810547,
      "learning_rate": 9.963022661771936e-07,
      "loss": 2.08,
      "step": 3904
    },
    {
      "epoch": 2.5707702435813036,
      "grad_norm": 26.080671310424805,
      "learning_rate": 9.933046532641965e-07,
      "loss": 2.0306,
      "step": 3905
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 1.7205278873443604,
      "learning_rate": 9.903113209758098e-07,
      "loss": 1.5281,
      "step": 3906
    },
    {
      "epoch": 2.5720868992758392,
      "grad_norm": 18.325233459472656,
      "learning_rate": 9.873222707346763e-07,
      "loss": 2.2945,
      "step": 3907
    },
    {
      "epoch": 2.5727452271231073,
      "grad_norm": 8.664691925048828,
      "learning_rate": 9.843375039614078e-07,
      "loss": 1.8884,
      "step": 3908
    },
    {
      "epoch": 2.5734035549703753,
      "grad_norm": 2.08870792388916,
      "learning_rate": 9.813570220745793e-07,
      "loss": 1.5358,
      "step": 3909
    },
    {
      "epoch": 2.5740618828176434,
      "grad_norm": 16.12152671813965,
      "learning_rate": 9.783808264907268e-07,
      "loss": 2.3336,
      "step": 3910
    },
    {
      "epoch": 2.574720210664911,
      "grad_norm": 6.970021724700928,
      "learning_rate": 9.75408918624351e-07,
      "loss": 1.7662,
      "step": 3911
    },
    {
      "epoch": 2.575378538512179,
      "grad_norm": 8.81268310546875,
      "learning_rate": 9.724412998879152e-07,
      "loss": 1.7785,
      "step": 3912
    },
    {
      "epoch": 2.576036866359447,
      "grad_norm": 18.66457176208496,
      "learning_rate": 9.694779716918434e-07,
      "loss": 1.9153,
      "step": 3913
    },
    {
      "epoch": 2.5766951942067147,
      "grad_norm": 2.0158751010894775,
      "learning_rate": 9.665189354445203e-07,
      "loss": 1.5479,
      "step": 3914
    },
    {
      "epoch": 2.5773535220539827,
      "grad_norm": 20.327024459838867,
      "learning_rate": 9.635641925522931e-07,
      "loss": 2.1857,
      "step": 3915
    },
    {
      "epoch": 2.5780118499012508,
      "grad_norm": 14.033500671386719,
      "learning_rate": 9.606137444194662e-07,
      "loss": 1.7525,
      "step": 3916
    },
    {
      "epoch": 2.578670177748519,
      "grad_norm": 1.1641124486923218,
      "learning_rate": 9.576675924483014e-07,
      "loss": 1.5283,
      "step": 3917
    },
    {
      "epoch": 2.579328505595787,
      "grad_norm": 2.8878443241119385,
      "learning_rate": 9.547257380390206e-07,
      "loss": 1.5751,
      "step": 3918
    },
    {
      "epoch": 2.579986833443055,
      "grad_norm": 1.8903433084487915,
      "learning_rate": 9.517881825898046e-07,
      "loss": 1.5191,
      "step": 3919
    },
    {
      "epoch": 2.5806451612903225,
      "grad_norm": 1.4516444206237793,
      "learning_rate": 9.488549274967873e-07,
      "loss": 1.5189,
      "step": 3920
    },
    {
      "epoch": 2.5813034891375906,
      "grad_norm": 10.418148040771484,
      "learning_rate": 9.459259741540639e-07,
      "loss": 1.7712,
      "step": 3921
    },
    {
      "epoch": 2.5819618169848586,
      "grad_norm": 8.376724243164062,
      "learning_rate": 9.430013239536806e-07,
      "loss": 1.6737,
      "step": 3922
    },
    {
      "epoch": 2.582620144832126,
      "grad_norm": 7.624828338623047,
      "learning_rate": 9.40080978285639e-07,
      "loss": 1.5737,
      "step": 3923
    },
    {
      "epoch": 2.5832784726793943,
      "grad_norm": 0.6615903377532959,
      "learning_rate": 9.371649385378967e-07,
      "loss": 1.5115,
      "step": 3924
    },
    {
      "epoch": 2.5839368005266623,
      "grad_norm": 9.20581340789795,
      "learning_rate": 9.342532060963649e-07,
      "loss": 1.9642,
      "step": 3925
    },
    {
      "epoch": 2.5845951283739304,
      "grad_norm": 15.157388687133789,
      "learning_rate": 9.313457823449035e-07,
      "loss": 1.9513,
      "step": 3926
    },
    {
      "epoch": 2.5852534562211984,
      "grad_norm": 0.8649243712425232,
      "learning_rate": 9.284426686653303e-07,
      "loss": 1.5176,
      "step": 3927
    },
    {
      "epoch": 2.585911784068466,
      "grad_norm": 11.4138822555542,
      "learning_rate": 9.25543866437415e-07,
      "loss": 1.5753,
      "step": 3928
    },
    {
      "epoch": 2.586570111915734,
      "grad_norm": 1.0202584266662598,
      "learning_rate": 9.226493770388701e-07,
      "loss": 1.5132,
      "step": 3929
    },
    {
      "epoch": 2.587228439763002,
      "grad_norm": 0.7181811928749084,
      "learning_rate": 9.197592018453649e-07,
      "loss": 1.5133,
      "step": 3930
    },
    {
      "epoch": 2.5878867676102697,
      "grad_norm": 15.304092407226562,
      "learning_rate": 9.168733422305209e-07,
      "loss": 1.9361,
      "step": 3931
    },
    {
      "epoch": 2.5885450954575377,
      "grad_norm": 35.238590240478516,
      "learning_rate": 9.139917995658998e-07,
      "loss": 2.2722,
      "step": 3932
    },
    {
      "epoch": 2.589203423304806,
      "grad_norm": 6.279012680053711,
      "learning_rate": 9.111145752210171e-07,
      "loss": 1.598,
      "step": 3933
    },
    {
      "epoch": 2.589861751152074,
      "grad_norm": 14.996182441711426,
      "learning_rate": 9.082416705633379e-07,
      "loss": 2.308,
      "step": 3934
    },
    {
      "epoch": 2.590520078999342,
      "grad_norm": 7.166698932647705,
      "learning_rate": 9.053730869582688e-07,
      "loss": 1.7547,
      "step": 3935
    },
    {
      "epoch": 2.5911784068466095,
      "grad_norm": 0.8283844590187073,
      "learning_rate": 9.025088257691672e-07,
      "loss": 1.5216,
      "step": 3936
    },
    {
      "epoch": 2.5918367346938775,
      "grad_norm": 3.0173938274383545,
      "learning_rate": 8.996488883573351e-07,
      "loss": 1.5638,
      "step": 3937
    },
    {
      "epoch": 2.5924950625411456,
      "grad_norm": 5.45245885848999,
      "learning_rate": 8.967932760820164e-07,
      "loss": 1.7035,
      "step": 3938
    },
    {
      "epoch": 2.593153390388413,
      "grad_norm": 0.9564692974090576,
      "learning_rate": 8.939419903004032e-07,
      "loss": 1.5108,
      "step": 3939
    },
    {
      "epoch": 2.5938117182356812,
      "grad_norm": 6.063210964202881,
      "learning_rate": 8.910950323676304e-07,
      "loss": 1.7139,
      "step": 3940
    },
    {
      "epoch": 2.5944700460829493,
      "grad_norm": 40.07081985473633,
      "learning_rate": 8.882524036367757e-07,
      "loss": 3.9559,
      "step": 3941
    },
    {
      "epoch": 2.5951283739302173,
      "grad_norm": 9.366470336914062,
      "learning_rate": 8.854141054588583e-07,
      "loss": 1.7824,
      "step": 3942
    },
    {
      "epoch": 2.5957867017774854,
      "grad_norm": 20.476205825805664,
      "learning_rate": 8.825801391828437e-07,
      "loss": 2.0315,
      "step": 3943
    },
    {
      "epoch": 2.596445029624753,
      "grad_norm": 9.807123184204102,
      "learning_rate": 8.797505061556288e-07,
      "loss": 1.6455,
      "step": 3944
    },
    {
      "epoch": 2.597103357472021,
      "grad_norm": 20.064638137817383,
      "learning_rate": 8.769252077220614e-07,
      "loss": 1.901,
      "step": 3945
    },
    {
      "epoch": 2.597761685319289,
      "grad_norm": 2.3458786010742188,
      "learning_rate": 8.741042452249237e-07,
      "loss": 1.5408,
      "step": 3946
    },
    {
      "epoch": 2.5984200131665567,
      "grad_norm": 19.858722686767578,
      "learning_rate": 8.712876200049392e-07,
      "loss": 1.9328,
      "step": 3947
    },
    {
      "epoch": 2.5990783410138247,
      "grad_norm": 39.9640998840332,
      "learning_rate": 8.68475333400769e-07,
      "loss": 3.4306,
      "step": 3948
    },
    {
      "epoch": 2.599736668861093,
      "grad_norm": 0.6913717985153198,
      "learning_rate": 8.65667386749014e-07,
      "loss": 1.5081,
      "step": 3949
    },
    {
      "epoch": 2.600394996708361,
      "grad_norm": 12.936773300170898,
      "learning_rate": 8.628637813842067e-07,
      "loss": 2.0883,
      "step": 3950
    },
    {
      "epoch": 2.601053324555629,
      "grad_norm": 11.49652099609375,
      "learning_rate": 8.600645186388234e-07,
      "loss": 1.8422,
      "step": 3951
    },
    {
      "epoch": 2.6017116524028965,
      "grad_norm": 9.81057357788086,
      "learning_rate": 8.572695998432756e-07,
      "loss": 1.8582,
      "step": 3952
    },
    {
      "epoch": 2.6023699802501645,
      "grad_norm": 9.345162391662598,
      "learning_rate": 8.544790263259029e-07,
      "loss": 1.7661,
      "step": 3953
    },
    {
      "epoch": 2.6030283080974326,
      "grad_norm": 6.195741176605225,
      "learning_rate": 8.516927994129887e-07,
      "loss": 1.6143,
      "step": 3954
    },
    {
      "epoch": 2.6036866359447006,
      "grad_norm": 23.42362403869629,
      "learning_rate": 8.489109204287472e-07,
      "loss": 2.2831,
      "step": 3955
    },
    {
      "epoch": 2.6043449637919682,
      "grad_norm": 22.83004379272461,
      "learning_rate": 8.461333906953262e-07,
      "loss": 2.1992,
      "step": 3956
    },
    {
      "epoch": 2.6050032916392363,
      "grad_norm": 66.35678100585938,
      "learning_rate": 8.433602115328032e-07,
      "loss": 2.6303,
      "step": 3957
    },
    {
      "epoch": 2.6056616194865043,
      "grad_norm": 0.6342076063156128,
      "learning_rate": 8.405913842591929e-07,
      "loss": 1.5111,
      "step": 3958
    },
    {
      "epoch": 2.6063199473337724,
      "grad_norm": 12.809159278869629,
      "learning_rate": 8.378269101904412e-07,
      "loss": 2.0191,
      "step": 3959
    },
    {
      "epoch": 2.6069782751810404,
      "grad_norm": 21.520044326782227,
      "learning_rate": 8.350667906404186e-07,
      "loss": 2.1246,
      "step": 3960
    },
    {
      "epoch": 2.607636603028308,
      "grad_norm": 11.592634201049805,
      "learning_rate": 8.323110269209355e-07,
      "loss": 1.8169,
      "step": 3961
    },
    {
      "epoch": 2.608294930875576,
      "grad_norm": 5.765501976013184,
      "learning_rate": 8.295596203417277e-07,
      "loss": 1.6979,
      "step": 3962
    },
    {
      "epoch": 2.608953258722844,
      "grad_norm": 11.209542274475098,
      "learning_rate": 8.268125722104569e-07,
      "loss": 1.8589,
      "step": 3963
    },
    {
      "epoch": 2.6096115865701117,
      "grad_norm": 4.9496612548828125,
      "learning_rate": 8.240698838327177e-07,
      "loss": 1.6851,
      "step": 3964
    },
    {
      "epoch": 2.6102699144173798,
      "grad_norm": 25.714750289916992,
      "learning_rate": 8.213315565120338e-07,
      "loss": 1.7259,
      "step": 3965
    },
    {
      "epoch": 2.610928242264648,
      "grad_norm": 18.890544891357422,
      "learning_rate": 8.185975915498501e-07,
      "loss": 1.8277,
      "step": 3966
    },
    {
      "epoch": 2.611586570111916,
      "grad_norm": 0.7208752632141113,
      "learning_rate": 8.158679902455424e-07,
      "loss": 1.5117,
      "step": 3967
    },
    {
      "epoch": 2.612244897959184,
      "grad_norm": 1.0100700855255127,
      "learning_rate": 8.131427538964165e-07,
      "loss": 1.5153,
      "step": 3968
    },
    {
      "epoch": 2.6129032258064515,
      "grad_norm": 14.156956672668457,
      "learning_rate": 8.10421883797694e-07,
      "loss": 2.1575,
      "step": 3969
    },
    {
      "epoch": 2.6135615536537196,
      "grad_norm": 20.959209442138672,
      "learning_rate": 8.077053812425295e-07,
      "loss": 1.9092,
      "step": 3970
    },
    {
      "epoch": 2.6142198815009876,
      "grad_norm": 11.116841316223145,
      "learning_rate": 8.049932475219991e-07,
      "loss": 1.6477,
      "step": 3971
    },
    {
      "epoch": 2.614878209348255,
      "grad_norm": 29.6297664642334,
      "learning_rate": 8.022854839250993e-07,
      "loss": 2.1248,
      "step": 3972
    },
    {
      "epoch": 2.6155365371955233,
      "grad_norm": 0.8148247599601746,
      "learning_rate": 7.995820917387565e-07,
      "loss": 1.5142,
      "step": 3973
    },
    {
      "epoch": 2.6161948650427913,
      "grad_norm": 12.653575897216797,
      "learning_rate": 7.968830722478127e-07,
      "loss": 1.6979,
      "step": 3974
    },
    {
      "epoch": 2.6168531928900594,
      "grad_norm": 0.7100340127944946,
      "learning_rate": 7.941884267350353e-07,
      "loss": 1.5168,
      "step": 3975
    },
    {
      "epoch": 2.6175115207373274,
      "grad_norm": 25.931310653686523,
      "learning_rate": 7.914981564811141e-07,
      "loss": 2.1821,
      "step": 3976
    },
    {
      "epoch": 2.618169848584595,
      "grad_norm": 4.317093372344971,
      "learning_rate": 7.888122627646577e-07,
      "loss": 1.5627,
      "step": 3977
    },
    {
      "epoch": 2.618828176431863,
      "grad_norm": 7.717916965484619,
      "learning_rate": 7.861307468621915e-07,
      "loss": 1.7831,
      "step": 3978
    },
    {
      "epoch": 2.619486504279131,
      "grad_norm": 17.73118782043457,
      "learning_rate": 7.834536100481649e-07,
      "loss": 1.905,
      "step": 3979
    },
    {
      "epoch": 2.6201448321263987,
      "grad_norm": 4.937529563903809,
      "learning_rate": 7.807808535949435e-07,
      "loss": 1.6814,
      "step": 3980
    },
    {
      "epoch": 2.6208031599736668,
      "grad_norm": 9.659930229187012,
      "learning_rate": 7.781124787728133e-07,
      "loss": 1.7826,
      "step": 3981
    },
    {
      "epoch": 2.621461487820935,
      "grad_norm": 31.79007911682129,
      "learning_rate": 7.754484868499767e-07,
      "loss": 2.7982,
      "step": 3982
    },
    {
      "epoch": 2.622119815668203,
      "grad_norm": 0.9760028719902039,
      "learning_rate": 7.727888790925542e-07,
      "loss": 1.5171,
      "step": 3983
    },
    {
      "epoch": 2.622778143515471,
      "grad_norm": 9.561308860778809,
      "learning_rate": 7.701336567645768e-07,
      "loss": 1.8858,
      "step": 3984
    },
    {
      "epoch": 2.6234364713627385,
      "grad_norm": 10.321864128112793,
      "learning_rate": 7.674828211279994e-07,
      "loss": 1.6878,
      "step": 3985
    },
    {
      "epoch": 2.6240947992100065,
      "grad_norm": 26.76616859436035,
      "learning_rate": 7.648363734426867e-07,
      "loss": 2.2679,
      "step": 3986
    },
    {
      "epoch": 2.6247531270572746,
      "grad_norm": 24.13945198059082,
      "learning_rate": 7.621943149664191e-07,
      "loss": 1.834,
      "step": 3987
    },
    {
      "epoch": 2.625411454904542,
      "grad_norm": 17.727291107177734,
      "learning_rate": 7.595566469548931e-07,
      "loss": 2.1566,
      "step": 3988
    },
    {
      "epoch": 2.6260697827518102,
      "grad_norm": 13.669951438903809,
      "learning_rate": 7.569233706617185e-07,
      "loss": 1.9438,
      "step": 3989
    },
    {
      "epoch": 2.6267281105990783,
      "grad_norm": 29.161378860473633,
      "learning_rate": 7.542944873384106e-07,
      "loss": 2.1663,
      "step": 3990
    },
    {
      "epoch": 2.6273864384463463,
      "grad_norm": 1.1299668550491333,
      "learning_rate": 7.516699982344067e-07,
      "loss": 1.5196,
      "step": 3991
    },
    {
      "epoch": 2.6280447662936144,
      "grad_norm": 1.035068154335022,
      "learning_rate": 7.490499045970512e-07,
      "loss": 1.5231,
      "step": 3992
    },
    {
      "epoch": 2.6287030941408824,
      "grad_norm": 23.91621971130371,
      "learning_rate": 7.46434207671598e-07,
      "loss": 1.9324,
      "step": 3993
    },
    {
      "epoch": 2.62936142198815,
      "grad_norm": 13.84755802154541,
      "learning_rate": 7.438229087012116e-07,
      "loss": 1.9627,
      "step": 3994
    },
    {
      "epoch": 2.630019749835418,
      "grad_norm": 0.6585103869438171,
      "learning_rate": 7.412160089269715e-07,
      "loss": 1.5114,
      "step": 3995
    },
    {
      "epoch": 2.630678077682686,
      "grad_norm": 17.340831756591797,
      "learning_rate": 7.386135095878621e-07,
      "loss": 1.7215,
      "step": 3996
    },
    {
      "epoch": 2.6313364055299537,
      "grad_norm": 0.8206447958946228,
      "learning_rate": 7.360154119207752e-07,
      "loss": 1.5152,
      "step": 3997
    },
    {
      "epoch": 2.631994733377222,
      "grad_norm": 4.468569755554199,
      "learning_rate": 7.334217171605118e-07,
      "loss": 1.5885,
      "step": 3998
    },
    {
      "epoch": 2.63265306122449,
      "grad_norm": 0.6630908846855164,
      "learning_rate": 7.308324265397837e-07,
      "loss": 1.5145,
      "step": 3999
    },
    {
      "epoch": 2.633311389071758,
      "grad_norm": 3.9626665115356445,
      "learning_rate": 7.282475412892031e-07,
      "loss": 1.5695,
      "step": 4000
    },
    {
      "epoch": 2.633969716919026,
      "grad_norm": 1.1788146495819092,
      "learning_rate": 7.25667062637292e-07,
      "loss": 1.5153,
      "step": 4001
    },
    {
      "epoch": 2.6346280447662935,
      "grad_norm": 31.39427947998047,
      "learning_rate": 7.230909918104834e-07,
      "loss": 2.3149,
      "step": 4002
    },
    {
      "epoch": 2.6352863726135616,
      "grad_norm": 13.054898262023926,
      "learning_rate": 7.205193300331037e-07,
      "loss": 1.9479,
      "step": 4003
    },
    {
      "epoch": 2.6359447004608296,
      "grad_norm": 15.430573463439941,
      "learning_rate": 7.179520785273941e-07,
      "loss": 2.1473,
      "step": 4004
    },
    {
      "epoch": 2.6366030283080972,
      "grad_norm": 9.193427085876465,
      "learning_rate": 7.153892385134953e-07,
      "loss": 1.8216,
      "step": 4005
    },
    {
      "epoch": 2.6372613561553653,
      "grad_norm": 5.69447660446167,
      "learning_rate": 7.128308112094495e-07,
      "loss": 1.5532,
      "step": 4006
    },
    {
      "epoch": 2.6379196840026333,
      "grad_norm": 12.978089332580566,
      "learning_rate": 7.102767978312064e-07,
      "loss": 1.9089,
      "step": 4007
    },
    {
      "epoch": 2.6385780118499014,
      "grad_norm": 12.868903160095215,
      "learning_rate": 7.077271995926138e-07,
      "loss": 1.9584,
      "step": 4008
    },
    {
      "epoch": 2.6392363396971694,
      "grad_norm": 0.9956955909729004,
      "learning_rate": 7.051820177054236e-07,
      "loss": 1.5123,
      "step": 4009
    },
    {
      "epoch": 2.639894667544437,
      "grad_norm": 23.279268264770508,
      "learning_rate": 7.026412533792881e-07,
      "loss": 2.2386,
      "step": 4010
    },
    {
      "epoch": 2.640552995391705,
      "grad_norm": 2.344665050506592,
      "learning_rate": 7.001049078217614e-07,
      "loss": 1.5553,
      "step": 4011
    },
    {
      "epoch": 2.641211323238973,
      "grad_norm": 1.2470966577529907,
      "learning_rate": 6.975729822382926e-07,
      "loss": 1.5265,
      "step": 4012
    },
    {
      "epoch": 2.6418696510862407,
      "grad_norm": 9.660523414611816,
      "learning_rate": 6.950454778322357e-07,
      "loss": 1.7881,
      "step": 4013
    },
    {
      "epoch": 2.6425279789335088,
      "grad_norm": 1.4526827335357666,
      "learning_rate": 6.92522395804841e-07,
      "loss": 1.5283,
      "step": 4014
    },
    {
      "epoch": 2.643186306780777,
      "grad_norm": 1.7053759098052979,
      "learning_rate": 6.90003737355256e-07,
      "loss": 1.532,
      "step": 4015
    },
    {
      "epoch": 2.643844634628045,
      "grad_norm": 9.847821235656738,
      "learning_rate": 6.874895036805284e-07,
      "loss": 1.9282,
      "step": 4016
    },
    {
      "epoch": 2.644502962475313,
      "grad_norm": 10.978320121765137,
      "learning_rate": 6.849796959756027e-07,
      "loss": 1.6842,
      "step": 4017
    },
    {
      "epoch": 2.6451612903225805,
      "grad_norm": 13.31529712677002,
      "learning_rate": 6.824743154333157e-07,
      "loss": 2.0499,
      "step": 4018
    },
    {
      "epoch": 2.6458196181698486,
      "grad_norm": 8.86645793914795,
      "learning_rate": 6.79973363244405e-07,
      "loss": 1.6055,
      "step": 4019
    },
    {
      "epoch": 2.6464779460171166,
      "grad_norm": 1.851271629333496,
      "learning_rate": 6.774768405975008e-07,
      "loss": 1.5213,
      "step": 4020
    },
    {
      "epoch": 2.647136273864384,
      "grad_norm": 1.6023614406585693,
      "learning_rate": 6.749847486791283e-07,
      "loss": 1.5195,
      "step": 4021
    },
    {
      "epoch": 2.6477946017116523,
      "grad_norm": 1.958650827407837,
      "learning_rate": 6.724970886737092e-07,
      "loss": 1.53,
      "step": 4022
    },
    {
      "epoch": 2.6484529295589203,
      "grad_norm": 18.614910125732422,
      "learning_rate": 6.700138617635577e-07,
      "loss": 2.0606,
      "step": 4023
    },
    {
      "epoch": 2.6491112574061884,
      "grad_norm": 15.651564598083496,
      "learning_rate": 6.675350691288773e-07,
      "loss": 1.8601,
      "step": 4024
    },
    {
      "epoch": 2.6497695852534564,
      "grad_norm": 7.399918079376221,
      "learning_rate": 6.650607119477692e-07,
      "loss": 1.7385,
      "step": 4025
    },
    {
      "epoch": 2.650427913100724,
      "grad_norm": 17.254310607910156,
      "learning_rate": 6.62590791396226e-07,
      "loss": 1.9279,
      "step": 4026
    },
    {
      "epoch": 2.651086240947992,
      "grad_norm": 18.58666229248047,
      "learning_rate": 6.601253086481252e-07,
      "loss": 1.808,
      "step": 4027
    },
    {
      "epoch": 2.65174456879526,
      "grad_norm": 22.335050582885742,
      "learning_rate": 6.576642648752451e-07,
      "loss": 2.5923,
      "step": 4028
    },
    {
      "epoch": 2.6524028966425277,
      "grad_norm": 2.098107099533081,
      "learning_rate": 6.552076612472502e-07,
      "loss": 1.5322,
      "step": 4029
    },
    {
      "epoch": 2.6530612244897958,
      "grad_norm": 21.356380462646484,
      "learning_rate": 6.527554989316898e-07,
      "loss": 1.9325,
      "step": 4030
    },
    {
      "epoch": 2.653719552337064,
      "grad_norm": 1.9435153007507324,
      "learning_rate": 6.503077790940093e-07,
      "loss": 1.5227,
      "step": 4031
    },
    {
      "epoch": 2.654377880184332,
      "grad_norm": 10.099411010742188,
      "learning_rate": 6.478645028975372e-07,
      "loss": 1.7893,
      "step": 4032
    },
    {
      "epoch": 2.6550362080316,
      "grad_norm": 0.8821552991867065,
      "learning_rate": 6.454256715034979e-07,
      "loss": 1.5115,
      "step": 4033
    },
    {
      "epoch": 2.655694535878868,
      "grad_norm": 2.2962899208068848,
      "learning_rate": 6.429912860709909e-07,
      "loss": 1.5232,
      "step": 4034
    },
    {
      "epoch": 2.6563528637261355,
      "grad_norm": 10.991244316101074,
      "learning_rate": 6.40561347757015e-07,
      "loss": 1.8172,
      "step": 4035
    },
    {
      "epoch": 2.6570111915734036,
      "grad_norm": 5.417031764984131,
      "learning_rate": 6.38135857716452e-07,
      "loss": 1.6783,
      "step": 4036
    },
    {
      "epoch": 2.6576695194206716,
      "grad_norm": 18.494657516479492,
      "learning_rate": 6.357148171020621e-07,
      "loss": 2.0157,
      "step": 4037
    },
    {
      "epoch": 2.6583278472679392,
      "grad_norm": 30.509096145629883,
      "learning_rate": 6.332982270645005e-07,
      "loss": 2.8993,
      "step": 4038
    },
    {
      "epoch": 2.6589861751152073,
      "grad_norm": 16.306806564331055,
      "learning_rate": 6.308860887523049e-07,
      "loss": 2.1022,
      "step": 4039
    },
    {
      "epoch": 2.6596445029624753,
      "grad_norm": 6.176921367645264,
      "learning_rate": 6.284784033118907e-07,
      "loss": 1.7192,
      "step": 4040
    },
    {
      "epoch": 2.6603028308097434,
      "grad_norm": 22.643190383911133,
      "learning_rate": 6.260751718875646e-07,
      "loss": 2.1707,
      "step": 4041
    },
    {
      "epoch": 2.6609611586570114,
      "grad_norm": 20.906681060791016,
      "learning_rate": 6.236763956215131e-07,
      "loss": 2.018,
      "step": 4042
    },
    {
      "epoch": 2.661619486504279,
      "grad_norm": 20.692968368530273,
      "learning_rate": 6.212820756538073e-07,
      "loss": 1.9788,
      "step": 4043
    },
    {
      "epoch": 2.662277814351547,
      "grad_norm": 18.35509490966797,
      "learning_rate": 6.188922131223974e-07,
      "loss": 2.0771,
      "step": 4044
    },
    {
      "epoch": 2.662936142198815,
      "grad_norm": 12.897122383117676,
      "learning_rate": 6.165068091631199e-07,
      "loss": 1.9638,
      "step": 4045
    },
    {
      "epoch": 2.6635944700460827,
      "grad_norm": 0.7951638698577881,
      "learning_rate": 6.141258649096837e-07,
      "loss": 1.5137,
      "step": 4046
    },
    {
      "epoch": 2.664252797893351,
      "grad_norm": 9.995294570922852,
      "learning_rate": 6.11749381493687e-07,
      "loss": 1.8372,
      "step": 4047
    },
    {
      "epoch": 2.664911125740619,
      "grad_norm": 0.7806053757667542,
      "learning_rate": 6.09377360044604e-07,
      "loss": 1.5119,
      "step": 4048
    },
    {
      "epoch": 2.665569453587887,
      "grad_norm": 17.13844108581543,
      "learning_rate": 6.070098016897874e-07,
      "loss": 1.9461,
      "step": 4049
    },
    {
      "epoch": 2.666227781435155,
      "grad_norm": 1.2279112339019775,
      "learning_rate": 6.046467075544715e-07,
      "loss": 1.5113,
      "step": 4050
    },
    {
      "epoch": 2.6668861092824225,
      "grad_norm": 12.825505256652832,
      "learning_rate": 6.02288078761768e-07,
      "loss": 1.6688,
      "step": 4051
    },
    {
      "epoch": 2.6675444371296906,
      "grad_norm": 1.9378201961517334,
      "learning_rate": 5.999339164326634e-07,
      "loss": 1.527,
      "step": 4052
    },
    {
      "epoch": 2.6682027649769586,
      "grad_norm": 1.1645299196243286,
      "learning_rate": 5.975842216860239e-07,
      "loss": 1.5208,
      "step": 4053
    },
    {
      "epoch": 2.6688610928242262,
      "grad_norm": 0.9944173693656921,
      "learning_rate": 5.95238995638594e-07,
      "loss": 1.5139,
      "step": 4054
    },
    {
      "epoch": 2.6695194206714943,
      "grad_norm": 0.7982810735702515,
      "learning_rate": 5.92898239404991e-07,
      "loss": 1.5096,
      "step": 4055
    },
    {
      "epoch": 2.6701777485187623,
      "grad_norm": 18.708423614501953,
      "learning_rate": 5.905619540977092e-07,
      "loss": 2.0093,
      "step": 4056
    },
    {
      "epoch": 2.6708360763660304,
      "grad_norm": 1.0148972272872925,
      "learning_rate": 5.882301408271207e-07,
      "loss": 1.522,
      "step": 4057
    },
    {
      "epoch": 2.6714944042132984,
      "grad_norm": 1.4769514799118042,
      "learning_rate": 5.859028007014667e-07,
      "loss": 1.5342,
      "step": 4058
    },
    {
      "epoch": 2.672152732060566,
      "grad_norm": 14.594158172607422,
      "learning_rate": 5.835799348268656e-07,
      "loss": 2.0465,
      "step": 4059
    },
    {
      "epoch": 2.672811059907834,
      "grad_norm": 43.2928352355957,
      "learning_rate": 5.812615443073122e-07,
      "loss": 2.313,
      "step": 4060
    },
    {
      "epoch": 2.673469387755102,
      "grad_norm": 2.1920006275177,
      "learning_rate": 5.789476302446662e-07,
      "loss": 1.5397,
      "step": 4061
    },
    {
      "epoch": 2.6741277156023697,
      "grad_norm": 18.559837341308594,
      "learning_rate": 5.766381937386679e-07,
      "loss": 2.0129,
      "step": 4062
    },
    {
      "epoch": 2.6747860434496378,
      "grad_norm": 31.10828399658203,
      "learning_rate": 5.74333235886928e-07,
      "loss": 2.4845,
      "step": 4063
    },
    {
      "epoch": 2.675444371296906,
      "grad_norm": 0.6547829508781433,
      "learning_rate": 5.720327577849238e-07,
      "loss": 1.5133,
      "step": 4064
    },
    {
      "epoch": 2.676102699144174,
      "grad_norm": 4.387041091918945,
      "learning_rate": 5.697367605260073e-07,
      "loss": 1.5409,
      "step": 4065
    },
    {
      "epoch": 2.676761026991442,
      "grad_norm": 19.306520462036133,
      "learning_rate": 5.674452452014034e-07,
      "loss": 1.9402,
      "step": 4066
    },
    {
      "epoch": 2.6774193548387095,
      "grad_norm": 38.87457275390625,
      "learning_rate": 5.651582129001987e-07,
      "loss": 3.2817,
      "step": 4067
    },
    {
      "epoch": 2.6780776826859776,
      "grad_norm": 21.637571334838867,
      "learning_rate": 5.628756647093559e-07,
      "loss": 1.9389,
      "step": 4068
    },
    {
      "epoch": 2.6787360105332456,
      "grad_norm": 2.2155368328094482,
      "learning_rate": 5.605976017137082e-07,
      "loss": 1.5275,
      "step": 4069
    },
    {
      "epoch": 2.679394338380513,
      "grad_norm": 15.08833122253418,
      "learning_rate": 5.58324024995951e-07,
      "loss": 1.9054,
      "step": 4070
    },
    {
      "epoch": 2.6800526662277813,
      "grad_norm": 11.183688163757324,
      "learning_rate": 5.560549356366496e-07,
      "loss": 1.8827,
      "step": 4071
    },
    {
      "epoch": 2.6807109940750493,
      "grad_norm": 0.8606430292129517,
      "learning_rate": 5.537903347142382e-07,
      "loss": 1.5168,
      "step": 4072
    },
    {
      "epoch": 2.6813693219223174,
      "grad_norm": 6.184493541717529,
      "learning_rate": 5.515302233050191e-07,
      "loss": 1.5464,
      "step": 4073
    },
    {
      "epoch": 2.6820276497695854,
      "grad_norm": 1.469017744064331,
      "learning_rate": 5.492746024831541e-07,
      "loss": 1.5278,
      "step": 4074
    },
    {
      "epoch": 2.6826859776168535,
      "grad_norm": 0.8430739045143127,
      "learning_rate": 5.470234733206758e-07,
      "loss": 1.5132,
      "step": 4075
    },
    {
      "epoch": 2.683344305464121,
      "grad_norm": 15.364953994750977,
      "learning_rate": 5.447768368874872e-07,
      "loss": 1.795,
      "step": 4076
    },
    {
      "epoch": 2.684002633311389,
      "grad_norm": 0.8182265758514404,
      "learning_rate": 5.425346942513443e-07,
      "loss": 1.5158,
      "step": 4077
    },
    {
      "epoch": 2.684660961158657,
      "grad_norm": 18.545879364013672,
      "learning_rate": 5.402970464778756e-07,
      "loss": 2.0254,
      "step": 4078
    },
    {
      "epoch": 2.6853192890059248,
      "grad_norm": 6.6806321144104,
      "learning_rate": 5.380638946305716e-07,
      "loss": 1.6024,
      "step": 4079
    },
    {
      "epoch": 2.685977616853193,
      "grad_norm": 23.265878677368164,
      "learning_rate": 5.358352397707833e-07,
      "loss": 2.1952,
      "step": 4080
    },
    {
      "epoch": 2.686635944700461,
      "grad_norm": 16.53083038330078,
      "learning_rate": 5.336110829577279e-07,
      "loss": 1.8357,
      "step": 4081
    },
    {
      "epoch": 2.687294272547729,
      "grad_norm": 23.545568466186523,
      "learning_rate": 5.313914252484842e-07,
      "loss": 1.9564,
      "step": 4082
    },
    {
      "epoch": 2.687952600394997,
      "grad_norm": 31.377506256103516,
      "learning_rate": 5.291762676979917e-07,
      "loss": 4.7806,
      "step": 4083
    },
    {
      "epoch": 2.6886109282422646,
      "grad_norm": 23.049354553222656,
      "learning_rate": 5.269656113590505e-07,
      "loss": 2.2799,
      "step": 4084
    },
    {
      "epoch": 2.6892692560895326,
      "grad_norm": 1.3387885093688965,
      "learning_rate": 5.247594572823245e-07,
      "loss": 1.516,
      "step": 4085
    },
    {
      "epoch": 2.6899275839368006,
      "grad_norm": 2.7622196674346924,
      "learning_rate": 5.225578065163317e-07,
      "loss": 1.5667,
      "step": 4086
    },
    {
      "epoch": 2.6905859117840683,
      "grad_norm": 0.9462894797325134,
      "learning_rate": 5.203606601074573e-07,
      "loss": 1.523,
      "step": 4087
    },
    {
      "epoch": 2.6912442396313363,
      "grad_norm": 42.7758674621582,
      "learning_rate": 5.181680190999394e-07,
      "loss": 3.8775,
      "step": 4088
    },
    {
      "epoch": 2.6919025674786043,
      "grad_norm": 16.359140396118164,
      "learning_rate": 5.159798845358798e-07,
      "loss": 1.834,
      "step": 4089
    },
    {
      "epoch": 2.6925608953258724,
      "grad_norm": 13.187623977661133,
      "learning_rate": 5.137962574552358e-07,
      "loss": 1.9054,
      "step": 4090
    },
    {
      "epoch": 2.6932192231731404,
      "grad_norm": 2.161961078643799,
      "learning_rate": 5.116171388958235e-07,
      "loss": 1.5568,
      "step": 4091
    },
    {
      "epoch": 2.693877551020408,
      "grad_norm": 7.742835998535156,
      "learning_rate": 5.094425298933136e-07,
      "loss": 1.7788,
      "step": 4092
    },
    {
      "epoch": 2.694535878867676,
      "grad_norm": 11.670698165893555,
      "learning_rate": 5.072724314812372e-07,
      "loss": 1.6712,
      "step": 4093
    },
    {
      "epoch": 2.695194206714944,
      "grad_norm": 25.457317352294922,
      "learning_rate": 5.051068446909791e-07,
      "loss": 1.987,
      "step": 4094
    },
    {
      "epoch": 2.6958525345622117,
      "grad_norm": 0.8126751780509949,
      "learning_rate": 5.029457705517793e-07,
      "loss": 1.5148,
      "step": 4095
    },
    {
      "epoch": 2.69651086240948,
      "grad_norm": 11.18605899810791,
      "learning_rate": 5.007892100907374e-07,
      "loss": 1.8517,
      "step": 4096
    },
    {
      "epoch": 2.697169190256748,
      "grad_norm": 13.833537101745605,
      "learning_rate": 4.986371643328048e-07,
      "loss": 1.765,
      "step": 4097
    },
    {
      "epoch": 2.697827518104016,
      "grad_norm": 11.509090423583984,
      "learning_rate": 4.964896343007852e-07,
      "loss": 1.7938,
      "step": 4098
    },
    {
      "epoch": 2.698485845951284,
      "grad_norm": 15.837156295776367,
      "learning_rate": 4.943466210153391e-07,
      "loss": 1.7747,
      "step": 4099
    },
    {
      "epoch": 2.6991441737985515,
      "grad_norm": 6.564402103424072,
      "learning_rate": 4.922081254949806e-07,
      "loss": 1.7193,
      "step": 4100
    },
    {
      "epoch": 2.6998025016458196,
      "grad_norm": 12.936012268066406,
      "learning_rate": 4.900741487560723e-07,
      "loss": 1.8487,
      "step": 4101
    },
    {
      "epoch": 2.7004608294930876,
      "grad_norm": 0.8736149668693542,
      "learning_rate": 4.879446918128339e-07,
      "loss": 1.5064,
      "step": 4102
    },
    {
      "epoch": 2.7011191573403552,
      "grad_norm": 10.841869354248047,
      "learning_rate": 4.858197556773381e-07,
      "loss": 1.6588,
      "step": 4103
    },
    {
      "epoch": 2.7017774851876233,
      "grad_norm": 1.0030908584594727,
      "learning_rate": 4.836993413595026e-07,
      "loss": 1.5173,
      "step": 4104
    },
    {
      "epoch": 2.7024358130348913,
      "grad_norm": 16.71080780029297,
      "learning_rate": 4.815834498671012e-07,
      "loss": 1.8963,
      "step": 4105
    },
    {
      "epoch": 2.7030941408821594,
      "grad_norm": 31.825969696044922,
      "learning_rate": 4.794720822057575e-07,
      "loss": 1.8667,
      "step": 4106
    },
    {
      "epoch": 2.7037524687294274,
      "grad_norm": 12.217175483703613,
      "learning_rate": 4.773652393789407e-07,
      "loss": 2.1128,
      "step": 4107
    },
    {
      "epoch": 2.704410796576695,
      "grad_norm": 1.6220906972885132,
      "learning_rate": 4.7526292238797676e-07,
      "loss": 1.5194,
      "step": 4108
    },
    {
      "epoch": 2.705069124423963,
      "grad_norm": 5.27752685546875,
      "learning_rate": 4.731651322320341e-07,
      "loss": 1.6303,
      "step": 4109
    },
    {
      "epoch": 2.705727452271231,
      "grad_norm": 7.259685039520264,
      "learning_rate": 4.7107186990813404e-07,
      "loss": 1.6133,
      "step": 4110
    },
    {
      "epoch": 2.706385780118499,
      "grad_norm": 37.54216384887695,
      "learning_rate": 4.68983136411143e-07,
      "loss": 2.5636,
      "step": 4111
    },
    {
      "epoch": 2.7070441079657668,
      "grad_norm": 1.4489929676055908,
      "learning_rate": 4.6689893273377805e-07,
      "loss": 1.5602,
      "step": 4112
    },
    {
      "epoch": 2.707702435813035,
      "grad_norm": 6.7803497314453125,
      "learning_rate": 4.648192598666013e-07,
      "loss": 1.7354,
      "step": 4113
    },
    {
      "epoch": 2.708360763660303,
      "grad_norm": 21.611682891845703,
      "learning_rate": 4.6274411879801974e-07,
      "loss": 2.3671,
      "step": 4114
    },
    {
      "epoch": 2.709019091507571,
      "grad_norm": 9.901476860046387,
      "learning_rate": 4.60673510514289e-07,
      "loss": 1.7288,
      "step": 4115
    },
    {
      "epoch": 2.709677419354839,
      "grad_norm": 0.9157590270042419,
      "learning_rate": 4.5860743599951186e-07,
      "loss": 1.5274,
      "step": 4116
    },
    {
      "epoch": 2.7103357472021066,
      "grad_norm": 17.568437576293945,
      "learning_rate": 4.5654589623563285e-07,
      "loss": 1.77,
      "step": 4117
    },
    {
      "epoch": 2.7109940750493746,
      "grad_norm": 1.0161293745040894,
      "learning_rate": 4.544888922024426e-07,
      "loss": 1.5155,
      "step": 4118
    },
    {
      "epoch": 2.7116524028966427,
      "grad_norm": 10.5527982711792,
      "learning_rate": 4.524364248775792e-07,
      "loss": 1.7653,
      "step": 4119
    },
    {
      "epoch": 2.7123107307439103,
      "grad_norm": 0.8145741820335388,
      "learning_rate": 4.5038849523651785e-07,
      "loss": 1.511,
      "step": 4120
    },
    {
      "epoch": 2.7129690585911783,
      "grad_norm": 21.363502502441406,
      "learning_rate": 4.4834510425258214e-07,
      "loss": 2.0489,
      "step": 4121
    },
    {
      "epoch": 2.7136273864384464,
      "grad_norm": 16.75841522216797,
      "learning_rate": 4.463062528969375e-07,
      "loss": 2.1393,
      "step": 4122
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 13.977145195007324,
      "learning_rate": 4.4427194213859216e-07,
      "loss": 2.011,
      "step": 4123
    },
    {
      "epoch": 2.7149440421329825,
      "grad_norm": 1.2665901184082031,
      "learning_rate": 4.4224217294439596e-07,
      "loss": 1.5257,
      "step": 4124
    },
    {
      "epoch": 2.71560236998025,
      "grad_norm": 1.5718002319335938,
      "learning_rate": 4.4021694627904065e-07,
      "loss": 1.5314,
      "step": 4125
    },
    {
      "epoch": 2.716260697827518,
      "grad_norm": 10.905608177185059,
      "learning_rate": 4.3819626310505516e-07,
      "loss": 1.8194,
      "step": 4126
    },
    {
      "epoch": 2.716919025674786,
      "grad_norm": 4.7850213050842285,
      "learning_rate": 4.3618012438281565e-07,
      "loss": 1.6505,
      "step": 4127
    },
    {
      "epoch": 2.7175773535220538,
      "grad_norm": 11.390030860900879,
      "learning_rate": 4.341685310705346e-07,
      "loss": 1.8509,
      "step": 4128
    },
    {
      "epoch": 2.718235681369322,
      "grad_norm": 19.218446731567383,
      "learning_rate": 4.321614841242638e-07,
      "loss": 1.9432,
      "step": 4129
    },
    {
      "epoch": 2.71889400921659,
      "grad_norm": 23.720829010009766,
      "learning_rate": 4.30158984497896e-07,
      "loss": 2.0866,
      "step": 4130
    },
    {
      "epoch": 2.719552337063858,
      "grad_norm": 10.22804069519043,
      "learning_rate": 4.2816103314316295e-07,
      "loss": 1.9346,
      "step": 4131
    },
    {
      "epoch": 2.720210664911126,
      "grad_norm": 31.190492630004883,
      "learning_rate": 4.2616763100963187e-07,
      "loss": 2.4768,
      "step": 4132
    },
    {
      "epoch": 2.7208689927583936,
      "grad_norm": 22.986812591552734,
      "learning_rate": 4.241787790447105e-07,
      "loss": 2.3314,
      "step": 4133
    },
    {
      "epoch": 2.7215273206056616,
      "grad_norm": 17.395950317382812,
      "learning_rate": 4.221944781936438e-07,
      "loss": 1.8268,
      "step": 4134
    },
    {
      "epoch": 2.7221856484529297,
      "grad_norm": 1.6508331298828125,
      "learning_rate": 4.202147293995107e-07,
      "loss": 1.522,
      "step": 4135
    },
    {
      "epoch": 2.7228439763001973,
      "grad_norm": 1.6704163551330566,
      "learning_rate": 4.182395336032319e-07,
      "loss": 1.5313,
      "step": 4136
    },
    {
      "epoch": 2.7235023041474653,
      "grad_norm": 21.218164443969727,
      "learning_rate": 4.162688917435631e-07,
      "loss": 1.8994,
      "step": 4137
    },
    {
      "epoch": 2.7241606319947334,
      "grad_norm": 17.39865493774414,
      "learning_rate": 4.143028047570896e-07,
      "loss": 2.0579,
      "step": 4138
    },
    {
      "epoch": 2.7248189598420014,
      "grad_norm": 14.76169490814209,
      "learning_rate": 4.123412735782384e-07,
      "loss": 1.9007,
      "step": 4139
    },
    {
      "epoch": 2.7254772876892694,
      "grad_norm": 16.378225326538086,
      "learning_rate": 4.1038429913927035e-07,
      "loss": 1.7022,
      "step": 4140
    },
    {
      "epoch": 2.726135615536537,
      "grad_norm": 1.2039262056350708,
      "learning_rate": 4.0843188237027596e-07,
      "loss": 1.5175,
      "step": 4141
    },
    {
      "epoch": 2.726793943383805,
      "grad_norm": 14.439841270446777,
      "learning_rate": 4.064840241991863e-07,
      "loss": 1.9383,
      "step": 4142
    },
    {
      "epoch": 2.727452271231073,
      "grad_norm": 11.901518821716309,
      "learning_rate": 4.0454072555176195e-07,
      "loss": 1.9859,
      "step": 4143
    },
    {
      "epoch": 2.7281105990783407,
      "grad_norm": 18.0549259185791,
      "learning_rate": 4.0260198735159514e-07,
      "loss": 1.8331,
      "step": 4144
    },
    {
      "epoch": 2.728768926925609,
      "grad_norm": 0.9893878698348999,
      "learning_rate": 4.006678105201156e-07,
      "loss": 1.5119,
      "step": 4145
    },
    {
      "epoch": 2.729427254772877,
      "grad_norm": 12.721420288085938,
      "learning_rate": 3.9873819597658124e-07,
      "loss": 1.6958,
      "step": 4146
    },
    {
      "epoch": 2.730085582620145,
      "grad_norm": 3.1148934364318848,
      "learning_rate": 3.9681314463808187e-07,
      "loss": 1.5954,
      "step": 4147
    },
    {
      "epoch": 2.730743910467413,
      "grad_norm": 7.149095058441162,
      "learning_rate": 3.9489265741954e-07,
      "loss": 1.7881,
      "step": 4148
    },
    {
      "epoch": 2.731402238314681,
      "grad_norm": 23.251054763793945,
      "learning_rate": 3.929767352337077e-07,
      "loss": 2.2004,
      "step": 4149
    },
    {
      "epoch": 2.7320605661619486,
      "grad_norm": 11.69288158416748,
      "learning_rate": 3.910653789911678e-07,
      "loss": 1.8724,
      "step": 4150
    },
    {
      "epoch": 2.7327188940092166,
      "grad_norm": 1.9181456565856934,
      "learning_rate": 3.891585896003336e-07,
      "loss": 1.5276,
      "step": 4151
    },
    {
      "epoch": 2.7333772218564847,
      "grad_norm": 12.118693351745605,
      "learning_rate": 3.8725636796744794e-07,
      "loss": 1.8868,
      "step": 4152
    },
    {
      "epoch": 2.7340355497037523,
      "grad_norm": 23.515321731567383,
      "learning_rate": 3.853587149965832e-07,
      "loss": 1.745,
      "step": 4153
    },
    {
      "epoch": 2.7346938775510203,
      "grad_norm": 0.8602810502052307,
      "learning_rate": 3.834656315896379e-07,
      "loss": 1.5068,
      "step": 4154
    },
    {
      "epoch": 2.7353522053982884,
      "grad_norm": 9.495943069458008,
      "learning_rate": 3.8157711864634017e-07,
      "loss": 1.5979,
      "step": 4155
    },
    {
      "epoch": 2.7360105332455564,
      "grad_norm": 24.542034149169922,
      "learning_rate": 3.7969317706424757e-07,
      "loss": 2.1003,
      "step": 4156
    },
    {
      "epoch": 2.7366688610928245,
      "grad_norm": 4.832615852355957,
      "learning_rate": 3.7781380773874276e-07,
      "loss": 1.6105,
      "step": 4157
    },
    {
      "epoch": 2.737327188940092,
      "grad_norm": 15.701546669006348,
      "learning_rate": 3.759390115630357e-07,
      "loss": 1.8754,
      "step": 4158
    },
    {
      "epoch": 2.73798551678736,
      "grad_norm": 12.875889778137207,
      "learning_rate": 3.740687894281658e-07,
      "loss": 1.9439,
      "step": 4159
    },
    {
      "epoch": 2.738643844634628,
      "grad_norm": 8.893793106079102,
      "learning_rate": 3.7220314222299437e-07,
      "loss": 1.8716,
      "step": 4160
    },
    {
      "epoch": 2.739302172481896,
      "grad_norm": 11.96914005279541,
      "learning_rate": 3.703420708342087e-07,
      "loss": 1.8876,
      "step": 4161
    },
    {
      "epoch": 2.739960500329164,
      "grad_norm": 25.265050888061523,
      "learning_rate": 3.6848557614632466e-07,
      "loss": 2.2406,
      "step": 4162
    },
    {
      "epoch": 2.740618828176432,
      "grad_norm": 1.107450246810913,
      "learning_rate": 3.666336590416808e-07,
      "loss": 1.5317,
      "step": 4163
    },
    {
      "epoch": 2.7412771560237,
      "grad_norm": 1.13833487033844,
      "learning_rate": 3.6478632040044093e-07,
      "loss": 1.522,
      "step": 4164
    },
    {
      "epoch": 2.741935483870968,
      "grad_norm": 26.46705436706543,
      "learning_rate": 3.629435611005916e-07,
      "loss": 2.2327,
      "step": 4165
    },
    {
      "epoch": 2.7425938117182356,
      "grad_norm": 19.922500610351562,
      "learning_rate": 3.611053820179433e-07,
      "loss": 2.1934,
      "step": 4166
    },
    {
      "epoch": 2.7432521395655036,
      "grad_norm": 27.07373809814453,
      "learning_rate": 3.592717840261295e-07,
      "loss": 1.9511,
      "step": 4167
    },
    {
      "epoch": 2.7439104674127717,
      "grad_norm": 1.0396455526351929,
      "learning_rate": 3.574427679966097e-07,
      "loss": 1.5181,
      "step": 4168
    },
    {
      "epoch": 2.7445687952600393,
      "grad_norm": 13.349143981933594,
      "learning_rate": 3.5561833479865746e-07,
      "loss": 1.8883,
      "step": 4169
    },
    {
      "epoch": 2.7452271231073073,
      "grad_norm": 17.74573516845703,
      "learning_rate": 3.537984852993792e-07,
      "loss": 2.0352,
      "step": 4170
    },
    {
      "epoch": 2.7458854509545754,
      "grad_norm": 2.8536388874053955,
      "learning_rate": 3.5198322036369523e-07,
      "loss": 1.5862,
      "step": 4171
    },
    {
      "epoch": 2.7465437788018434,
      "grad_norm": 5.9175333976745605,
      "learning_rate": 3.5017254085434773e-07,
      "loss": 1.6009,
      "step": 4172
    },
    {
      "epoch": 2.7472021066491115,
      "grad_norm": 9.249058723449707,
      "learning_rate": 3.4836644763190264e-07,
      "loss": 1.722,
      "step": 4173
    },
    {
      "epoch": 2.747860434496379,
      "grad_norm": 2.276339530944824,
      "learning_rate": 3.465649415547445e-07,
      "loss": 1.5713,
      "step": 4174
    },
    {
      "epoch": 2.748518762343647,
      "grad_norm": 1.4444671869277954,
      "learning_rate": 3.4476802347907623e-07,
      "loss": 1.5172,
      "step": 4175
    },
    {
      "epoch": 2.749177090190915,
      "grad_norm": 21.463653564453125,
      "learning_rate": 3.429756942589202e-07,
      "loss": 2.4445,
      "step": 4176
    },
    {
      "epoch": 2.7498354180381828,
      "grad_norm": 0.8813523650169373,
      "learning_rate": 3.41187954746125e-07,
      "loss": 1.5104,
      "step": 4177
    },
    {
      "epoch": 2.750493745885451,
      "grad_norm": 7.756206512451172,
      "learning_rate": 3.3940480579034653e-07,
      "loss": 1.7969,
      "step": 4178
    },
    {
      "epoch": 2.751152073732719,
      "grad_norm": 3.379122257232666,
      "learning_rate": 3.3762624823906577e-07,
      "loss": 1.5883,
      "step": 4179
    },
    {
      "epoch": 2.751810401579987,
      "grad_norm": 12.328265190124512,
      "learning_rate": 3.358522829375821e-07,
      "loss": 1.9968,
      "step": 4180
    },
    {
      "epoch": 2.752468729427255,
      "grad_norm": 33.0652961730957,
      "learning_rate": 3.340829107290078e-07,
      "loss": 2.5516,
      "step": 4181
    },
    {
      "epoch": 2.7531270572745226,
      "grad_norm": 0.9761161804199219,
      "learning_rate": 3.323181324542768e-07,
      "loss": 1.5132,
      "step": 4182
    },
    {
      "epoch": 2.7537853851217906,
      "grad_norm": 12.247359275817871,
      "learning_rate": 3.305579489521371e-07,
      "loss": 1.8598,
      "step": 4183
    },
    {
      "epoch": 2.7544437129690587,
      "grad_norm": 0.8114038705825806,
      "learning_rate": 3.288023610591529e-07,
      "loss": 1.5111,
      "step": 4184
    },
    {
      "epoch": 2.7551020408163263,
      "grad_norm": 15.137323379516602,
      "learning_rate": 3.2705136960970554e-07,
      "loss": 1.9588,
      "step": 4185
    },
    {
      "epoch": 2.7557603686635943,
      "grad_norm": 14.751242637634277,
      "learning_rate": 3.253049754359927e-07,
      "loss": 2.0192,
      "step": 4186
    },
    {
      "epoch": 2.7564186965108624,
      "grad_norm": 0.8060374855995178,
      "learning_rate": 3.2356317936802275e-07,
      "loss": 1.5115,
      "step": 4187
    },
    {
      "epoch": 2.7570770243581304,
      "grad_norm": 1.0668026208877563,
      "learning_rate": 3.218259822336234e-07,
      "loss": 1.5178,
      "step": 4188
    },
    {
      "epoch": 2.7577353522053984,
      "grad_norm": 7.369295120239258,
      "learning_rate": 3.200933848584331e-07,
      "loss": 1.8242,
      "step": 4189
    },
    {
      "epoch": 2.7583936800526665,
      "grad_norm": 1.3837640285491943,
      "learning_rate": 3.1836538806590764e-07,
      "loss": 1.5284,
      "step": 4190
    },
    {
      "epoch": 2.759052007899934,
      "grad_norm": 1.139994502067566,
      "learning_rate": 3.166419926773134e-07,
      "loss": 1.5189,
      "step": 4191
    },
    {
      "epoch": 2.759710335747202,
      "grad_norm": 14.477264404296875,
      "learning_rate": 3.149231995117319e-07,
      "loss": 1.8306,
      "step": 4192
    },
    {
      "epoch": 2.76036866359447,
      "grad_norm": 34.1145133972168,
      "learning_rate": 3.1320900938605627e-07,
      "loss": 2.5811,
      "step": 4193
    },
    {
      "epoch": 2.761026991441738,
      "grad_norm": 14.01525592803955,
      "learning_rate": 3.1149942311498925e-07,
      "loss": 1.9593,
      "step": 4194
    },
    {
      "epoch": 2.761685319289006,
      "grad_norm": 5.101184368133545,
      "learning_rate": 3.097944415110521e-07,
      "loss": 1.5664,
      "step": 4195
    },
    {
      "epoch": 2.762343647136274,
      "grad_norm": 7.846833229064941,
      "learning_rate": 3.08094065384571e-07,
      "loss": 1.7179,
      "step": 4196
    },
    {
      "epoch": 2.763001974983542,
      "grad_norm": 34.18485641479492,
      "learning_rate": 3.0639829554368726e-07,
      "loss": 2.1844,
      "step": 4197
    },
    {
      "epoch": 2.76366030283081,
      "grad_norm": 1.1007606983184814,
      "learning_rate": 3.0470713279435184e-07,
      "loss": 1.5137,
      "step": 4198
    },
    {
      "epoch": 2.7643186306780776,
      "grad_norm": 3.620866298675537,
      "learning_rate": 3.030205779403272e-07,
      "loss": 1.5709,
      "step": 4199
    },
    {
      "epoch": 2.7649769585253456,
      "grad_norm": 17.78203010559082,
      "learning_rate": 3.0133863178318237e-07,
      "loss": 1.9962,
      "step": 4200
    },
    {
      "epoch": 2.7656352863726137,
      "grad_norm": 18.769495010375977,
      "learning_rate": 2.996612951222988e-07,
      "loss": 2.1113,
      "step": 4201
    },
    {
      "epoch": 2.7662936142198813,
      "grad_norm": 13.377326965332031,
      "learning_rate": 2.979885687548678e-07,
      "loss": 1.8739,
      "step": 4202
    },
    {
      "epoch": 2.7669519420671493,
      "grad_norm": 15.742716789245605,
      "learning_rate": 2.9632045347588566e-07,
      "loss": 1.9841,
      "step": 4203
    },
    {
      "epoch": 2.7676102699144174,
      "grad_norm": 6.73507833480835,
      "learning_rate": 2.9465695007816374e-07,
      "loss": 1.6302,
      "step": 4204
    },
    {
      "epoch": 2.7682685977616854,
      "grad_norm": 2.6388230323791504,
      "learning_rate": 2.929980593523152e-07,
      "loss": 1.5834,
      "step": 4205
    },
    {
      "epoch": 2.7689269256089535,
      "grad_norm": 34.74070358276367,
      "learning_rate": 2.913437820867637e-07,
      "loss": 2.145,
      "step": 4206
    },
    {
      "epoch": 2.769585253456221,
      "grad_norm": 28.549951553344727,
      "learning_rate": 2.8969411906773935e-07,
      "loss": 2.3509,
      "step": 4207
    },
    {
      "epoch": 2.770243581303489,
      "grad_norm": 2.7270395755767822,
      "learning_rate": 2.880490710792816e-07,
      "loss": 1.5372,
      "step": 4208
    },
    {
      "epoch": 2.770901909150757,
      "grad_norm": 32.89223098754883,
      "learning_rate": 2.864086389032317e-07,
      "loss": 2.8044,
      "step": 4209
    },
    {
      "epoch": 2.771560236998025,
      "grad_norm": 22.9837703704834,
      "learning_rate": 2.847728233192415e-07,
      "loss": 2.209,
      "step": 4210
    },
    {
      "epoch": 2.772218564845293,
      "grad_norm": 1.2622486352920532,
      "learning_rate": 2.831416251047703e-07,
      "loss": 1.5198,
      "step": 4211
    },
    {
      "epoch": 2.772876892692561,
      "grad_norm": 13.124272346496582,
      "learning_rate": 2.8151504503507566e-07,
      "loss": 2.2227,
      "step": 4212
    },
    {
      "epoch": 2.773535220539829,
      "grad_norm": 2.455192804336548,
      "learning_rate": 2.798930838832259e-07,
      "loss": 1.5654,
      "step": 4213
    },
    {
      "epoch": 2.774193548387097,
      "grad_norm": 31.47231101989746,
      "learning_rate": 2.7827574242009434e-07,
      "loss": 2.168,
      "step": 4214
    },
    {
      "epoch": 2.7748518762343646,
      "grad_norm": 46.58917999267578,
      "learning_rate": 2.7666302141435506e-07,
      "loss": 1.9545,
      "step": 4215
    },
    {
      "epoch": 2.7755102040816326,
      "grad_norm": 33.04547119140625,
      "learning_rate": 2.750549216324894e-07,
      "loss": 3.1642,
      "step": 4216
    },
    {
      "epoch": 2.7761685319289007,
      "grad_norm": 6.6212005615234375,
      "learning_rate": 2.734514438387803e-07,
      "loss": 1.6015,
      "step": 4217
    },
    {
      "epoch": 2.7768268597761683,
      "grad_norm": 0.8147762417793274,
      "learning_rate": 2.71852588795315e-07,
      "loss": 1.5081,
      "step": 4218
    },
    {
      "epoch": 2.7774851876234363,
      "grad_norm": 1.7734816074371338,
      "learning_rate": 2.7025835726198433e-07,
      "loss": 1.5323,
      "step": 4219
    },
    {
      "epoch": 2.7781435154707044,
      "grad_norm": 1.0114659070968628,
      "learning_rate": 2.686687499964824e-07,
      "loss": 1.5262,
      "step": 4220
    },
    {
      "epoch": 2.7788018433179724,
      "grad_norm": 16.920814514160156,
      "learning_rate": 2.6708376775430035e-07,
      "loss": 1.9371,
      "step": 4221
    },
    {
      "epoch": 2.7794601711652405,
      "grad_norm": 1.4809125661849976,
      "learning_rate": 2.6550341128873667e-07,
      "loss": 1.5556,
      "step": 4222
    },
    {
      "epoch": 2.780118499012508,
      "grad_norm": 26.187503814697266,
      "learning_rate": 2.6392768135088955e-07,
      "loss": 2.3904,
      "step": 4223
    },
    {
      "epoch": 2.780776826859776,
      "grad_norm": 1.2903776168823242,
      "learning_rate": 2.623565786896587e-07,
      "loss": 1.5221,
      "step": 4224
    },
    {
      "epoch": 2.781435154707044,
      "grad_norm": 1.2710862159729004,
      "learning_rate": 2.6079010405174355e-07,
      "loss": 1.5139,
      "step": 4225
    },
    {
      "epoch": 2.782093482554312,
      "grad_norm": 6.658785820007324,
      "learning_rate": 2.592282581816452e-07,
      "loss": 1.6568,
      "step": 4226
    },
    {
      "epoch": 2.78275181040158,
      "grad_norm": 1.8093421459197998,
      "learning_rate": 2.5767104182166544e-07,
      "loss": 1.5302,
      "step": 4227
    },
    {
      "epoch": 2.783410138248848,
      "grad_norm": 15.588774681091309,
      "learning_rate": 2.561184557119023e-07,
      "loss": 2.2131,
      "step": 4228
    },
    {
      "epoch": 2.784068466096116,
      "grad_norm": 5.333750247955322,
      "learning_rate": 2.5457050059025766e-07,
      "loss": 1.7592,
      "step": 4229
    },
    {
      "epoch": 2.784726793943384,
      "grad_norm": 0.7490883469581604,
      "learning_rate": 2.5302717719242864e-07,
      "loss": 1.5091,
      "step": 4230
    },
    {
      "epoch": 2.785385121790652,
      "grad_norm": 14.506229400634766,
      "learning_rate": 2.514884862519129e-07,
      "loss": 2.0214,
      "step": 4231
    },
    {
      "epoch": 2.7860434496379196,
      "grad_norm": 14.410774230957031,
      "learning_rate": 2.499544285000066e-07,
      "loss": 1.7004,
      "step": 4232
    },
    {
      "epoch": 2.7867017774851877,
      "grad_norm": 0.894683837890625,
      "learning_rate": 2.484250046658054e-07,
      "loss": 1.5155,
      "step": 4233
    },
    {
      "epoch": 2.7873601053324557,
      "grad_norm": 3.9480104446411133,
      "learning_rate": 2.469002154761979e-07,
      "loss": 1.5885,
      "step": 4234
    },
    {
      "epoch": 2.7880184331797233,
      "grad_norm": 28.240543365478516,
      "learning_rate": 2.4538006165587323e-07,
      "loss": 2.0997,
      "step": 4235
    },
    {
      "epoch": 2.7886767610269914,
      "grad_norm": 33.04494857788086,
      "learning_rate": 2.438645439273168e-07,
      "loss": 2.8983,
      "step": 4236
    },
    {
      "epoch": 2.7893350888742594,
      "grad_norm": 9.247055053710938,
      "learning_rate": 2.4235366301081253e-07,
      "loss": 1.7653,
      "step": 4237
    },
    {
      "epoch": 2.7899934167215275,
      "grad_norm": 8.59865665435791,
      "learning_rate": 2.4084741962443814e-07,
      "loss": 1.8027,
      "step": 4238
    },
    {
      "epoch": 2.7906517445687955,
      "grad_norm": 24.70685386657715,
      "learning_rate": 2.393458144840688e-07,
      "loss": 2.5721,
      "step": 4239
    },
    {
      "epoch": 2.791310072416063,
      "grad_norm": 3.9881339073181152,
      "learning_rate": 2.378488483033714e-07,
      "loss": 1.637,
      "step": 4240
    },
    {
      "epoch": 2.791968400263331,
      "grad_norm": 0.9923832416534424,
      "learning_rate": 2.3635652179381462e-07,
      "loss": 1.5187,
      "step": 4241
    },
    {
      "epoch": 2.792626728110599,
      "grad_norm": 0.7663981318473816,
      "learning_rate": 2.348688356646578e-07,
      "loss": 1.5101,
      "step": 4242
    },
    {
      "epoch": 2.793285055957867,
      "grad_norm": 1.051369547843933,
      "learning_rate": 2.3338579062295308e-07,
      "loss": 1.5161,
      "step": 4243
    },
    {
      "epoch": 2.793943383805135,
      "grad_norm": 2.9089431762695312,
      "learning_rate": 2.3190738737355113e-07,
      "loss": 1.5374,
      "step": 4244
    },
    {
      "epoch": 2.794601711652403,
      "grad_norm": 12.043601989746094,
      "learning_rate": 2.3043362661909652e-07,
      "loss": 1.913,
      "step": 4245
    },
    {
      "epoch": 2.795260039499671,
      "grad_norm": 1.6044340133666992,
      "learning_rate": 2.2896450906002232e-07,
      "loss": 1.5265,
      "step": 4246
    },
    {
      "epoch": 2.795918367346939,
      "grad_norm": 0.9266946315765381,
      "learning_rate": 2.2750003539456e-07,
      "loss": 1.5177,
      "step": 4247
    },
    {
      "epoch": 2.7965766951942066,
      "grad_norm": 0.9327749013900757,
      "learning_rate": 2.260402063187328e-07,
      "loss": 1.5243,
      "step": 4248
    },
    {
      "epoch": 2.7972350230414746,
      "grad_norm": 15.560029029846191,
      "learning_rate": 2.2458502252635238e-07,
      "loss": 2.0034,
      "step": 4249
    },
    {
      "epoch": 2.7978933508887427,
      "grad_norm": 7.098039150238037,
      "learning_rate": 2.231344847090289e-07,
      "loss": 1.7371,
      "step": 4250
    },
    {
      "epoch": 2.7985516787360103,
      "grad_norm": 15.144810676574707,
      "learning_rate": 2.216885935561608e-07,
      "loss": 2.0632,
      "step": 4251
    },
    {
      "epoch": 2.7992100065832783,
      "grad_norm": 1.3644224405288696,
      "learning_rate": 2.2024734975493844e-07,
      "loss": 1.5193,
      "step": 4252
    },
    {
      "epoch": 2.7998683344305464,
      "grad_norm": 26.99726676940918,
      "learning_rate": 2.188107539903439e-07,
      "loss": 1.7977,
      "step": 4253
    },
    {
      "epoch": 2.8005266622778144,
      "grad_norm": 10.759363174438477,
      "learning_rate": 2.17378806945151e-07,
      "loss": 1.7545,
      "step": 4254
    },
    {
      "epoch": 2.8011849901250825,
      "grad_norm": 1.2795281410217285,
      "learning_rate": 2.1595150929992202e-07,
      "loss": 1.5509,
      "step": 4255
    },
    {
      "epoch": 2.80184331797235,
      "grad_norm": 6.215941429138184,
      "learning_rate": 2.145288617330099e-07,
      "loss": 1.7215,
      "step": 4256
    },
    {
      "epoch": 2.802501645819618,
      "grad_norm": 16.846330642700195,
      "learning_rate": 2.1311086492056043e-07,
      "loss": 1.9181,
      "step": 4257
    },
    {
      "epoch": 2.803159973666886,
      "grad_norm": 1.3373557329177856,
      "learning_rate": 2.1169751953650564e-07,
      "loss": 1.5264,
      "step": 4258
    },
    {
      "epoch": 2.803818301514154,
      "grad_norm": 5.903208255767822,
      "learning_rate": 2.102888262525682e-07,
      "loss": 1.6175,
      "step": 4259
    },
    {
      "epoch": 2.804476629361422,
      "grad_norm": 20.361846923828125,
      "learning_rate": 2.0888478573826033e-07,
      "loss": 1.927,
      "step": 4260
    },
    {
      "epoch": 2.80513495720869,
      "grad_norm": 9.555825233459473,
      "learning_rate": 2.0748539866088157e-07,
      "loss": 1.8493,
      "step": 4261
    },
    {
      "epoch": 2.805793285055958,
      "grad_norm": 14.160176277160645,
      "learning_rate": 2.0609066568552104e-07,
      "loss": 1.9177,
      "step": 4262
    },
    {
      "epoch": 2.806451612903226,
      "grad_norm": 17.43738555908203,
      "learning_rate": 2.0470058747505516e-07,
      "loss": 1.9073,
      "step": 4263
    },
    {
      "epoch": 2.8071099407504936,
      "grad_norm": 15.700631141662598,
      "learning_rate": 2.033151646901488e-07,
      "loss": 1.9128,
      "step": 4264
    },
    {
      "epoch": 2.8077682685977616,
      "grad_norm": 9.714540481567383,
      "learning_rate": 2.0193439798925295e-07,
      "loss": 1.7688,
      "step": 4265
    },
    {
      "epoch": 2.8084265964450297,
      "grad_norm": 19.320722579956055,
      "learning_rate": 2.0055828802860722e-07,
      "loss": 2.0684,
      "step": 4266
    },
    {
      "epoch": 2.8090849242922977,
      "grad_norm": 27.202220916748047,
      "learning_rate": 1.9918683546223727e-07,
      "loss": 2.7135,
      "step": 4267
    },
    {
      "epoch": 2.8097432521395653,
      "grad_norm": 0.7026146650314331,
      "learning_rate": 1.9782004094195505e-07,
      "loss": 1.5087,
      "step": 4268
    },
    {
      "epoch": 2.8104015799868334,
      "grad_norm": 35.80150604248047,
      "learning_rate": 1.9645790511735874e-07,
      "loss": 2.7159,
      "step": 4269
    },
    {
      "epoch": 2.8110599078341014,
      "grad_norm": 20.251110076904297,
      "learning_rate": 1.9510042863583268e-07,
      "loss": 2.2297,
      "step": 4270
    },
    {
      "epoch": 2.8117182356813695,
      "grad_norm": 0.8175075650215149,
      "learning_rate": 1.9374761214254635e-07,
      "loss": 1.5154,
      "step": 4271
    },
    {
      "epoch": 2.8123765635286375,
      "grad_norm": 2.0194132328033447,
      "learning_rate": 1.9239945628045543e-07,
      "loss": 1.5328,
      "step": 4272
    },
    {
      "epoch": 2.813034891375905,
      "grad_norm": 19.791057586669922,
      "learning_rate": 1.9105596169029962e-07,
      "loss": 2.1744,
      "step": 4273
    },
    {
      "epoch": 2.813693219223173,
      "grad_norm": 10.64494800567627,
      "learning_rate": 1.8971712901060257e-07,
      "loss": 1.8284,
      "step": 4274
    },
    {
      "epoch": 2.814351547070441,
      "grad_norm": 13.655122756958008,
      "learning_rate": 1.883829588776742e-07,
      "loss": 1.8023,
      "step": 4275
    },
    {
      "epoch": 2.815009874917709,
      "grad_norm": 2.671926975250244,
      "learning_rate": 1.8705345192560843e-07,
      "loss": 1.5314,
      "step": 4276
    },
    {
      "epoch": 2.815668202764977,
      "grad_norm": 4.119401454925537,
      "learning_rate": 1.857286087862775e-07,
      "loss": 1.5377,
      "step": 4277
    },
    {
      "epoch": 2.816326530612245,
      "grad_norm": 3.97298526763916,
      "learning_rate": 1.844084300893456e-07,
      "loss": 1.6749,
      "step": 4278
    },
    {
      "epoch": 2.816984858459513,
      "grad_norm": 11.185672760009766,
      "learning_rate": 1.8309291646225634e-07,
      "loss": 1.8814,
      "step": 4279
    },
    {
      "epoch": 2.817643186306781,
      "grad_norm": 24.03297996520996,
      "learning_rate": 1.8178206853023295e-07,
      "loss": 2.0537,
      "step": 4280
    },
    {
      "epoch": 2.8183015141540486,
      "grad_norm": 11.715015411376953,
      "learning_rate": 1.8047588691628482e-07,
      "loss": 1.8451,
      "step": 4281
    },
    {
      "epoch": 2.8189598420013167,
      "grad_norm": 12.724862098693848,
      "learning_rate": 1.7917437224120315e-07,
      "loss": 2.1536,
      "step": 4282
    },
    {
      "epoch": 2.8196181698485847,
      "grad_norm": 17.73977279663086,
      "learning_rate": 1.7787752512356093e-07,
      "loss": 2.0169,
      "step": 4283
    },
    {
      "epoch": 2.8202764976958523,
      "grad_norm": 13.274462699890137,
      "learning_rate": 1.765853461797107e-07,
      "loss": 1.8599,
      "step": 4284
    },
    {
      "epoch": 2.8209348255431204,
      "grad_norm": 9.697179794311523,
      "learning_rate": 1.7529783602379003e-07,
      "loss": 1.7633,
      "step": 4285
    },
    {
      "epoch": 2.8215931533903884,
      "grad_norm": 43.16257095336914,
      "learning_rate": 1.7401499526771503e-07,
      "loss": 4.0188,
      "step": 4286
    },
    {
      "epoch": 2.8222514812376565,
      "grad_norm": 1.3866512775421143,
      "learning_rate": 1.7273682452118357e-07,
      "loss": 1.5173,
      "step": 4287
    },
    {
      "epoch": 2.8229098090849245,
      "grad_norm": 11.870980262756348,
      "learning_rate": 1.714633243916741e-07,
      "loss": 2.0877,
      "step": 4288
    },
    {
      "epoch": 2.823568136932192,
      "grad_norm": 7.719543933868408,
      "learning_rate": 1.7019449548444257e-07,
      "loss": 1.5899,
      "step": 4289
    },
    {
      "epoch": 2.82422646477946,
      "grad_norm": 5.586095809936523,
      "learning_rate": 1.689303384025287e-07,
      "loss": 1.6571,
      "step": 4290
    },
    {
      "epoch": 2.824884792626728,
      "grad_norm": 1.26957106590271,
      "learning_rate": 1.6767085374675085e-07,
      "loss": 1.5222,
      "step": 4291
    },
    {
      "epoch": 2.825543120473996,
      "grad_norm": 7.9999518394470215,
      "learning_rate": 1.6641604211570461e-07,
      "loss": 1.7219,
      "step": 4292
    },
    {
      "epoch": 2.826201448321264,
      "grad_norm": 19.710674285888672,
      "learning_rate": 1.6516590410576628e-07,
      "loss": 1.83,
      "step": 4293
    },
    {
      "epoch": 2.826859776168532,
      "grad_norm": 50.80888748168945,
      "learning_rate": 1.6392044031109168e-07,
      "loss": 3.3506,
      "step": 4294
    },
    {
      "epoch": 2.8275181040158,
      "grad_norm": 9.393173217773438,
      "learning_rate": 1.6267965132361286e-07,
      "loss": 1.6826,
      "step": 4295
    },
    {
      "epoch": 2.828176431863068,
      "grad_norm": 20.619903564453125,
      "learning_rate": 1.6144353773304255e-07,
      "loss": 1.9003,
      "step": 4296
    },
    {
      "epoch": 2.8288347597103356,
      "grad_norm": 25.58301544189453,
      "learning_rate": 1.6021210012686862e-07,
      "loss": 2.1041,
      "step": 4297
    },
    {
      "epoch": 2.8294930875576036,
      "grad_norm": 5.836606502532959,
      "learning_rate": 1.5898533909035952e-07,
      "loss": 1.6565,
      "step": 4298
    },
    {
      "epoch": 2.8301514154048717,
      "grad_norm": 1.0227110385894775,
      "learning_rate": 1.577632552065589e-07,
      "loss": 1.5142,
      "step": 4299
    },
    {
      "epoch": 2.8308097432521393,
      "grad_norm": 0.7531931400299072,
      "learning_rate": 1.5654584905628988e-07,
      "loss": 1.5066,
      "step": 4300
    },
    {
      "epoch": 2.8314680710994073,
      "grad_norm": 10.458147048950195,
      "learning_rate": 1.553331212181486e-07,
      "loss": 1.8193,
      "step": 4301
    },
    {
      "epoch": 2.8321263989466754,
      "grad_norm": 1.529517650604248,
      "learning_rate": 1.541250722685106e-07,
      "loss": 1.5169,
      "step": 4302
    },
    {
      "epoch": 2.8327847267939434,
      "grad_norm": 9.005282402038574,
      "learning_rate": 1.5292170278152774e-07,
      "loss": 1.7584,
      "step": 4303
    },
    {
      "epoch": 2.8334430546412115,
      "grad_norm": 45.334136962890625,
      "learning_rate": 1.5172301332912586e-07,
      "loss": 2.8384,
      "step": 4304
    },
    {
      "epoch": 2.8341013824884795,
      "grad_norm": 16.369245529174805,
      "learning_rate": 1.5052900448100815e-07,
      "loss": 1.8803,
      "step": 4305
    },
    {
      "epoch": 2.834759710335747,
      "grad_norm": 0.7469393610954285,
      "learning_rate": 1.493396768046529e-07,
      "loss": 1.5094,
      "step": 4306
    },
    {
      "epoch": 2.835418038183015,
      "grad_norm": 9.959969520568848,
      "learning_rate": 1.481550308653157e-07,
      "loss": 1.9994,
      "step": 4307
    },
    {
      "epoch": 2.8360763660302832,
      "grad_norm": 13.750096321105957,
      "learning_rate": 1.4697506722602172e-07,
      "loss": 2.0588,
      "step": 4308
    },
    {
      "epoch": 2.836734693877551,
      "grad_norm": 3.353931427001953,
      "learning_rate": 1.4579978644757463e-07,
      "loss": 1.5464,
      "step": 4309
    },
    {
      "epoch": 2.837393021724819,
      "grad_norm": 6.74446964263916,
      "learning_rate": 1.4462918908855318e-07,
      "loss": 1.8402,
      "step": 4310
    },
    {
      "epoch": 2.838051349572087,
      "grad_norm": 0.8179217576980591,
      "learning_rate": 1.4346327570530672e-07,
      "loss": 1.5207,
      "step": 4311
    },
    {
      "epoch": 2.838709677419355,
      "grad_norm": 0.8205592632293701,
      "learning_rate": 1.4230204685196202e-07,
      "loss": 1.5108,
      "step": 4312
    },
    {
      "epoch": 2.839368005266623,
      "grad_norm": 11.434306144714355,
      "learning_rate": 1.4114550308041875e-07,
      "loss": 1.796,
      "step": 4313
    },
    {
      "epoch": 2.8400263331138906,
      "grad_norm": 0.7821592688560486,
      "learning_rate": 1.3999364494034716e-07,
      "loss": 1.5126,
      "step": 4314
    },
    {
      "epoch": 2.8406846609611587,
      "grad_norm": 34.35410690307617,
      "learning_rate": 1.3884647297919273e-07,
      "loss": 3.082,
      "step": 4315
    },
    {
      "epoch": 2.8413429888084267,
      "grad_norm": 18.77177619934082,
      "learning_rate": 1.3770398774217485e-07,
      "loss": 1.8617,
      "step": 4316
    },
    {
      "epoch": 2.8420013166556943,
      "grad_norm": 13.928350448608398,
      "learning_rate": 1.3656618977228253e-07,
      "loss": 1.8032,
      "step": 4317
    },
    {
      "epoch": 2.8426596445029624,
      "grad_norm": 9.469419479370117,
      "learning_rate": 1.3543307961027873e-07,
      "loss": 1.6061,
      "step": 4318
    },
    {
      "epoch": 2.8433179723502304,
      "grad_norm": 1.7392393350601196,
      "learning_rate": 1.3430465779469937e-07,
      "loss": 1.5289,
      "step": 4319
    },
    {
      "epoch": 2.8439763001974985,
      "grad_norm": 48.92757034301758,
      "learning_rate": 1.3318092486185096e-07,
      "loss": 1.6331,
      "step": 4320
    },
    {
      "epoch": 2.8446346280447665,
      "grad_norm": 2.7491533756256104,
      "learning_rate": 1.3206188134581078e-07,
      "loss": 1.5718,
      "step": 4321
    },
    {
      "epoch": 2.845292955892034,
      "grad_norm": 6.548672676086426,
      "learning_rate": 1.3094752777842888e-07,
      "loss": 1.745,
      "step": 4322
    },
    {
      "epoch": 2.845951283739302,
      "grad_norm": 1.6021788120269775,
      "learning_rate": 1.2983786468932503e-07,
      "loss": 1.5267,
      "step": 4323
    },
    {
      "epoch": 2.84660961158657,
      "grad_norm": 1.0055193901062012,
      "learning_rate": 1.287328926058906e-07,
      "loss": 1.5118,
      "step": 4324
    },
    {
      "epoch": 2.847267939433838,
      "grad_norm": 14.13939380645752,
      "learning_rate": 1.276326120532867e-07,
      "loss": 1.8609,
      "step": 4325
    },
    {
      "epoch": 2.847926267281106,
      "grad_norm": 9.405367851257324,
      "learning_rate": 1.2653702355444608e-07,
      "loss": 1.7937,
      "step": 4326
    },
    {
      "epoch": 2.848584595128374,
      "grad_norm": 7.262197971343994,
      "learning_rate": 1.2544612763007002e-07,
      "loss": 1.7064,
      "step": 4327
    },
    {
      "epoch": 2.849242922975642,
      "grad_norm": 1.621739387512207,
      "learning_rate": 1.2435992479863158e-07,
      "loss": 1.5495,
      "step": 4328
    },
    {
      "epoch": 2.84990125082291,
      "grad_norm": 15.658426284790039,
      "learning_rate": 1.2327841557636889e-07,
      "loss": 1.8949,
      "step": 4329
    },
    {
      "epoch": 2.8505595786701776,
      "grad_norm": 52.44617462158203,
      "learning_rate": 1.2220160047729523e-07,
      "loss": 2.4743,
      "step": 4330
    },
    {
      "epoch": 2.8512179065174457,
      "grad_norm": 32.044281005859375,
      "learning_rate": 1.2112948001318904e-07,
      "loss": 3.1769,
      "step": 4331
    },
    {
      "epoch": 2.8518762343647137,
      "grad_norm": 1.0145169496536255,
      "learning_rate": 1.200620546935971e-07,
      "loss": 1.5208,
      "step": 4332
    },
    {
      "epoch": 2.8525345622119813,
      "grad_norm": 17.092063903808594,
      "learning_rate": 1.1899932502583811e-07,
      "loss": 1.9202,
      "step": 4333
    },
    {
      "epoch": 2.8531928900592494,
      "grad_norm": 20.73301124572754,
      "learning_rate": 1.17941291514998e-07,
      "loss": 2.0754,
      "step": 4334
    },
    {
      "epoch": 2.8538512179065174,
      "grad_norm": 33.07374954223633,
      "learning_rate": 1.1688795466392567e-07,
      "loss": 2.2715,
      "step": 4335
    },
    {
      "epoch": 2.8545095457537855,
      "grad_norm": 0.9567301869392395,
      "learning_rate": 1.1583931497324619e-07,
      "loss": 1.5116,
      "step": 4336
    },
    {
      "epoch": 2.8551678736010535,
      "grad_norm": 8.035654067993164,
      "learning_rate": 1.1479537294134536e-07,
      "loss": 1.6625,
      "step": 4337
    },
    {
      "epoch": 2.855826201448321,
      "grad_norm": 18.63203239440918,
      "learning_rate": 1.1375612906438072e-07,
      "loss": 1.7515,
      "step": 4338
    },
    {
      "epoch": 2.856484529295589,
      "grad_norm": 0.7414283752441406,
      "learning_rate": 1.1272158383627273e-07,
      "loss": 1.5104,
      "step": 4339
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 45.846710205078125,
      "learning_rate": 1.1169173774871478e-07,
      "loss": 2.819,
      "step": 4340
    },
    {
      "epoch": 2.857801184990125,
      "grad_norm": 9.811197280883789,
      "learning_rate": 1.1066659129116086e-07,
      "loss": 1.8669,
      "step": 4341
    },
    {
      "epoch": 2.858459512837393,
      "grad_norm": 0.5232690572738647,
      "learning_rate": 1.0964614495083348e-07,
      "loss": 1.506,
      "step": 4342
    },
    {
      "epoch": 2.859117840684661,
      "grad_norm": 14.670907974243164,
      "learning_rate": 1.0863039921272133e-07,
      "loss": 1.976,
      "step": 4343
    },
    {
      "epoch": 2.859776168531929,
      "grad_norm": 29.48641586303711,
      "learning_rate": 1.0761935455958273e-07,
      "loss": 1.7008,
      "step": 4344
    },
    {
      "epoch": 2.860434496379197,
      "grad_norm": 21.469112396240234,
      "learning_rate": 1.0661301147193326e-07,
      "loss": 1.8356,
      "step": 4345
    },
    {
      "epoch": 2.861092824226465,
      "grad_norm": 8.721500396728516,
      "learning_rate": 1.0561137042806258e-07,
      "loss": 1.8984,
      "step": 4346
    },
    {
      "epoch": 2.8617511520737327,
      "grad_norm": 2.766256809234619,
      "learning_rate": 1.0461443190402098e-07,
      "loss": 1.5339,
      "step": 4347
    },
    {
      "epoch": 2.8624094799210007,
      "grad_norm": 1.8672677278518677,
      "learning_rate": 1.0362219637362391e-07,
      "loss": 1.5242,
      "step": 4348
    },
    {
      "epoch": 2.8630678077682687,
      "grad_norm": 1.6559849977493286,
      "learning_rate": 1.0263466430845415e-07,
      "loss": 1.5236,
      "step": 4349
    },
    {
      "epoch": 2.8637261356155364,
      "grad_norm": 22.298175811767578,
      "learning_rate": 1.0165183617785735e-07,
      "loss": 2.1152,
      "step": 4350
    },
    {
      "epoch": 2.8643844634628044,
      "grad_norm": 14.034626960754395,
      "learning_rate": 1.0067371244894208e-07,
      "loss": 1.6688,
      "step": 4351
    },
    {
      "epoch": 2.8650427913100724,
      "grad_norm": 2.6758458614349365,
      "learning_rate": 9.970029358658206e-08,
      "loss": 1.5442,
      "step": 4352
    },
    {
      "epoch": 2.8657011191573405,
      "grad_norm": 27.439876556396484,
      "learning_rate": 9.873158005341943e-08,
      "loss": 2.2691,
      "step": 4353
    },
    {
      "epoch": 2.8663594470046085,
      "grad_norm": 2.587171792984009,
      "learning_rate": 9.776757230985146e-08,
      "loss": 1.5481,
      "step": 4354
    },
    {
      "epoch": 2.867017774851876,
      "grad_norm": 1.0511600971221924,
      "learning_rate": 9.680827081404609e-08,
      "loss": 1.5169,
      "step": 4355
    },
    {
      "epoch": 2.867676102699144,
      "grad_norm": 1.1680755615234375,
      "learning_rate": 9.585367602193086e-08,
      "loss": 1.5356,
      "step": 4356
    },
    {
      "epoch": 2.8683344305464122,
      "grad_norm": 1.3455203771591187,
      "learning_rate": 9.490378838719839e-08,
      "loss": 1.5101,
      "step": 4357
    },
    {
      "epoch": 2.86899275839368,
      "grad_norm": 24.955158233642578,
      "learning_rate": 9.395860836130088e-08,
      "loss": 2.1525,
      "step": 4358
    },
    {
      "epoch": 2.869651086240948,
      "grad_norm": 23.108230590820312,
      "learning_rate": 9.301813639345792e-08,
      "loss": 2.3292,
      "step": 4359
    },
    {
      "epoch": 2.870309414088216,
      "grad_norm": 1.596530556678772,
      "learning_rate": 9.208237293064748e-08,
      "loss": 1.5414,
      "step": 4360
    },
    {
      "epoch": 2.870967741935484,
      "grad_norm": 23.20458984375,
      "learning_rate": 9.11513184176116e-08,
      "loss": 2.1869,
      "step": 4361
    },
    {
      "epoch": 2.871626069782752,
      "grad_norm": 10.473325729370117,
      "learning_rate": 9.022497329685409e-08,
      "loss": 1.7322,
      "step": 4362
    },
    {
      "epoch": 2.8722843976300196,
      "grad_norm": 15.550040245056152,
      "learning_rate": 8.930333800863944e-08,
      "loss": 1.9607,
      "step": 4363
    },
    {
      "epoch": 2.8729427254772877,
      "grad_norm": 57.88826370239258,
      "learning_rate": 8.838641299099504e-08,
      "loss": 2.7196,
      "step": 4364
    },
    {
      "epoch": 2.8736010533245557,
      "grad_norm": 1.8646950721740723,
      "learning_rate": 8.747419867970897e-08,
      "loss": 1.5248,
      "step": 4365
    },
    {
      "epoch": 2.8742593811718233,
      "grad_norm": 16.97378158569336,
      "learning_rate": 8.656669550832996e-08,
      "loss": 2.1019,
      "step": 4366
    },
    {
      "epoch": 2.8749177090190914,
      "grad_norm": 12.159758567810059,
      "learning_rate": 8.56639039081708e-08,
      "loss": 1.8571,
      "step": 4367
    },
    {
      "epoch": 2.8755760368663594,
      "grad_norm": 18.880172729492188,
      "learning_rate": 8.476582430830049e-08,
      "loss": 2.4367,
      "step": 4368
    },
    {
      "epoch": 2.8762343647136275,
      "grad_norm": 18.462854385375977,
      "learning_rate": 8.387245713554981e-08,
      "loss": 2.1597,
      "step": 4369
    },
    {
      "epoch": 2.8768926925608955,
      "grad_norm": 0.8059767484664917,
      "learning_rate": 8.298380281451246e-08,
      "loss": 1.5139,
      "step": 4370
    },
    {
      "epoch": 2.877551020408163,
      "grad_norm": 32.40264892578125,
      "learning_rate": 8.209986176753947e-08,
      "loss": 2.4123,
      "step": 4371
    },
    {
      "epoch": 2.878209348255431,
      "grad_norm": 17.98444366455078,
      "learning_rate": 8.12206344147426e-08,
      "loss": 1.7704,
      "step": 4372
    },
    {
      "epoch": 2.8788676761026992,
      "grad_norm": 39.399356842041016,
      "learning_rate": 8.03461211739931e-08,
      "loss": 3.7513,
      "step": 4373
    },
    {
      "epoch": 2.879526003949967,
      "grad_norm": 22.843843460083008,
      "learning_rate": 7.947632246092408e-08,
      "loss": 2.189,
      "step": 4374
    },
    {
      "epoch": 2.880184331797235,
      "grad_norm": 14.829201698303223,
      "learning_rate": 7.861123868892484e-08,
      "loss": 1.9176,
      "step": 4375
    },
    {
      "epoch": 2.880842659644503,
      "grad_norm": 43.16917419433594,
      "learning_rate": 7.775087026914318e-08,
      "loss": 2.4423,
      "step": 4376
    },
    {
      "epoch": 2.881500987491771,
      "grad_norm": 1.18999445438385,
      "learning_rate": 7.68952176104909e-08,
      "loss": 1.516,
      "step": 4377
    },
    {
      "epoch": 2.882159315339039,
      "grad_norm": 9.793853759765625,
      "learning_rate": 7.604428111963157e-08,
      "loss": 1.8161,
      "step": 4378
    },
    {
      "epoch": 2.8828176431863066,
      "grad_norm": 1.132645606994629,
      "learning_rate": 7.519806120099394e-08,
      "loss": 1.5223,
      "step": 4379
    },
    {
      "epoch": 2.8834759710335747,
      "grad_norm": 13.016139030456543,
      "learning_rate": 7.43565582567618e-08,
      "loss": 1.896,
      "step": 4380
    },
    {
      "epoch": 2.8841342988808427,
      "grad_norm": 17.65055274963379,
      "learning_rate": 7.351977268687526e-08,
      "loss": 2.1617,
      "step": 4381
    },
    {
      "epoch": 2.8847926267281108,
      "grad_norm": 20.41303062438965,
      "learning_rate": 7.26877048890362e-08,
      "loss": 2.0102,
      "step": 4382
    },
    {
      "epoch": 2.8854509545753784,
      "grad_norm": 0.676243245601654,
      "learning_rate": 7.186035525870272e-08,
      "loss": 1.507,
      "step": 4383
    },
    {
      "epoch": 2.8861092824226464,
      "grad_norm": 10.825220108032227,
      "learning_rate": 7.103772418909138e-08,
      "loss": 1.9716,
      "step": 4384
    },
    {
      "epoch": 2.8867676102699145,
      "grad_norm": 19.664831161499023,
      "learning_rate": 7.021981207117279e-08,
      "loss": 1.8261,
      "step": 4385
    },
    {
      "epoch": 2.8874259381171825,
      "grad_norm": 17.60458755493164,
      "learning_rate": 6.940661929367708e-08,
      "loss": 1.8337,
      "step": 4386
    },
    {
      "epoch": 2.8880842659644506,
      "grad_norm": 1.6746349334716797,
      "learning_rate": 6.859814624309514e-08,
      "loss": 1.5233,
      "step": 4387
    },
    {
      "epoch": 2.888742593811718,
      "grad_norm": 5.188080310821533,
      "learning_rate": 6.779439330366732e-08,
      "loss": 1.5241,
      "step": 4388
    },
    {
      "epoch": 2.889400921658986,
      "grad_norm": 1.4431818723678589,
      "learning_rate": 6.699536085739589e-08,
      "loss": 1.5242,
      "step": 4389
    },
    {
      "epoch": 2.8900592495062543,
      "grad_norm": 18.87523651123047,
      "learning_rate": 6.620104928403814e-08,
      "loss": 1.8505,
      "step": 4390
    },
    {
      "epoch": 2.890717577353522,
      "grad_norm": 13.906375885009766,
      "learning_rate": 6.541145896110767e-08,
      "loss": 1.9604,
      "step": 4391
    },
    {
      "epoch": 2.89137590520079,
      "grad_norm": 61.74871826171875,
      "learning_rate": 6.462659026387319e-08,
      "loss": 3.4172,
      "step": 4392
    },
    {
      "epoch": 2.892034233048058,
      "grad_norm": 20.997467041015625,
      "learning_rate": 6.384644356536185e-08,
      "loss": 2.1818,
      "step": 4393
    },
    {
      "epoch": 2.892692560895326,
      "grad_norm": 15.335543632507324,
      "learning_rate": 6.307101923635373e-08,
      "loss": 2.1097,
      "step": 4394
    },
    {
      "epoch": 2.893350888742594,
      "grad_norm": 15.567330360412598,
      "learning_rate": 6.230031764538735e-08,
      "loss": 1.7749,
      "step": 4395
    },
    {
      "epoch": 2.8940092165898617,
      "grad_norm": 1.098327875137329,
      "learning_rate": 6.153433915875417e-08,
      "loss": 1.5247,
      "step": 4396
    },
    {
      "epoch": 2.8946675444371297,
      "grad_norm": 36.10748291015625,
      "learning_rate": 6.077308414050075e-08,
      "loss": 2.7153,
      "step": 4397
    },
    {
      "epoch": 2.8953258722843978,
      "grad_norm": 25.118446350097656,
      "learning_rate": 6.001655295243213e-08,
      "loss": 2.4997,
      "step": 4398
    },
    {
      "epoch": 2.8959842001316654,
      "grad_norm": 17.60344696044922,
      "learning_rate": 5.926474595410403e-08,
      "loss": 2.0372,
      "step": 4399
    },
    {
      "epoch": 2.8966425279789334,
      "grad_norm": 24.789270401000977,
      "learning_rate": 5.8517663502829506e-08,
      "loss": 3.1194,
      "step": 4400
    },
    {
      "epoch": 2.8973008558262014,
      "grad_norm": 0.9457721710205078,
      "learning_rate": 5.777530595367564e-08,
      "loss": 1.508,
      "step": 4401
    },
    {
      "epoch": 2.8979591836734695,
      "grad_norm": 11.577295303344727,
      "learning_rate": 5.7037673659464664e-08,
      "loss": 1.676,
      "step": 4402
    },
    {
      "epoch": 2.8986175115207375,
      "grad_norm": 9.187254905700684,
      "learning_rate": 5.630476697076948e-08,
      "loss": 1.8879,
      "step": 4403
    },
    {
      "epoch": 2.899275839368005,
      "grad_norm": 3.114246129989624,
      "learning_rate": 5.557658623592255e-08,
      "loss": 1.5574,
      "step": 4404
    },
    {
      "epoch": 2.899934167215273,
      "grad_norm": 0.701408863067627,
      "learning_rate": 5.485313180100482e-08,
      "loss": 1.5085,
      "step": 4405
    },
    {
      "epoch": 2.9005924950625412,
      "grad_norm": 10.512002944946289,
      "learning_rate": 5.41344040098557e-08,
      "loss": 1.8981,
      "step": 4406
    },
    {
      "epoch": 2.901250822909809,
      "grad_norm": 22.887306213378906,
      "learning_rate": 5.3420403204064164e-08,
      "loss": 2.6939,
      "step": 4407
    },
    {
      "epoch": 2.901909150757077,
      "grad_norm": 22.974018096923828,
      "learning_rate": 5.271112972297543e-08,
      "loss": 2.1135,
      "step": 4408
    },
    {
      "epoch": 2.902567478604345,
      "grad_norm": 8.495987892150879,
      "learning_rate": 5.2006583903687626e-08,
      "loss": 1.6212,
      "step": 4409
    },
    {
      "epoch": 2.903225806451613,
      "grad_norm": 30.84787368774414,
      "learning_rate": 5.1306766081048456e-08,
      "loss": 2.3025,
      "step": 4410
    },
    {
      "epoch": 2.903884134298881,
      "grad_norm": 21.63819122314453,
      "learning_rate": 5.061167658766408e-08,
      "loss": 1.9264,
      "step": 4411
    },
    {
      "epoch": 2.9045424621461486,
      "grad_norm": 1.0782219171524048,
      "learning_rate": 4.9921315753889147e-08,
      "loss": 1.514,
      "step": 4412
    },
    {
      "epoch": 2.9052007899934167,
      "grad_norm": 15.697256088256836,
      "learning_rate": 4.9235683907833396e-08,
      "loss": 2.1442,
      "step": 4413
    },
    {
      "epoch": 2.9058591178406847,
      "grad_norm": 12.461981773376465,
      "learning_rate": 4.85547813753573e-08,
      "loss": 2.0443,
      "step": 4414
    },
    {
      "epoch": 2.9065174456879523,
      "grad_norm": 2.0070509910583496,
      "learning_rate": 4.7878608480074196e-08,
      "loss": 1.5462,
      "step": 4415
    },
    {
      "epoch": 2.9071757735352204,
      "grad_norm": 1.2811533212661743,
      "learning_rate": 4.7207165543350364e-08,
      "loss": 1.5485,
      "step": 4416
    },
    {
      "epoch": 2.9078341013824884,
      "grad_norm": 43.51778793334961,
      "learning_rate": 4.654045288430386e-08,
      "loss": 2.6722,
      "step": 4417
    },
    {
      "epoch": 2.9084924292297565,
      "grad_norm": 10.748653411865234,
      "learning_rate": 4.5878470819802347e-08,
      "loss": 1.9396,
      "step": 4418
    },
    {
      "epoch": 2.9091507570770245,
      "grad_norm": 26.8880615234375,
      "learning_rate": 4.522121966446857e-08,
      "loss": 2.442,
      "step": 4419
    },
    {
      "epoch": 2.909809084924292,
      "grad_norm": 0.7886802554130554,
      "learning_rate": 4.45686997306749e-08,
      "loss": 1.5108,
      "step": 4420
    },
    {
      "epoch": 2.91046741277156,
      "grad_norm": 0.9538605213165283,
      "learning_rate": 4.392091132854437e-08,
      "loss": 1.5398,
      "step": 4421
    },
    {
      "epoch": 2.9111257406188282,
      "grad_norm": 0.9110169410705566,
      "learning_rate": 4.327785476595292e-08,
      "loss": 1.5223,
      "step": 4422
    },
    {
      "epoch": 2.9117840684660963,
      "grad_norm": 5.617377758026123,
      "learning_rate": 4.263953034852719e-08,
      "loss": 1.5848,
      "step": 4423
    },
    {
      "epoch": 2.912442396313364,
      "grad_norm": 22.576961517333984,
      "learning_rate": 4.200593837964562e-08,
      "loss": 2.387,
      "step": 4424
    },
    {
      "epoch": 2.913100724160632,
      "grad_norm": 13.99268913269043,
      "learning_rate": 4.1377079160433985e-08,
      "loss": 2.0062,
      "step": 4425
    },
    {
      "epoch": 2.9137590520079,
      "grad_norm": 6.4942731857299805,
      "learning_rate": 4.075295298977322e-08,
      "loss": 1.6796,
      "step": 4426
    },
    {
      "epoch": 2.914417379855168,
      "grad_norm": 10.737401962280273,
      "learning_rate": 4.013356016429048e-08,
      "loss": 1.6239,
      "step": 4427
    },
    {
      "epoch": 2.915075707702436,
      "grad_norm": 32.18654251098633,
      "learning_rate": 3.951890097836697e-08,
      "loss": 3.6446,
      "step": 4428
    },
    {
      "epoch": 2.9157340355497037,
      "grad_norm": 1.1787618398666382,
      "learning_rate": 3.890897572413233e-08,
      "loss": 1.5314,
      "step": 4429
    },
    {
      "epoch": 2.9163923633969717,
      "grad_norm": 8.230609893798828,
      "learning_rate": 3.83037846914669e-08,
      "loss": 1.6386,
      "step": 4430
    },
    {
      "epoch": 2.9170506912442398,
      "grad_norm": 3.925316333770752,
      "learning_rate": 3.7703328167999485e-08,
      "loss": 1.5521,
      "step": 4431
    },
    {
      "epoch": 2.9177090190915074,
      "grad_norm": 10.678459167480469,
      "learning_rate": 3.710760643911071e-08,
      "loss": 1.6369,
      "step": 4432
    },
    {
      "epoch": 2.9183673469387754,
      "grad_norm": 16.934755325317383,
      "learning_rate": 3.651661978793075e-08,
      "loss": 1.832,
      "step": 4433
    },
    {
      "epoch": 2.9190256747860435,
      "grad_norm": 6.755341053009033,
      "learning_rate": 3.5930368495337155e-08,
      "loss": 1.7114,
      "step": 4434
    },
    {
      "epoch": 2.9196840026333115,
      "grad_norm": 14.023104667663574,
      "learning_rate": 3.5348852839959256e-08,
      "loss": 1.9066,
      "step": 4435
    },
    {
      "epoch": 2.9203423304805796,
      "grad_norm": 3.3644235134124756,
      "learning_rate": 3.4772073098175984e-08,
      "loss": 1.5309,
      "step": 4436
    },
    {
      "epoch": 2.921000658327847,
      "grad_norm": 0.6957792639732361,
      "learning_rate": 3.420002954411139e-08,
      "loss": 1.5188,
      "step": 4437
    },
    {
      "epoch": 2.921658986175115,
      "grad_norm": 37.66688537597656,
      "learning_rate": 3.3632722449643554e-08,
      "loss": 2.1009,
      "step": 4438
    },
    {
      "epoch": 2.9223173140223833,
      "grad_norm": 2.5143725872039795,
      "learning_rate": 3.307015208439679e-08,
      "loss": 1.5559,
      "step": 4439
    },
    {
      "epoch": 2.922975641869651,
      "grad_norm": 1.1076366901397705,
      "learning_rate": 3.2512318715745007e-08,
      "loss": 1.5228,
      "step": 4440
    },
    {
      "epoch": 2.923633969716919,
      "grad_norm": 1.023447871208191,
      "learning_rate": 3.195922260880946e-08,
      "loss": 1.5155,
      "step": 4441
    },
    {
      "epoch": 2.924292297564187,
      "grad_norm": 24.644100189208984,
      "learning_rate": 3.1410864026462096e-08,
      "loss": 1.7945,
      "step": 4442
    },
    {
      "epoch": 2.924950625411455,
      "grad_norm": 16.25848960876465,
      "learning_rate": 3.086724322932111e-08,
      "loss": 1.994,
      "step": 4443
    },
    {
      "epoch": 2.925608953258723,
      "grad_norm": 2.268339157104492,
      "learning_rate": 3.0328360475754274e-08,
      "loss": 1.5235,
      "step": 4444
    },
    {
      "epoch": 2.9262672811059907,
      "grad_norm": 2.9847917556762695,
      "learning_rate": 2.979421602187782e-08,
      "loss": 1.5496,
      "step": 4445
    },
    {
      "epoch": 2.9269256089532587,
      "grad_norm": 15.345587730407715,
      "learning_rate": 2.9264810121554244e-08,
      "loss": 1.9507,
      "step": 4446
    },
    {
      "epoch": 2.9275839368005268,
      "grad_norm": 3.1822376251220703,
      "learning_rate": 2.8740143026395605e-08,
      "loss": 1.5413,
      "step": 4447
    },
    {
      "epoch": 2.9282422646477944,
      "grad_norm": 22.580196380615234,
      "learning_rate": 2.8220214985760218e-08,
      "loss": 2.4565,
      "step": 4448
    },
    {
      "epoch": 2.9289005924950624,
      "grad_norm": 1.4863940477371216,
      "learning_rate": 2.7705026246757083e-08,
      "loss": 1.5534,
      "step": 4449
    },
    {
      "epoch": 2.9295589203423305,
      "grad_norm": 1.819033145904541,
      "learning_rate": 2.7194577054238114e-08,
      "loss": 1.5211,
      "step": 4450
    },
    {
      "epoch": 2.9302172481895985,
      "grad_norm": 20.239004135131836,
      "learning_rate": 2.6688867650808136e-08,
      "loss": 1.9715,
      "step": 4451
    },
    {
      "epoch": 2.9308755760368665,
      "grad_norm": 18.167390823364258,
      "learning_rate": 2.6187898276813785e-08,
      "loss": 2.2195,
      "step": 4452
    },
    {
      "epoch": 2.931533903884134,
      "grad_norm": 1.5594747066497803,
      "learning_rate": 2.5691669170351263e-08,
      "loss": 1.5729,
      "step": 4453
    },
    {
      "epoch": 2.932192231731402,
      "grad_norm": 0.7353081703186035,
      "learning_rate": 2.5200180567266364e-08,
      "loss": 1.5119,
      "step": 4454
    },
    {
      "epoch": 2.9328505595786702,
      "grad_norm": 7.76129150390625,
      "learning_rate": 2.47134327011489e-08,
      "loss": 1.5868,
      "step": 4455
    },
    {
      "epoch": 2.933508887425938,
      "grad_norm": 0.6836162209510803,
      "learning_rate": 2.4231425803334928e-08,
      "loss": 1.5082,
      "step": 4456
    },
    {
      "epoch": 2.934167215273206,
      "grad_norm": 27.532819747924805,
      "learning_rate": 2.3754160102908984e-08,
      "loss": 2.1818,
      "step": 4457
    },
    {
      "epoch": 2.934825543120474,
      "grad_norm": 29.72252655029297,
      "learning_rate": 2.3281635826702955e-08,
      "loss": 2.4538,
      "step": 4458
    },
    {
      "epoch": 2.935483870967742,
      "grad_norm": 11.349546432495117,
      "learning_rate": 2.2813853199292745e-08,
      "loss": 1.7693,
      "step": 4459
    },
    {
      "epoch": 2.93614219881501,
      "grad_norm": 12.856222152709961,
      "learning_rate": 2.2350812443002744e-08,
      "loss": 1.8464,
      "step": 4460
    },
    {
      "epoch": 2.936800526662278,
      "grad_norm": 16.079755783081055,
      "learning_rate": 2.189251377790247e-08,
      "loss": 1.8939,
      "step": 4461
    },
    {
      "epoch": 2.9374588545095457,
      "grad_norm": 2.1838440895080566,
      "learning_rate": 2.1438957421808793e-08,
      "loss": 1.5562,
      "step": 4462
    },
    {
      "epoch": 2.9381171823568137,
      "grad_norm": 3.7861649990081787,
      "learning_rate": 2.099014359028484e-08,
      "loss": 1.6093,
      "step": 4463
    },
    {
      "epoch": 2.938775510204082,
      "grad_norm": 25.402355194091797,
      "learning_rate": 2.054607249663665e-08,
      "loss": 2.1506,
      "step": 4464
    },
    {
      "epoch": 2.9394338380513494,
      "grad_norm": 17.048295974731445,
      "learning_rate": 2.010674435191984e-08,
      "loss": 1.8813,
      "step": 4465
    },
    {
      "epoch": 2.9400921658986174,
      "grad_norm": 17.153759002685547,
      "learning_rate": 1.9672159364935162e-08,
      "loss": 2.0338,
      "step": 4466
    },
    {
      "epoch": 2.9407504937458855,
      "grad_norm": 14.032471656799316,
      "learning_rate": 1.92423177422274e-08,
      "loss": 1.8035,
      "step": 4467
    },
    {
      "epoch": 2.9414088215931535,
      "grad_norm": 59.588802337646484,
      "learning_rate": 1.8817219688087585e-08,
      "loss": 3.2335,
      "step": 4468
    },
    {
      "epoch": 2.9420671494404216,
      "grad_norm": 9.636804580688477,
      "learning_rate": 1.83968654045541e-08,
      "loss": 1.6232,
      "step": 4469
    },
    {
      "epoch": 2.942725477287689,
      "grad_norm": 0.7881522178649902,
      "learning_rate": 1.7981255091409356e-08,
      "loss": 1.5093,
      "step": 4470
    },
    {
      "epoch": 2.9433838051349572,
      "grad_norm": 1.2821435928344727,
      "learning_rate": 1.7570388946178686e-08,
      "loss": 1.52,
      "step": 4471
    },
    {
      "epoch": 2.9440421329822253,
      "grad_norm": 35.5074348449707,
      "learning_rate": 1.7164267164138104e-08,
      "loss": 2.77,
      "step": 4472
    },
    {
      "epoch": 2.944700460829493,
      "grad_norm": 2.7544009685516357,
      "learning_rate": 1.6762889938303216e-08,
      "loss": 1.5778,
      "step": 4473
    },
    {
      "epoch": 2.945358788676761,
      "grad_norm": 14.679000854492188,
      "learning_rate": 1.6366257459438094e-08,
      "loss": 1.7881,
      "step": 4474
    },
    {
      "epoch": 2.946017116524029,
      "grad_norm": 23.814407348632812,
      "learning_rate": 1.5974369916050836e-08,
      "loss": 2.1587,
      "step": 4475
    },
    {
      "epoch": 2.946675444371297,
      "grad_norm": 1.3752878904342651,
      "learning_rate": 1.5587227494394674e-08,
      "loss": 1.5273,
      "step": 4476
    },
    {
      "epoch": 2.947333772218565,
      "grad_norm": 1.817077398300171,
      "learning_rate": 1.5204830378466872e-08,
      "loss": 1.5314,
      "step": 4477
    },
    {
      "epoch": 2.9479921000658327,
      "grad_norm": 7.291790008544922,
      "learning_rate": 1.482717875000983e-08,
      "loss": 1.7343,
      "step": 4478
    },
    {
      "epoch": 2.9486504279131007,
      "grad_norm": 11.944015502929688,
      "learning_rate": 1.4454272788511081e-08,
      "loss": 1.549,
      "step": 4479
    },
    {
      "epoch": 2.9493087557603688,
      "grad_norm": 24.99329376220703,
      "learning_rate": 1.408611267120219e-08,
      "loss": 2.2232,
      "step": 4480
    },
    {
      "epoch": 2.9499670836076364,
      "grad_norm": 1.5394278764724731,
      "learning_rate": 1.3722698573057635e-08,
      "loss": 1.5204,
      "step": 4481
    },
    {
      "epoch": 2.9506254114549044,
      "grad_norm": 10.310060501098633,
      "learning_rate": 1.3364030666799255e-08,
      "loss": 1.8257,
      "step": 4482
    },
    {
      "epoch": 2.9512837393021725,
      "grad_norm": 26.401256561279297,
      "learning_rate": 1.30101091228918e-08,
      "loss": 2.1468,
      "step": 4483
    },
    {
      "epoch": 2.9519420671494405,
      "grad_norm": 30.235942840576172,
      "learning_rate": 1.2660934109541833e-08,
      "loss": 2.4221,
      "step": 4484
    },
    {
      "epoch": 2.9526003949967086,
      "grad_norm": 0.839035153388977,
      "learning_rate": 1.2316505792705491e-08,
      "loss": 1.5156,
      "step": 4485
    },
    {
      "epoch": 2.953258722843976,
      "grad_norm": 12.992506980895996,
      "learning_rate": 1.1976824336077385e-08,
      "loss": 1.9752,
      "step": 4486
    },
    {
      "epoch": 2.953917050691244,
      "grad_norm": 13.764735221862793,
      "learning_rate": 1.164188990109949e-08,
      "loss": 1.7243,
      "step": 4487
    },
    {
      "epoch": 2.9545753785385123,
      "grad_norm": 26.59918975830078,
      "learning_rate": 1.1311702646955581e-08,
      "loss": 2.6356,
      "step": 4488
    },
    {
      "epoch": 2.95523370638578,
      "grad_norm": 32.415435791015625,
      "learning_rate": 1.098626273057568e-08,
      "loss": 2.6485,
      "step": 4489
    },
    {
      "epoch": 2.955892034233048,
      "grad_norm": 8.492107391357422,
      "learning_rate": 1.0665570306630512e-08,
      "loss": 1.7991,
      "step": 4490
    },
    {
      "epoch": 2.956550362080316,
      "grad_norm": 21.532976150512695,
      "learning_rate": 1.0349625527537044e-08,
      "loss": 2.635,
      "step": 4491
    },
    {
      "epoch": 2.957208689927584,
      "grad_norm": 9.227347373962402,
      "learning_rate": 1.0038428543454049e-08,
      "loss": 1.7244,
      "step": 4492
    },
    {
      "epoch": 2.957867017774852,
      "grad_norm": 5.598824501037598,
      "learning_rate": 9.731979502284328e-09,
      "loss": 1.681,
      "step": 4493
    },
    {
      "epoch": 2.9585253456221197,
      "grad_norm": 4.904458045959473,
      "learning_rate": 9.430278549675819e-09,
      "loss": 1.6977,
      "step": 4494
    },
    {
      "epoch": 2.9591836734693877,
      "grad_norm": 3.5277068614959717,
      "learning_rate": 9.13332582901716e-09,
      "loss": 1.5445,
      "step": 4495
    },
    {
      "epoch": 2.9598420013166558,
      "grad_norm": 1.631899356842041,
      "learning_rate": 8.841121481442117e-09,
      "loss": 1.5393,
      "step": 4496
    },
    {
      "epoch": 2.9605003291639234,
      "grad_norm": 0.7298063039779663,
      "learning_rate": 8.55366564582738e-09,
      "loss": 1.5134,
      "step": 4497
    },
    {
      "epoch": 2.9611586570111914,
      "grad_norm": 26.421321868896484,
      "learning_rate": 8.270958458790335e-09,
      "loss": 3.6226,
      "step": 4498
    },
    {
      "epoch": 2.9618169848584595,
      "grad_norm": 3.984029769897461,
      "learning_rate": 7.99300005469572e-09,
      "loss": 1.6009,
      "step": 4499
    },
    {
      "epoch": 2.9624753127057275,
      "grad_norm": 0.6277604699134827,
      "learning_rate": 7.719790565648978e-09,
      "loss": 1.5108,
      "step": 4500
    },
    {
      "epoch": 2.9631336405529956,
      "grad_norm": 1.1147687435150146,
      "learning_rate": 7.451330121498457e-09,
      "loss": 1.523,
      "step": 4501
    },
    {
      "epoch": 2.9637919684002636,
      "grad_norm": 0.8575043678283691,
      "learning_rate": 7.187618849835431e-09,
      "loss": 1.5102,
      "step": 4502
    },
    {
      "epoch": 2.964450296247531,
      "grad_norm": 30.417964935302734,
      "learning_rate": 6.928656875994089e-09,
      "loss": 2.6946,
      "step": 4503
    },
    {
      "epoch": 2.9651086240947993,
      "grad_norm": 8.954380989074707,
      "learning_rate": 6.674444323052642e-09,
      "loss": 1.7843,
      "step": 4504
    },
    {
      "epoch": 2.9657669519420673,
      "grad_norm": 19.768760681152344,
      "learning_rate": 6.424981311831113e-09,
      "loss": 2.1546,
      "step": 4505
    },
    {
      "epoch": 2.966425279789335,
      "grad_norm": 40.41783905029297,
      "learning_rate": 6.180267960890218e-09,
      "loss": 2.3832,
      "step": 4506
    },
    {
      "epoch": 2.967083607636603,
      "grad_norm": 6.904022693634033,
      "learning_rate": 5.940304386538032e-09,
      "loss": 1.6991,
      "step": 4507
    },
    {
      "epoch": 2.967741935483871,
      "grad_norm": 1.4004809856414795,
      "learning_rate": 5.705090702819993e-09,
      "loss": 1.5145,
      "step": 4508
    },
    {
      "epoch": 2.968400263331139,
      "grad_norm": 7.57819938659668,
      "learning_rate": 5.474627021526679e-09,
      "loss": 1.767,
      "step": 4509
    },
    {
      "epoch": 2.969058591178407,
      "grad_norm": 1.3092869520187378,
      "learning_rate": 5.248913452192695e-09,
      "loss": 1.5416,
      "step": 4510
    },
    {
      "epoch": 2.9697169190256747,
      "grad_norm": 9.223999977111816,
      "learning_rate": 5.027950102091117e-09,
      "loss": 1.8154,
      "step": 4511
    },
    {
      "epoch": 2.9703752468729427,
      "grad_norm": 12.515371322631836,
      "learning_rate": 4.811737076241274e-09,
      "loss": 1.6084,
      "step": 4512
    },
    {
      "epoch": 2.971033574720211,
      "grad_norm": 22.09813117980957,
      "learning_rate": 4.600274477402078e-09,
      "loss": 2.4503,
      "step": 4513
    },
    {
      "epoch": 2.9716919025674784,
      "grad_norm": 8.695454597473145,
      "learning_rate": 4.393562406075358e-09,
      "loss": 1.7694,
      "step": 4514
    },
    {
      "epoch": 2.9723502304147464,
      "grad_norm": 12.96822738647461,
      "learning_rate": 4.1916009605058594e-09,
      "loss": 1.937,
      "step": 4515
    },
    {
      "epoch": 2.9730085582620145,
      "grad_norm": 7.122030735015869,
      "learning_rate": 3.994390236681245e-09,
      "loss": 1.7442,
      "step": 4516
    },
    {
      "epoch": 2.9736668861092825,
      "grad_norm": 18.88886070251465,
      "learning_rate": 3.801930328327652e-09,
      "loss": 1.678,
      "step": 4517
    },
    {
      "epoch": 2.9743252139565506,
      "grad_norm": 16.696102142333984,
      "learning_rate": 3.6142213269174662e-09,
      "loss": 2.0436,
      "step": 4518
    },
    {
      "epoch": 2.974983541803818,
      "grad_norm": 19.869688034057617,
      "learning_rate": 3.431263321663769e-09,
      "loss": 2.1267,
      "step": 4519
    },
    {
      "epoch": 2.9756418696510862,
      "grad_norm": 14.899609565734863,
      "learning_rate": 3.2530563995192278e-09,
      "loss": 1.8655,
      "step": 4520
    },
    {
      "epoch": 2.9763001974983543,
      "grad_norm": 15.27358627319336,
      "learning_rate": 3.0796006451827563e-09,
      "loss": 1.9571,
      "step": 4521
    },
    {
      "epoch": 2.976958525345622,
      "grad_norm": 15.303521156311035,
      "learning_rate": 2.9108961410917458e-09,
      "loss": 1.9473,
      "step": 4522
    },
    {
      "epoch": 2.97761685319289,
      "grad_norm": 1.3845531940460205,
      "learning_rate": 2.746942967427613e-09,
      "loss": 1.5198,
      "step": 4523
    },
    {
      "epoch": 2.978275181040158,
      "grad_norm": 18.407548904418945,
      "learning_rate": 2.58774120211025e-09,
      "loss": 2.1059,
      "step": 4524
    },
    {
      "epoch": 2.978933508887426,
      "grad_norm": 6.993439674377441,
      "learning_rate": 2.4332909208069076e-09,
      "loss": 1.711,
      "step": 4525
    },
    {
      "epoch": 2.979591836734694,
      "grad_norm": 17.511722564697266,
      "learning_rate": 2.2835921969210917e-09,
      "loss": 2.2383,
      "step": 4526
    },
    {
      "epoch": 2.9802501645819617,
      "grad_norm": 1.3035311698913574,
      "learning_rate": 2.138645101600334e-09,
      "loss": 1.5232,
      "step": 4527
    },
    {
      "epoch": 2.9809084924292297,
      "grad_norm": 1.1490668058395386,
      "learning_rate": 1.9984497037350838e-09,
      "loss": 1.5138,
      "step": 4528
    },
    {
      "epoch": 2.9815668202764978,
      "grad_norm": 3.454268217086792,
      "learning_rate": 1.8630060699553755e-09,
      "loss": 1.5737,
      "step": 4529
    },
    {
      "epoch": 2.9822251481237654,
      "grad_norm": 31.063127517700195,
      "learning_rate": 1.7323142646341606e-09,
      "loss": 3.6196,
      "step": 4530
    },
    {
      "epoch": 2.9828834759710334,
      "grad_norm": 30.41859245300293,
      "learning_rate": 1.606374349885087e-09,
      "loss": 2.2417,
      "step": 4531
    },
    {
      "epoch": 2.9835418038183015,
      "grad_norm": 36.73031997680664,
      "learning_rate": 1.4851863855647185e-09,
      "loss": 1.8484,
      "step": 4532
    },
    {
      "epoch": 2.9842001316655695,
      "grad_norm": 1.953171968460083,
      "learning_rate": 1.3687504292692055e-09,
      "loss": 1.5549,
      "step": 4533
    },
    {
      "epoch": 2.9848584595128376,
      "grad_norm": 11.499150276184082,
      "learning_rate": 1.2570665363365043e-09,
      "loss": 2.0849,
      "step": 4534
    },
    {
      "epoch": 2.985516787360105,
      "grad_norm": 10.28454875946045,
      "learning_rate": 1.150134759848598e-09,
      "loss": 1.6359,
      "step": 4535
    },
    {
      "epoch": 2.986175115207373,
      "grad_norm": 3.4026567935943604,
      "learning_rate": 1.0479551506259456e-09,
      "loss": 1.5501,
      "step": 4536
    },
    {
      "epoch": 2.9868334430546413,
      "grad_norm": 14.122601509094238,
      "learning_rate": 9.505277572330328e-10,
      "loss": 1.7831,
      "step": 4537
    },
    {
      "epoch": 2.9874917709019093,
      "grad_norm": 6.587391376495361,
      "learning_rate": 8.578526259728215e-10,
      "loss": 1.7078,
      "step": 4538
    },
    {
      "epoch": 2.988150098749177,
      "grad_norm": 15.472638130187988,
      "learning_rate": 7.699298008911893e-10,
      "loss": 1.8767,
      "step": 4539
    },
    {
      "epoch": 2.988808426596445,
      "grad_norm": 32.97736358642578,
      "learning_rate": 6.867593237747106e-10,
      "loss": 2.5367,
      "step": 4540
    },
    {
      "epoch": 2.989466754443713,
      "grad_norm": 1.65522038936615,
      "learning_rate": 6.083412341539863e-10,
      "loss": 1.5227,
      "step": 4541
    },
    {
      "epoch": 2.990125082290981,
      "grad_norm": 0.7668448090553284,
      "learning_rate": 5.346755692980932e-10,
      "loss": 1.5198,
      "step": 4542
    },
    {
      "epoch": 2.990783410138249,
      "grad_norm": 1.2827105522155762,
      "learning_rate": 4.657623642179143e-10,
      "loss": 1.5186,
      "step": 4543
    },
    {
      "epoch": 2.9914417379855167,
      "grad_norm": 18.31343650817871,
      "learning_rate": 4.016016516661392e-10,
      "loss": 2.2496,
      "step": 4544
    },
    {
      "epoch": 2.9921000658327848,
      "grad_norm": 1.6539262533187866,
      "learning_rate": 3.421934621350431e-10,
      "loss": 1.5215,
      "step": 4545
    },
    {
      "epoch": 2.992758393680053,
      "grad_norm": 1.4887951612472534,
      "learning_rate": 2.8753782386203853e-10,
      "loss": 1.5145,
      "step": 4546
    },
    {
      "epoch": 2.9934167215273204,
      "grad_norm": 0.9272480010986328,
      "learning_rate": 2.3763476282190334e-10,
      "loss": 1.5082,
      "step": 4547
    },
    {
      "epoch": 2.9940750493745885,
      "grad_norm": 11.877034187316895,
      "learning_rate": 1.924843027334422e-10,
      "loss": 1.8594,
      "step": 4548
    },
    {
      "epoch": 2.9947333772218565,
      "grad_norm": 6.717353343963623,
      "learning_rate": 1.5208646505282531e-10,
      "loss": 1.5587,
      "step": 4549
    },
    {
      "epoch": 2.9953917050691246,
      "grad_norm": 1.2248945236206055,
      "learning_rate": 1.164412689824701e-10,
      "loss": 1.5243,
      "step": 4550
    },
    {
      "epoch": 2.9960500329163926,
      "grad_norm": 1.0623948574066162,
      "learning_rate": 8.554873146326969e-11,
      "loss": 1.5206,
      "step": 4551
    },
    {
      "epoch": 2.99670836076366,
      "grad_norm": 33.664642333984375,
      "learning_rate": 5.940886717570316e-11,
      "loss": 2.4095,
      "step": 4552
    },
    {
      "epoch": 2.9973666886109283,
      "grad_norm": 21.29426383972168,
      "learning_rate": 3.802168854538657e-11,
      "loss": 1.8721,
      "step": 4553
    },
    {
      "epoch": 2.9980250164581963,
      "grad_norm": 7.168196678161621,
      "learning_rate": 2.138720573641173e-11,
      "loss": 1.717,
      "step": 4554
    },
    {
      "epoch": 2.998683344305464,
      "grad_norm": 11.494026184082031,
      "learning_rate": 9.505426653566575e-12,
      "loss": 1.871,
      "step": 4555
    },
    {
      "epoch": 2.999341672152732,
      "grad_norm": 0.6780911684036255,
      "learning_rate": 2.3763569456658475e-12,
      "loss": 1.5059,
      "step": 4556
    },
    {
      "epoch": 3.0,
      "grad_norm": 16.957901000976562,
      "learning_rate": 0.0,
      "loss": 2.095,
      "step": 4557
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.5266272189349113,
      "eval_loss": 1.915813684463501,
      "eval_runtime": 88.1673,
      "eval_samples_per_second": 1.917,
      "eval_steps_per_second": 1.917,
      "step": 4557
    }
  ],
  "logging_steps": 1,
  "max_steps": 4557,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.120268941361152e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
